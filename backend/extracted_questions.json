[
  {
    "question_number": 92,
    "question_text": "A company is running an application in the AWS Cloud. The application collects and stores a large amount of unstructured data in an Amazon S3 bucket. The S3 bucket contains several terabytes of data and uses the S3 Standard storage class. The data increases in size by several gigabytes every day.\n\nThe company needs to query and analyze the data. The company does not access data that is more than 1 year old. However, the company must retain all the data indefinitely for compliance reasons.\n\nWhich solution will meet these requirements MOST cost-effectively?",
    "answers": {
      "A": "Use S3 Select to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.",
      "B": "Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.",
      "C": "Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.",
      "D": "Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Intelligent-Tiering."
    },
    "correct_answer": "A",
    "explanation": "### masetromain Highly Voted ✔️ 1 year, 6 months ago\n\n**Selected Answer: C**\n\nThe correct answer is C. Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.\n\nThis solution allows you to use Amazon Athena and the AWS Glue Data Catalog to query and analyze the data in an S3 bucket. Amazon Athena is a serverless, interactive query service that allows you to analyze data in S3 using SQL. The AWS Glue Data Catalog is a managed metadata repository that can be used to store and retrieve table definitions for data stored in S3. Together, these services can provide a cost-effective way to query and analyze large amounts of unstructured data. Additionally, by using an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive, you can retain the data indefinitely for compliance reasons while also reducing storage costs.\n\n*upvoted 19 times*\n\n---\n\n### masetromain 1 year, 6 months ago\n\nThe other options are not correct because:\n\nA. Using S3 Select is good for filtering data in S3, but it may not be a suitable solution for querying and analyzing large amounts of data.\n\nB. Amazon Redshift Spectrum can be used to query data stored in S3, but it may not be as cost-effective as using Amazon Athena for querying unstructured data.\n\nD. Using Amazon Redshift Spectrum with S3 Intelligent-Tiering could be a good solution, but S3 Intelligent-Tiering is designed to optimize storage costs based on access patterns and it would not be the best solution for compliance reasons as S3 Intelligent-Tiering will move data to other storage classes according to access patterns.\n\n*upvoted 8 times*\n\n---\n\n### Japanese1 8 months, 1 week ago\n\nThis is a nonsense explanation. In the first place, Redshift cannot handle unstructured data.\n\n*upvoted 4 times*\n\n---\n\n### dankoskiite 5 months, 3 weeks ago\n\nAmazon Redshift is designed for structured data. However, Amazon Redshift Spectrum enables you to run queries against exabytes of unstructured data in Amazon S3, with no loading or ETL required.\n\n*upvoted 2 times*\n\n---\n\n### Untamables Highly Voted ✔️ 1 year, 6 months ago\n\n**Selected Answer: C**\n\nGenerally, unstructured data should be converted structured data before querying them. AWS Glue can do that.  \n[https://docs.aws.amazon.com/glue/latest/dg/schema-relationalize.html](https://docs.aws.amazon.com/glue/latest/dg/schema-relationalize.html)  \n[https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html](https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html)\n\n*upvoted 7 times*\n\n---\n\n### gofavad926 Most Recent 4 months, 3 weeks ago\n\n---\n\n# Page 265\n\n### Selected Answer: C\nC, aws glue + amazon athena  \nupvoted 1 times\n\n---\n\n#### AimarlLee6 6 months, 1 week ago\nMany comments were not convincing of not using Redshift Spectrum.. the only reason I see it to exclude that option is a Redshift Spectrum MUST have a Redshift Cluster available to start the query to S3.  \nupvoted 1 times\n\n---\n\n#### djeong95 5 months ago\nThis question is actually pretty difficult since both Redshift Spectrum and AWS Glue + Athena could query unstructured data. Redshift Spectrum and Athena actually cost about the same per TB. However, with Athena, you could lower the cost by compressing the data. Glue doesn’t seem to cost that much either.\n\n- [https://aws.amazon.com/redshift/pricing/](https://aws.amazon.com/redshift/pricing/)\n- [https://aws.amazon.com/athena/pricing/](https://aws.amazon.com/athena/pricing/)\n- [https://aws.amazon.com/glue/pricing/](https://aws.amazon.com/glue/pricing/)\n\nupvoted 1 times\n\n---\n\n#### ninomff64 6 months, 2 weeks ago\n\n### Selected Answer: C\nA = S3 Select good for filtering an retrieve subset of data, not enough to analyze  \nB = need a Redshift instance that is expensive  \nC = correct (Glue Data Catalog can help putting some structure to data and Athena is good for both query and analytics, transition to Deep Archive after 1 year)  \nD = see answer B + Intelligent-Tiering not the best option here  \nupvoted 1 times\n\n---\n\n#### nzin4x 6 months, 4 weeks ago\nredshift spectrum vs athena: [https://www.upsolver.com/blog/aws-serverless-redshift-spectrum-athena](https://www.upsolver.com/blog/aws-serverless-redshift-spectrum-athena)\n\nBoth are good solutions to query S3 data. However, redshift spectrum is useful for joining S3 data with other data in Redshift, and if the data is only in S3, it would be preferable to choose athena.  \nupvoted 1 times\n\n---\n\n#### career360guru 7 months, 2 weeks ago\n\n### Selected Answer: C\nC is the right answer as Data needs to be queried and Analyzed.  \nupvoted 2 times\n\n---\n\n#### subburpo 8 months ago\nAthena and aws glue is more cost , so better go with A . and what is the purpose for aws glue here. AWS glue is for ETL purpose unnecessary  \nupvoted 1 times\n\n---\n\n#### Andy16240 8 months, 2 weeks ago\nC correct: S3 copy command in AWS CLI is less operational processes than the batch operation.  \nupvoted 1 times\n\n---\n\n#### u6CrW1IaB 11 months ago\n\n### Selected Answer: C\nIn this particular scenario, using Amazon Athena and AWS Glue Data Catalog might be a better fit due to the large amount of data stored in S3 buckets and growing every day. Athena can query data across an entire S3 bucket or across multiple buckets, which is useful when parsing multiple files and large amounts of data.  \nupvoted 2 times\n\n---\n\n#### chico2023 1 year ago\n\n### Selected Answer: C\nAnswer: C\n\nCriminally tricky question. S3 Select does the same thing as Athena but there are some differences. The key here is \"...a large amount of unstructured data...\"  \nIf wasn't this, S3 Select hands down.  \nupvoted 3 times\n\n---\n\n#### chico2023 1 year ago\nUsing an Olabia to explain the differences between the two:\n\n1. **Query Capability**: Amazon Athena is a fully managed interactive query service that allows you to run SQL queries directly on your data in S3. It supports complex queries, joins, aggregations, and even nested data structures. Athena is designed for ad-hoc querying and analysis of large datasets.\n\n   On the other hand, S3 Select is a feature of Amazon S3 that allows you to retrieve a subset of data from an object using SQL expressions. It is primarily used for selective retrieval of specific data within an object, rather than running complex queries across multiple objects.  \n   upvoted 2 times\n\n---\n\n# Page 266\n\n2. Data Format: Amazon Athena supports various data formats such as CSV, JSON, Parquet, Avro, and more. It can automatically infer the schema of your data or you can provide a schema explicitly. Athena can handle structured, semi-structured, and unstructured data.\n\n   S3 Select, on the other hand, is limited to querying CSV, JSON, and Parquet files. It requires the data to be in a specific format and does not support nested data structures.\n   \n   *upvoted 2 times*\n\n---\n\n**chico2023** 1 year ago\n\n3. Performance: Amazon Athena is optimized for running queries on large datasets and can parallelize the query execution across multiple nodes. It automatically scales resources based on the query complexity and data size, providing fast and efficient query performance.\n\n   S3 Select, on the other hand, is designed for retrieving a subset of data from an object. It can significantly reduce the amount of data transferred over the network and improve query performance by only retrieving the necessary data.\n\n4. Cost: Both Amazon Athena and S3 Select have different pricing models. Amazon Athena charges based on the amount of data scanned by your queries, while S3 Select charges based on the amount of data selected and returned by your queries. The cost will depend on the size of your data and the complexity of your queries.\n\n   *upvoted 3 times*\n\n---\n\n**Jonalb** 1 year ago\n\n**Selected Answer: C**\n\nits a C, true question!\n\n*upvoted 1 times*\n\n---\n\n**NikkyDicky** 1 year, 1 month ago\n\nC for sure\n\n*upvoted 1 times*\n\n---\n\n**johnballs221** 1 year, 2 months ago\n\n**Selected Answer: B**\n\nredshift spectrum can run sql queries directly on s3\n\n*upvoted 1 times*\n\n---\n\n**rxhan** 1 year, 1 month ago\n\nNot the best for cost.\n\n*upvoted 1 times*\n\n---\n\n**msfec** 1 year, 4 months ago\n\n**Selected Answer: C**\n\nC is the best choice for unstructured data\n\n*upvoted 3 times*\n\n---\n\n**God_Is_Love** 1 year, 5 months ago\n\n**Selected Answer: C**\n\nS3 select only to select few parts of the data and here its lot of unstructured data. So A is wrong. Use Athena console to create Glue crawler as referred here -  \n[https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html](https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html)\n\n*upvoted 4 times*\n\n---\n\n**sambb** 1 year, 5 months ago\n\nI think \"semi-structured\" is the right word here, because unstructured can be videos, images or text that has no schema. Assuming that we want to query semi-structured data:\n\nI don't understand why everyone is voting Athena.  \nAthena is fast in certain cases and has more features for aggregation, but we are just asking querying here (and analyzing is very vague).  \nIn terms of cost, S3 select is around 2$ by TB scanned, and Athena is 5$.  \nGlue data catalog brings ease of use, but is not required for querying with athena.  \nS3 select is not limited in the amount of scanned data, only in the row size (1MB)\n\nCan someone explain ?\n\n*upvoted 3 times*\n\n---\n\n# Page 267\n\n"
  },
  {
    "question_number": 93,
    "question_text": "A video processing company wants to build a machine learning (ML) model by using 600 TB of compressed data that is stored as thousands of files in the company's on-premises network attached storage system. The company does not have the necessary compute resources on premises for ML experiments and wants to use AWS.\n\nThe company needs to complete the data transfer to AWS within 3 weeks. The data transfer will be a one-time transfer. The data must be encrypted in transit. The measured upload speed of the company's internet connection is 100 Mbps, and multiple departments share the connection.\n\nWhich solution will meet these requirements MOST cost-effectively?",
    "answers": {
      "A": "Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS.",
      "B": "Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.",
      "C": "Create a VPN connection between the on-premises network attached storage and the nearest AWS Region. Transfer the data over the VPN connection.",
      "D": "Deploy an AWS Storage Gateway file gateway on premises. Configure the file gateway with a destination S3 bucket. Copy the data to the file gateway."
    },
    "correct_answer": "A",
    "explanation": "### Comments\n\n**masetromain** Highly Voted 1 year, 6 months ago\n\n**Selected Answer: A**\n\nThe correct answer is A. Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS.\n\nThis option will meet the requirements to complete the data transfer within 3 weeks, as the Snowball Edge devices can transfer large amounts of data quickly and securely. The data will be encrypted in transit and at rest. The company's internet connection speed is not a bottleneck as the data transfer will happen on the devices and not over the internet.\n\n*upvoted 11 times*\n\n---\n\n**masetromain** 1 year, 6 months ago\n\nOption B is not a cost-effective solution, as setting up and maintaining a 10 Gbps Direct Connect connection can be quite expensive, especially if it’s only needed for a one-time data transfer.\n\nOption C is not a cost-effective solution, as creating a VPN connection between the on-premises storage and the nearest AWS region would require significant networking configuration and maintenance, and would likely be more expensive than using Snowball Edge devices.\n\nOption D is not a cost-effective solution, as deploying an AWS Storage Gateway file gateway on premises would require additional hardware and ongoing maintenance costs, and may not be necessary for a one-time data transfer.\n\n*upvoted 3 times*\n\n---\n\n**ninomf64** Most Recent 6 months, 2 weeks ago\n\n**Selected Answer: A**\n\nA = correct  \nB = takes a month or more to setup DX  \nC = this would take more than 3 weeks for transferring data  \nD = this would take more than 3 weeks for transferring data\n\n*upvoted 2 times*\n\n---\n\n**career360guru** 7 months, 2 weeks ago\n\n**Selected Answer: A**\n\nOption A\n\n*upvoted 1 times*\n\n---\n\n**yorickure** 9 months, 1 week ago\n\n**Selected Answer: A**\n\nwish all the questions were like this. happy days :)\n\n*upvoted 1 times*\n\n---\n\n# Page 268\n\n- **xplusfb** 12 months ago  \n  **Selected Answer: A**  \n  as we know snowball storage optimized NVMe up to 210 TB <3 A is the best and easy answer  \n  upvoted 4 times\n\n  - **xplusfb** 12 months ago  \n    like several sorry for any confusion :)  \n    upvoted 1 times\n\n    - **chikorita** 11 months, 1 week ago  \n      several thanks too :)  \n      upvoted 1 times\n\n- **NikkyDicky** 1 year, 1 month ago  \n  **Selected Answer: A**  \n  A - basic snowball use case  \n  upvoted 1 times\n\n- **Maria2023** 1 year, 1 month ago  \n  **Selected Answer: A**  \n  Given the deadline (3 weeks) and the amount of data I would use Snowball Edge  \n  upvoted 1 times\n\n- **mfsec** 1 year, 4 months ago  \n  **Selected Answer: A**  \n  A obviously  \n  upvoted 3 times\n\n- **God_Is_Love** 1 year, 5 months ago  \n  **Selected Answer: A**  \n  Around 8 devices and snowball (actually a Rectangular box)  \n  Snowball Edge Storage Optimized device is equipped with up to 80 terabytes (TB) of storage capacity, as well as 40 vCPUs and 80 GB of memory for running compute-intensive applications. It also includes an optional GPU for accelerated computing workloads.\n\n  Built-in security features such as tamper-resistant enclosures, an E Ink shipping label, and 256-bit encryption for data at rest and in transit.  \n  upvoted 4 times\n\n- **zozza2023** 1 year, 6 months ago  \n  **Selected Answer: A**  \n  3 weeks + cost effective ==> Snowball Edge Storage  \n  upvoted 1 times\n\n---\n\n# Page 269\n\n"
  },
  {
    "question_number": 95,
    "question_text": "A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platform includes a web front end that is hosted on a fleet of VMs, RabbitMQ to connect the front end to the backend, and a Kubernetes cluster to run a containerized backend system to process the orders. The company does not want to make any major changes to the application.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "answers": {
      "A": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.",
      "B": "Create a custom AWS Lambda runtime to mimic the web server environment. Create an Amazon API Gateway API to replace the front-end web servers. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.",
      "C": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Install Kubernetes on a fleet of different EC2 instances to host the order-processing backend.",
      "D": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up an Amazon Simple Queue Service (Amazon SQS) queue to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend."
    },
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "question_number": 96,
    "question_text": "A solutions architect needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The solutions architect created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose.\n\nThe solutions architect created the following IAM policy and attached it to an IAM role:\n\n```json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"DownloadUpload\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\",\n                \"s3:PutObject\",\n                \"s3:PutObjectAcl\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::BucketName/*\"\n        },\n        {\n            \"Sid\": \"KMSAccess\",\n            \"Action\": [\n                \"kms:Decrypt\",\n                \"kms:Encrypt\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:kms:Region:Account:key/Key ID\"\n        }\n    ]\n}\n```\n\nDuring tests, the solutions architect was able to successfully get existing test objects in the S3 bucket. However, attempts to upload a new object resulted in an error message. The error message stated that the action was forbidden.\n\nWhich action must the solutions architect add to the IAM policy to meet all the requirements?",
    "answers": {
      "A": "kms:GenerateDataKey",
      "B": "kms:GetKeyPolicy",
      "C": "kms:GetPublicKey",
      "D": "kms:Sign"
    },
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "question_number": 109,
    "question_text": "A large company is running a popular web application. The application runs on several Amazon EC2 Linux instances in an Auto Scaling group in a private subnet. An Application Load Balancer is targeting the instances in the Auto Scaling group in the private subnet. AWS Systems Manager Session Manager is configured, and AWS Systems Manager Agent is running on all the EC2 instances.\n\nThe company recently released a new version of the application. Some EC2 instances are now being marked as unhealthy and are being terminated. As a result, the application is running at reduced capacity. A solutions architect tries to determine the root cause by analyzing Amazon CloudWatch logs that are collected from the application, but the logs are inconclusive.\n\nHow should the solutions architect gain access to an EC2 instance to troubleshoot the issue?",
    "answers": {
      "A": "Suspend the Auto Scaling group's HealthCheck scaling process. Use Session Manager to log in to an instance that is marked as unhealthy.",
      "B": "Enable EC2 instance termination protection. Use Session Manager to log in to an instance that is marked as unhealthy.",
      "C": "Set the termination policy to OldestInstance on the Auto Scaling group. Use Session Manager to log in to an instance that is marked as unhealthy.",
      "D": "Suspend the Auto Scaling group's Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy."
    },
    "correct_answer": "D",
    "explanation": "### Comments\n\n- **zozza2023** *Highly Voted* 🟦 1 year, 6 months ago  \n  **Selected Answer: D**  \n  The correct answer is D.  \n  upvoted 10 times\n\n- **God_Is_Love** *Highly Voted* 🟦 1 year, 5 months ago  \n  **Selected Answer: D**  \n  Disabling health check won't let SA know which instance is unhealthy. So A is certainly wrong. D is correct.  \n  upvoted 9 times\n\n- **gofavad926** *Most Recent* 🟦 4 months, 3 weeks ago  \n  **Selected Answer: D**  \n  D, stop the autoscaling process  \n  upvoted 1 times\n\n- **AWSLord32** 6 months, 1 week ago  \n  Why not B? Can the ASG override the EC2 termination protection?  \n  upvoted 3 times\n\n- **career360guru** 7 months, 2 weeks ago  \n  **Selected Answer: D**  \n  Option D  \n  upvoted 1 times\n\n- **severlight** 8 months, 3 weeks ago  \n  **Selected Answer: D**  \n  you can stop auto-scaling processes, here you need to stop termination, you need health checks to know which instance to check  \n  upvoted 2 times\n\n- **venvig** 11 months, 1 week ago  \n  **Selected Answer: D**  \n  If ASG terminates the instances because they are unhealthy there is no way we can login to the instance using session manager or otherwise to investigate the problem. So, suspend the termination.  \n  upvoted 4 times\n\n- **NikkyDicky** 1 year, 1 month ago  \n  **Selected Answer: D**  \n  d of course  \n  upvoted 1 times\n\n---\n\n# Page 306\n\n- **SkyZeroZx1** 1 year, 1 month ago  \n  **Selected Answer: D**  \n  keyword == Auto Scaling group's Terminate process.  \n  upvoted 1 times\n\n- **Alando** 10 months, 4 weeks ago  \n  Have you cleared the exam?  \n  upvoted 1 times\n\n- **mfsec** 1 year, 1 month ago  \n  **Selected Answer: D**  \n  Suspend the Auto Scaling group's Terminate process.  \n  upvoted 2 times\n\n- **zejout** 1 year, 4 months ago  \n  **Selected Answer: D**  \n  Amazon EC2 Auto Scaling stops marking instances unhealthy as a result of EC2 and Elastic Load Balancing health checks. Your custom health check continue to function properly. After you suspend HealthCheck, if you need to, you can manually set the health state of instances in your group and have ReplaceUnhealthy replace them.  \n  Suspending the Terminate process doesn't prevent the successful termination of instances using the force delete option with the delete-auto-scaling-group command.  \n  [https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html](https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html)  \n  [https://docs.aws.amazon.com/systems-manager/latest/userguide/incident-manager.html](https://docs.aws.amazon.com/systems-manager/latest/userguide/incident-manager.html)  \n  We want the health checks to continue failing, just stop terminating to identify root cause  \n  upvoted 4 times\n\n- **testingaws123** 1 year, 5 months ago  \n  **Selected Answer: A**  \n  Answer is A  \n  If you do not want instances to be replaced, we recommend that you suspend the ReplaceUnhealthy and HealthCheck process for individual Auto Scaling groups. For more information, see Suspend and resume a process for an Auto Scaling group.  \n  [https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-health-checks.html](https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-health-checks.html)  \n  upvoted 3 times\n\n- **zejout** 1 year, 4 months ago  \n  That does not solve, it removes the healthcheck process, but also removes the ones that are being marked as unhealthy. The issue now is that one it is tagged as unhealthy they are being terminated. So, any that are already marked get terminated and you just removed the health checks to find remaining. you can't troubleshoot what you don't know.  \n  upvoted 5 times\n\n- **masteromain** 1 year, 6 months ago  \n  **Selected Answer: D**  \n  [https://www.examtopics.com/discussions/amazon/view/51249-exam-aws-certified-solutions-architect-professional-topic-1/](https://www.examtopics.com/discussions/amazon/view/51249-exam-aws-certified-solutions-architect-professional-topic-1/)  \n  The correct answer is D.  \n  In this solution, the architect can suspend the Auto Scaling group's Terminate process, which will prevent the instances marked as unhealthy from being terminated. This will allow the architect to log in to the instance using Session Manager and troubleshoot the issue without losing access to the instance.  \n  upvoted 7 times\n\n- **masteromain** 1 year, 6 months ago  \n  Option A is incorrect because suspending the HealthCheck scaling process will not prevent instances from being terminated.  \n  Option B is incorrect because enabling EC2 instance termination protection will not prevent instances from being terminated by Auto Scaling group.  \n  Option C is incorrect because setting the termination policy to OldestInstance on the Auto Scaling group will not prevent instances marked as unhealthy from being terminated.  \n  upvoted 4 times\n\n---\n\n# Page 307\n\n"
  },
  {
    "question_number": 110,
    "question_text": "A company wants to deploy an AWS WAF solution to manage AWS WAF rules across multiple AWS accounts. The accounts are managed under different OUs in AWS Organizations.\n\nAdministrators must be able to add or remove accounts or OUs from managed AWS WAF rule sets as needed. Administrators also must have the ability to automatically update and remediate noncompliant AWS WAF rules in all accounts.\n\nWhich solution meets these requirements with the **LEAST** amount of operational overhead?",
    "answers": {
      "A": "Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Use an AWS Systems Manager Parameter Store parameter to store account numbers and OUs to manage. Update the parameter as needed to add or remove accounts or OUs. Use an Amazon EventBridge rule to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account.",
      "B": "Deploy an organization-wide AWS Config rule that requires all resources in the selected OUs to associate the AWS WAF rules. Deploy automated remediation actions by using AWS Lambda to fix noncompliant resources. Deploy AWS WAF rules by using an AWS CloudFormation stack set to target the same OUs where the AWS Config rule is applied.",
      "C": "Create AWS WAF rules in the management account of the organization. Use AWS Lambda environment variables to store account numbers and OUs to manage. Update environment variables as needed to add or remove accounts or OUs. Create cross-account IAM roles in member accounts. Assume the roles by using AWS Security Token Service (AWS STS) in the Lambda function to create and update AWS WAF rules in the member accounts.",
      "D": "Use AWS Control Tower to manage AWS WAF rules across accounts in the organization. Use AWS Key Management Service (AWS KMS) to store account numbers and OUs to manage. Update AWS KMS as needed to add or remove accounts or OUs. Create IAM users in member accounts. Allow AWS Control Tower in the management account to use the access key and secret access key to create and update AWS WAF rules in the member accounts."
    },
    "correct_answer": "D",
    "explanation": "### masetromain 1 year, 6 months ago\n\n**Selected Answer: A**\n\nThe correct answer is A.\n\nIn this solution, AWS Firewall Manager is used to manage AWS WAF rules across accounts in the organization. An AWS Systems Manager Parameter Store parameter is used to store account numbers and OUs to manage. This parameter can be updated as needed to add or remove accounts or OUs. An Amazon EventBridge rule is used to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account. This solution allows for easy management of AWS WAF rules across multiple accounts with minimal operational overhead.\nupvoted 18 times\n\n### masetromain 1 year, 6 months ago\n\nOption B does not meet the requirement of being able to add or remove accounts or OUs from managed AWS WAF rule sets as needed.\n\nOption C is not the best approach as it requires manual configuration of the cross-account IAM roles and assume-role calls in the Lambda function, increasing the operational overhead.\n\nOption D does not meet the requirement of providing a centralized management console to manage the WAF rules across multiple accounts.\nupvoted 3 times\n\n### Untamables 1 year, 6 months ago\n\n**Selected Answer: A**\n\n[https://aws.amazon.com/solutions/implementations/automations-for-aws-firewall-manager/](https://aws.amazon.com/solutions/implementations/automations-for-aws-firewall-manager/)\nupvoted 6 times\n\n### career360guru 7 months, 2 weeks ago\n\n**Selected Answer: A**\n\nOption A\nupvoted 1 times\n\n### venvig 11 months, 1 week ago\n\n**Selected Answer: A**\n\n\n---\n\n# Page 308\n\nAWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. Firewall Manager supports a wide variety of services, including:\n\n- AWS WAF\n- VPC Security Groups\n- AWS Network Firewall\n- Route53 DNS Firewall\n- AWS Shield Advanced\n- Palo Alto Cloud Next-generation firewalls\n\nThe Prerequisites are: AWS Organizations + AWS Config.\n\n*upvoted 5 times*\n\n---\n\n### CuteRunRun 11 months, 3 weeks ago\n\n**Selected Answer: A**\n\nI have to say A is right.  \nPlease take a look at this:  \n[https://aws.amazon.com/blogs/security/centrally-manage-aws-waf-api-v2-and-aws-managed-rules-at-scale-with-firewall-manager/](https://aws.amazon.com/blogs/security/centrally-manage-aws-waf-api-v2-and-aws-managed-rules-at-scale-with-firewall-manager/)\n\n*upvoted 2 times*\n\n---\n\n### NikkyDicky 1 year, 1 month ago\n\n**Selected Answer: A**\n\nA is a good option\n\n*upvoted 1 time*\n\n---\n\n### SkyZeroX 1 year, 1 month ago\n\n**Selected Answer: A**\n\nKeyword == AWS Firewall Manager\n\n*upvoted 2 times*\n\n---\n\n### tromyunpak 1 year, 2 months ago\n\nThe correct answer is A  \n[https://docs.aws.amazon.com/solutions/latest/automations-for-aws-firewall-manager/architecture-overview.html](https://docs.aws.amazon.com/solutions/latest/automations-for-aws-firewall-manager/architecture-overview.html)\n\n*upvoted 2 times*\n\n---\n\n### rbm2023 1 year, 2 months ago\n\n**Selected Answer: A**\n\nThis is a complex question. But I voted A because the Firewall manager seems to be the correct way to centralize the rules across accounts. Below are some interesting references I could find:\n\n- [https://catalog.us-east-1.prod.workshops.aws/workshops/4cbae3b-ceba-48e3-bd56-eca138f7a66c/en-US](https://catalog.us-east-1.prod.workshops.aws/workshops/4cbae3b-ceba-48e3-bd56-eca138f7a66c/en-US)\n- [https://aws.amazon.com/blogs/security/use-aws-firewall-manager-vpc-security-groups-to-protect-applications-hosted-on-ec2-instances/](https://aws.amazon.com/blogs/security/use-aws-firewall-manager-vpc-security-groups-to-protect-applications-hosted-on-ec2-instances/)\n- [https://aws.amazon.com/blogs/security/automatically-updating-aws-waf-rule-in-real-time-using-amazon-eventbridge/](https://aws.amazon.com/blogs/security/automatically-updating-aws-waf-rule-in-real-time-using-amazon-eventbridge/)\n\n*upvoted 2 times*\n\n---\n\n### mfsec 1 year, 4 months ago\n\n**Selected Answer: A**\n\nUse AWS Firewall Manager to manage AWS WAF rules\n\n*upvoted 1 time*\n\n---\n\n### God_Is_Love 1 year, 5 months ago\n\n**Selected Answer: A**\n\nNot D, KMS to store account numbers?\n\n*upvoted 1 time*\n\n---\n\n### zozza2023 1 year, 6 months ago\n\n**Selected Answer: A**\n\nThe correct answer is A.\n\n*upvoted 2 times*\n\n---\n\n# Page 309\n\n"
  },
  {
    "question_number": 111,
    "question_text": "### Topic 1\n\nA solutions architect is auditing the security setup of an AWS Lambda function for a company. The Lambda function retrieves the latest changes from an Amazon Aurora database. The Lambda function and the database run in the same VPC. Lambda environment variables are providing the database credentials to the Lambda function.\n\nThe Lambda function aggregates data and makes the data available in an Amazon S3 bucket that is configured for server-side encryption with AWS KMS managed encryption keys (SSE-KMS). The data must not travel across the Internet. If any database credentials become compromised, the company needs a solution that minimizes the impact of the compromise.\n\n**What should the solutions architect recommend to meet these requirements?**",
    "answers": {
      "C": "Save the database credentials in AWS Systems Manager Parameter Store. Set up password rotation on the credentials in Parameter Store. Change the IAM role for the Lambda function to allow the function to access Parameter Store. Modify the Lambda function to retrieve the credentials from Parameter Store. Deploy a gateway VPC endpoint for Amazon S3 in the VPC.",
      "A": "Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Deploy a gateway VPC endpoint for Amazon S3 in the VPC.",
      "B": "Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Enforce HTTPS on the connection to Amazon S3 during data transfers.",
      "D": "Save the database credentials in AWS Secrets Manager. Set up password rotation on the credentials in Secrets Manager. Change the IAM role for the Lambda function to allow the function to access Secrets Manager. Modify the Lambda function to retrieve the credentials from Secrets Manager. Enforce HTTPS on the connection to Amazon S3 during data transfers.\n\n---"
    },
    "correct_answer": "D",
    "explanation": "**Comments:**\n\n- **zozza2023** (Highly Voted) - 1 year, 6 months ago  \n  **Selected Answer: A**  \n  A little bit confused between A and D but as said by others members D doesn't address the question of \"data must not travel across the Internet\" => A is the answer  \n  _upvoted 18 times_\n\n- **MikelH93** (Highly Voted) - 1 year, 3 months ago  \n  **Selected Answer: A**  \n  B and D are out because you need the VPC endpoints.  \n  C is out because you cannot enable rotation in Parameter Store  \n  (https://docs.aws.amazon.com/secretsmanager/latest/userguide/integrating_parameterstore.html)  \n  _upvoted 6 times_\n\n- **AWSPRO1234** (Most Recent) - 4 months, 1 week ago  \n  **Selected Answer: A**  \n  Key is data must not travel across the internet mean use VPC gateway  \n  _upvoted 1 time_\n\n- **gofavad0926** - 4 months, 3 weeks ago  \n  **Selected Answer: A**  \n  A, \"data must not travel across the internet\". This setup ensures internal network use only, meeting the security and networking requirements efficiently  \n  _upvoted 1 time_\n\n- **a54b16f** - 5 months, 1 week ago  \n  **Selected Answer: A**  \n  The data must not travel across the Internet.  \n  _upvoted 2 times_\n\n- **8608f25** - 5 months, 3 weeks ago  \n  **Selected Answer: D**  \n  Option D offers a comprehensive solution by leveraging AWS Secrets Manager for storing and automatically rotating database credentials, which directly addresses the concern of minimizing the impact if credentials become compromised. Changing the Lambda function to retrieve credentials from Secrets Manager enhances security by not storing credentials within environment variables. Enforcing HTTPS for S3 data transfers ensures the data in transit is encrypted. While deploying a gateway VPC endpoint for S3 (as mentioned in other options) is a best practice to keep data secure.\n\n\n---\n\n# Page 310\n\ntraffic within the AWS network, enforcing HTTPS also contributes to securing data transfers without explicitly stating the need to avoid Internet travel. Secrets Manager inherently provides secure access to secrets without needing to travel across the Internet when accessed from AWS services within the same region.\n\nOption A does not address the requirement for securing and rotating database credentials stored as Lambda environment variables.\nupvoted 1 times\n\ncareer360guru 7 months, 2 weeks ago\n\n**Selected Answer: A**\n\nAnswer is A as S3 VPC is endpoint is needed to avoid data going over internet.\nupvoted 1 times\n\ntask_7 10 months, 2 weeks ago\n\n**Selected Answer: D**\n\nAWS Secrets Manager is meant for this job, why go with any other option\nupvoted 1 times\n\ntask_7 10 months, 2 weeks ago\n\nMy bad A is\nD is not addressing this point\nThe data must not travel across the Internet\nupvoted 6 times\n\nCuteRunRun 11 months, 3 weeks ago\n\nI prefer A\nupvoted 2 times\n\nJonalb 1 year ago\n\n**Selected Answer: A**\n\nA\n\n[https://aws.amazon.com/pt/blogs/database/iam-role-based-authentication-to-amazon-aurora-from-serverless-applications/](https://aws.amazon.com/pt/blogs/database/iam-role-based-authentication-to-amazon-aurora-from-serverless-applications/)\nupvoted 3 times\n\nJonalb 1 year, 1 month ago\n\n[https://docs.aws.amazon.com/pt_br/secretsmanager/latest/userguide/vpc-endpoint-overview.html](https://docs.aws.amazon.com/pt_br/secretsmanager/latest/userguide/vpc-endpoint-overview.html)\nupvoted 1 times\n\nNikkeyDicky 1 year, 1 month ago\n\n**Selected Answer: A**\n\nA for sure\nupvoted 1 times\n\nrbm2023 1 year, 2 months ago\n\n**Selected Answer: A**\n\nI was about to chose D however just enforcing the HTTP will not avoid the data to travel across internet. You will need the option where the gateway VPC endpoint is deployed for access the S3. The answer is A.\nA will also solve the issue related to authenticate the lambda to aurora without needing to store passwords, refer to - [https://aws.amazon.com/blogs/database/iam-role-based-authentication-to-amazon-aurora-from-serverless-applications/](https://aws.amazon.com/blogs/database/iam-role-based-authentication-to-amazon-aurora-from-serverless-applications/)\nupvoted 1 times\n\nOCHT 1 year, 3 months ago\n\n**Selected Answer: D**\n\nHowever, Option A is not the best choice for the given scenario because:\n\nIt doesn't address the requirement to minimize the impact of compromised database credentials. IAM database authentication eliminates traditional user credentials, but it doesn't implement password rotation for the remaining IAM credentials.\n\nWhile the VPC endpoint keeps traffic within the AWS network, it doesn't enforce encryption during data transfers to Amazon S3.\n\nOption D, on the other hand, addresses both the requirement of minimizing the impact of compromised credentials through password rotation using AWS Secrets Manager and ensuring encrypted data transfers to Amazon S3 by enforcing HTTPS. That's why Option D is the better choice for this scenario.\nupvoted 2 times\n\nrbm2023 1 year, 2 months ago\n\nI was also choosing D however just enforcing the HTTP will not avoid the data to travel across internet. You will need the option where the gateway VPC endpoint is deployed for access the S3. The answer is A\nupvoted 3 times\n\nmfsec 1 year, 4 months ago\n\n**Selected Answer: A**\n\nA for sure due to VPC endpoints.\nupvoted 2 times\n\n---\n\n# Page 311\n\n- **kiran15789** 1 year, 5 months ago\n\n  **Selected Answer: A**\n\n  I had a strong opinion about D but after reading and doing some research convince about A  \n  [https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.IAMDBAuth.html](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.IAMDBAuth.html)  \n  upvoted 3 times\n\n- **God_Is_Love** 1 year, 5 months ago\n\n  **Selected Answer: A**\n\n  Key is - Data must not travel on the internet. Only S3 VPC Endpoints have this feature.\n\n  A VPC endpoint allows you to connect privately to S3 from within your Amazon Virtual Private Cloud (VPC) without the need for an internet gateway, NAT device, or VPN connection. Instead, the endpoint provides a direct and secure connection between your VPC and S3 over the Amazon network backbone.  \n  upvoted 4 times\n\n---\n\n# Page 312\n\n"
  },
  {
    "question_number": 114,
    "question_text": "A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due to heavy ingestion and it frequently runs out of storage.\n\nThe company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should include the following attributes:\n\n- Managed AWS services to minimize operational complexity.\n- A buffer that automatically scales to match the throughput of data and requires no ongoing administration.\n- A visualization tool to create dashboards to observe events in near-real time.\n- Support for semi-structured JSON data and dynamic schemas.\n\nWhich combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.)",
    "answers": {
      "A": "Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.",
      "B": "Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events.",
      "C": "Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.",
      "D": "Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.\n\nE. Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards."
    },
    "correct_answer": "AD",
    "explanation": "### Comments\n\n#### God_Is_Love - Highly Voted - 1 year, 5 months ago\n\n**Selected Answer: AD**\n\nAmazon Kinesis Data Firehose (A) allows you to buffer events in two ways: through buffering size or buffering time. With buffering size, you can configure the maximum size of the buffer in MB or the maximum number of records in the buffer. Once the buffer is full, it will automatically deliver the data to the destination.\n\nAmazon ES (D) has its ability to receive events from various sources in real-time. Amazon ES can ingest data from a variety of sources, such as Amazon Kinesis Data Firehose, Amazon CloudWatch Logs, and Amazon S3, making it a powerful tool for organizations looking to analyze and visualize real-time streaming data. (Kibana dashboards)\n\n*upvoted 14 times*\n\n#### oCH - Highly Voted - 1 year, 4 months ago\n\n**Selected Answer: AD**\n\nOption B includes using an Amazon Kinesis data stream to buffer events, which is a valid solution for a streaming data use case. However, it requires more ongoing administration compared to using Amazon Kinesis Data Firehose, which is a fully managed service. Additionally, the use of Amazon Kinesis Data Firehose allows the company to take advantage of built-in data transformation and processing capabilities, which can reduce the amount of code required to implement the solution. Therefore, I selected option A over option B as it better meets the requirement of minimizing operational complexity.\n\n*upvoted 12 times*\n\n#### Smart - Most Recent - 3 months ago\n\n**Selected Answer: AD**\n\n\"A buffer that automatically scales to match the throughput of data and requires no ongoing administration.\"\n\nI think buffer, here, means a solution that will reliably hold information for further successful processing. I don't think it means to buffer and batch process the events so I don't agree with other people's comments in regards to buffer.\n\nThat said, my concern is with \"automatically scales to match the throughput of data\". Firehose does it automatically. Kinesis can also do automatically if on-demand mode is chosen.\n\nAlso, \"Support for semi-structured JSON data and dynamic schemas.\" Dynamic Schemas? Firehose or Data stream don't do that. Firehose does do dynamic partitioning and JSON deserializing. I guess that's what the question meant?\n\n*upvoted 1 times*\n\n#### TonytheTiger - 4 months ago\n\n**Selected Answer: AD**\n\n---\n\n# Page 318\n\nOption A Not Option B - Amazon Data Firehose buffers incoming streaming data in memory to a certain size (buffering size) and for a certain period of time (buffering interval) before delivering it to the specified destinations.\n\n[https://docs.aws.amazon.com/firehose/latest/dev/buffering-hints.html](https://docs.aws.amazon.com/firehose/latest/dev/buffering-hints.html)  \nupvoted 1 times\n\n---\n\n### Dgix 4 months, 3 weeks ago\n\n**Selected Answer: AD**\n\nOn second thought: A because B requires manual shard configuration.  \nupvoted 1 times\n\n---\n\n### Dgix 4 months, 3 weeks ago\n\n**Selected Answer: BE**\n\nAlso, Streams is more real-time.  \nupvoted 1 times\n\n---\n\n### Dgix 4 months, 3 weeks ago\n\n**Selected Answer: BD**\n\nB rather than A because B integrates the lambda functionality for transformation of the data, which must be done as an added step in A, thereby increasing operational overhead.  \nupvoted 1 times\n\n---\n\n### 8608f25 5 months, 3 weeks ago\n\n**Selected Answer: AD**\n\nA. Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events. Amazon Kinesis Data Firehose provides a fully managed service for effortlessly loading streaming data into AWS services such as Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk. It scales automatically to match the throughput of data and requires no ongoing administration. AWS Lambda can be used in conjunction with Kinesis Data Firehose to process and transform the data before it's loaded into the destination, supporting dynamic schemas and semi-structured JSON data. Additionally, Amazon Kinesis Data Firehose has built-in buffering capabilities and can be used to observe events in near-real time, making it a more appropriate choice for the given scenario.  \nupvoted 2 times\n\n---\n\n### 8608f25 5 months, 3 weeks ago\n\nD. Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards. Amazon Elasticsearch Service (Amazon ES) is a managed service that makes it easy to deploy, secure, operate, and scale Elasticsearch to search, analyze, and visualize data in real-time. Kibana is an open-source visualization tool designed to work with Elasticsearch, providing powerful and easy-to-use features to create dashboards that can visualize data in near-real-time.  \nupvoted 1 times\n\n---\n\n### AimarLeo 6 months, 1 week ago\n\nElasticSearch is the ex name of new OpenSearch  \nupvoted 2 times\n\n---\n\n### ninomfr64 6 months, 2 weeks ago\n\n**Selected Answer: BD**\n\nI choose Data Stream (KDS) over Data Firehose (KDF) in this scenario:\n\n- KDS allows you to store events up to 1 year, allowing to achieve buffering with no constraints on size and with a very large time limit. KDS support on-demand capacity mode\n- KDF transport mechanism is based on buffering, but here buffering is limited on size (max 128MiB) and time (up to 900 sec)\n\nupvoted 1 times\n\n---\n\n### career360guru 7 months, 2 weeks ago\n\n**Selected Answer: AD**\n\nA and D  \nupvoted 1 times\n\n---\n\n### AMohanty 11 months ago\n\nBD  \nQuestion states near-Real time  \nThats the differentiating factor between Kinesis data stream and Firehose\n\nI would go for B and D  \nupvoted 2 times\n\n---\n\n### chikorita 10 months, 4 weeks ago\n\nbut about \"* Managed AWS services to minimize operational complexity.\"  \nI believe Kinesis Firehose is managed solution whereas DataStream required operational overhead\n\n---\n\n### NikkyDicky 1 year, 1 month ago\n\n**Selected Answer: AD**\n\nAD for unstructured data  \nupvoted 1 times\n\n---\n\n### mfsec 1 year, 4 months ago\n\n\n---\n\n# Page 319\n\n**Selected Answer: AD**\n\nAD is my vote  \nupvoted 1 times\n\n---\n\n### Zek 1 year, 5 months ago\n\nAD seems correct. [https://www.examtopics.com/discussions/amazon/view/47625-exam-aws-certified-solutions-architect-professional-topic-1/](https://www.examtopics.com/discussions/amazon/view/47625-exam-aws-certified-solutions-architect-professional-topic-1/)  \nupvoted 1 times\n\n---\n\n### zhangyu20000 1 year, 6 months ago\n\nAD are correct  \nupvoted 1 times\n\n---\n\n### maestroiman 1 year, 6 months ago\n\n**Selected Answer: AD**\n\nThe combination of components that will enable the company to create a monitoring solution that will satisfy these requirements is:\n\n- **A. Use Amazon Kinesis Data Firehose to buffer events.** This service can automatically scale to match the throughput of data, and it requires no ongoing administration. With Firehose, it's possible to use a Lambda function to process and transform events as well as to store them in other services like S3 or Redshift.\n\n- **D. Configure Amazon Elasticsearch Service (Amazon ES) to receive events.** With Amazon Elasticsearch Service, it's possible to create an index for the events, making them searchable and queryable. This service is a fully managed service so it minimizes operational complexity. Also, it's possible to use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.  \n  upvoted 7 times\n\n---\n\n### maestroiman 1 year, 6 months ago\n\nOption B: Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events. is incorrect because Kinesis Data Stream is a different service than Kinesis Data Firehose and does not have the buffer feature.\n\nOption C: Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards. is incorrect because Amazon Aurora is a relational database service and does not support JSON data or dynamic schemas.\n\nOption E: Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards. is incorrect because Amazon Neptune is a graph database service and does not support JSON data or dynamic schemas.  \nupvoted 4 times\n\n---\n\n### Sarutobi 1 year, 5 months ago\n\nWe use the Kinesis data stream specifically for its capability to store data \"aka buffer events\". Firehose also has some resemblance of this feature but is more of a transportation service.  \nupvoted 4 times\n\n---\n\n### jpa8300 7 months ago\n\nWhat does it mean? You choose Kinesis data stream over Forehose?  \nupvoted 1 times\n\n---\n\n# Page 320\n\n"
  },
  {
    "question_number": 115,
    "question_text": "A team collects and routes behavioral data for an entire company. The company runs a Multi-AZ VPC environment with public subnets, private subnets, and an internet gateway. Each public subnet also contains a NAT gateway. Most of the company's applications read from and write to Amazon Kinesis Data Streams. Most of the workloads run in private subnets.\n\nA solutions architect must review the infrastructure. The solution architect needs to reduce costs and maintain the function of the applications. The solutions architect uses Cost Explorer and notices that the cost in the EC2-Other category is consistently high. A further review shows that NatGateway-Bytes charges are increasing the cost in the EC2-Other category.\n\nWhat should the solutions architect do to meet these requirements?",
    "answers": {
      "A": "Enable VPC Flow Logs. Use Amazon Athena to analyze the logs for traffic that can be removed. Ensure that security groups are blocking traffic that is responsible for high costs.",
      "B": "Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that applications have the correct IAM permissions to use the interface VPC endpoint.",
      "C": "Enable VPC Flow Logs and Amazon Detective. Review Detective findings for traffic that is not related to Kinesis Data Streams. Configure security groups to block that traffic.",
      "D": "Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that the VPC endpoint policy allows traffic from the applications."
    },
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "question_number": 116,
    "question_text": "A retail company has an on-premises data center in Europe. The company also has a multi-Region AWS presence that includes the eu-west-1 and us-east-1 Regions. The company wants to be able to route network traffic from its on-premises infrastructure into VPCs in either of those Regions. The company also needs to support traffic that is routed directly between VPCs in those Regions. No single points of failure can exist on the network.\n\nThe company already has created two 1 Gbps AWS Direct Connect connections from its on-premises data center. Each connection goes into a separate Direct Connect location in Europe for high availability. These two locations are named DX-A and DX-B, respectively. Each Region has a single AWS Transit Gateway that is configured to route all inter-VPC traffic within that Region.\n\nWhich solution will meet these requirements?",
    "answers": {
      "A": "Create a private VIF from the DX-A connection into a Direct Connect gateway. Create a private VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with the Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.",
      "B": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Associate the eu-west-1 transit gateway with this Direct Connect gateway. Create a transit VIF from the DX-B connection into a separate Direct Connect gateway. Associate the us-east-1 transit gateway with this separate Direct Connect gateway. Peer the Direct Connect gateways with each other to support high availability and cross-Region routing.",
      "C": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Configure the Direct Connect gateway to route traffic between the transit gateways.",
      "D": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.\n\n---"
    },
    "correct_answer": "A",
    "explanation": "### Comments\n\n**God_Is_Love** *Highly Voted* 1 year, 5 months ago\n\n**Selected Answer: D**\n\n[https://docs.aws.amazon.com/images/whitepapers/latest/hybrid-connectivity/images/dx-dxgw-transit-gateway-multi-region-public-vif.png](https://docs.aws.amazon.com/images/whitepapers/latest/hybrid-connectivity/images/dx-dxgw-transit-gateway-multi-region-public-vif.png)\n\nB is wrong as it says, two DX Gateways contradictory  \nC is wrong as it says to configure DXG to route traffic. In fact, Transit gateway peering needs to be done between two transit gateways of each region.  \nA is wrong because Private VIF is not mentioned in the config of the question. Public VIF is correct (Transit public VIF)  \nIf you are using a single DX Gateway\n\nupvoted 14 times\n\n---\n\n**God_Is_Love** 1 year, 5 months ago\n\nWhichever option has this text is correct - \"Peer the transit gateways with each other to support cross-Region routing\"\n\nupvoted 4 times\n\n---\n\n**teo1257** *Most Recent* 3 months, 1 week ago\n\nIt can be both A or D based on AWS documentation: [https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/hybrid-network-connections.html](https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/hybrid-network-connections.html)\n\nupvoted 1 times\n\n---\n\n**Dgix 4 months, 2 weeks ago**\n\n**Selected Answer: D**\n\nDon't let \"No single points of failure can exist on the network\" mislead you into thinking that you need two DCGWs. DCGWs are not part of the region they connect to. Therefore, no SPOF translates to a double DC connection to a single DCGW. Hence, D.\n\nupvoted 1 times\n\n---\n\n**gofavad926 4 months, 3 weeks ago**\n\n**Selected Answer: D**\n\nD, this approach ensures high availability and robust network connectivity across the specified AWS regions and the on-premises data center.\n\nupvoted 1 times\n\n---\n\n# Page 324\n\n- **_Jassybanga_** 5 months, 1 week ago  \n  Answer D - As per from AWS  \n  [https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-more-than-3.html](https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-more-than-3.html)  \n  upvoted 2 times\n\n- **career360guru** 7 months, 2 weeks ago  \n  **Selected Answer: D**  \n  Choice is between C and D. Better the two D is the right option.  \n  upvoted 1 times\n\n- **subburpo** 8 months ago  \n  D is correct ref architecture  \n  [https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html](https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html)  \n  upvoted 3 times\n\n- **shaaam80** 8 months ago  \n  **Selected Answer: D**  \n  Answer D. Peer the transit gateways for cross-region routing.  \n  upvoted 1 times\n\n- **severlight** 8 months, 3 weeks ago  \n  **Selected Answer: D**  \n  to connect to transit gateways through the dx gateway you should use transit VIF  \n  upvoted 1 times\n\n- **fffavroeto** 11 months ago  \n  I agree 'D' is a good answer to the problem, but isn't the DXGW a single point of failure?  \n  Question says \"No single points of failure can exist on the network.\"  \n  upvoted 2 times\n\n- **NikkyDicky** 1 year, 1 month ago  \n  **Selected Answer: D**  \n  it's D  \n  upvoted 1 times\n\n- **happystrawberry** 1 year, 2 months ago  \n  Would it be C for the answer? A Direct Connect gateway supports communication between attached transit virtual interfaces and associated transit gateways only and may enable a virtual private gateway to another virtual private gateway.  \n  [https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-transit-gateways.html](https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-transit-gateways.html)  \n  upvoted 1 times\n\n  - **happystrawberry** 1 year, 2 months ago  \n    Actually, D is a proper answer.  \n    upvoted 1 times\n\n- **rbm2023** 1 year, 2 months ago  \n  **Selected Answer: D**  \n  I agree with option D  \n  Refer to the diagram below which explains in detail the use of Transit VIF and Public VIF. Also demonstrates the necessity for peering the transit gateways to allow the cross-region routing.  \n  [https://docs.aws.amazon.com/images/whitepapers/latest/hybrid-connectivity/images/dx-dxgw-transit-gateway-multi-region-public-vif.png](https://docs.aws.amazon.com/images/whitepapers/latest/hybrid-connectivity/images/dx-dxgw-transit-gateway-multi-region-public-vif.png)  \n  The only options that are using the cross-region routing are A and D. Option A mentions the use of Private VIF and not the Transit VIF. Hence A is incorrect.  \n  upvoted 4 times\n\n  - **rbm2023** 1 year, 2 months ago  \n    Refer to the following article  \n    [https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html](https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html)  \n    upvoted 3 times\n\n- **dev112233xx** 1 year, 3 months ago  \n  **Selected Answer: D**  \n  Transit VIF required to connect to Transit Gateway, and Transit peering is required to connect multi regions...  \n  Here is the full diagram:  \n  [https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html](https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html)  \n  upvoted 3 times\n\n- **mfsec** 1 year, 4 months ago  \n  **Selected Answer: D**  \n  D is the answer  \n  upvoted 2 times\n\n---\n\n# Page 325\n\n### zejout 1 year, 4 months ago\n\n**Selected Answer: D**\n\nThis model is constructed of the following:\n\n- Multi AWS Regions\n- Dual Direct Connect connections to independent DX locations\n- Single on-premises data center with dual connections to AWS\n- AWS DXGW with AWS Transit Gateway\n- High scale of VPCs per Region\n\n[https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html](https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html)\n\nupvoted 2 times\n\n### Sarutobi 1 year, 5 months ago\n\n**Selected Answer: D**\n\nYeah, a single DX-GW tied to TGW on different regions that further connect to the VPCs on those regions.\n\nupvoted 2 times\n\n---\n\n# Page 326\n\n"
  },
  {
    "question_number": 117,
    "question_text": "A company is running an application in the AWS Cloud. The company's security team must approve the creation of all new IAM users. When a new IAM user is created, all access for the user must be removed automatically. The security team must then receive a notification to approve the user. The company has a multi-Region AWS CloudTrail trail in the AWS account.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
    "answers": {
      "A": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser.",
      "B": "Configure CloudTrail to send a notification for the CreateUser event to an Amazon Simple Notification Service (Amazon SNS) topic.",
      "C": "Invoke a container that runs in Amazon Elastic Container Service (Amazon ECS) with AWS Fargate technology to remove access.",
      "D": "Invoke an AWS Step Functions state machine to remove access.\n\nE. Use Amazon Simple Notification Service (Amazon SNS) to notify the security team.\n\nF. Use Amazon Pinpoint to notify the security team."
    },
    "correct_answer": "ADE",
    "explanation": "### Comments\n\n#### God_Is_Love (Highly Voted) - 1 year, 5 months ago\n\n**Selected Answer: ADE**\n\nEvent Bus (EventBridge) system to receive event notification (Option A). Step function can get triggered with workflow of doing steps like removing access and sending email etc. (Option D, E)\n\nEventBridge enables you to create event rules that match events from different sources, such as AWS services, SaaS applications, custom applications, and other AWS accounts. Once an event rule is triggered, EventBridge can route the event to one or more targets, such as AWS Lambda functions, Amazon SNS topics, Amazon SQS queues, or custom HTTP endpoints.\n\nAWS Step Functions supports several AWS services, such as AWS Lambda, Amazon Simple Notification Service (SNS), and Amazon Simple Queue Service (SQS). You can use these services to trigger actions and pass data between steps in your state machine.\n\nPinpoint is chat system which question did not ask, F is wrong. Not C as\n\n*upvoted 12 times*\n\n#### Jay_2pt0_1 - 1 year, 3 months ago\n\nI agree with this.\n\n*upvoted 1 times*\n\n#### hobokabob - 1 year, 4 months ago\n\nthis explanation makes sense to me.\n\n*upvoted 1 times*\n\n#### TonytheTiger (Most Recent) - 4 months ago\n\n**Selected Answer: ACE**\n\nOption ADE: Most people agree with option AE. There can be situations where human intervention is required before the workflow can progress. For example, approving a substantial credit increase may require human approval\n\n[https://docs.aws.amazon.com/step-functions/latest/dg/use-cases-security-automation.html](https://docs.aws.amazon.com/step-functions/latest/dg/use-cases-security-automation.html)\n\n*upvoted 1 times*\n\n#### 24GeI - 4 months, 3 weeks ago\n\nWhy not BCE? or ACE?\n\nHow to use Step Function to remove permission?\n\n*upvoted 1 times*\n\n#### dankositzke - 5 months, 3 weeks ago\n\nPoorly constructed answer choices, but ADE is the least worst option.\n\n*upvoted 1 times*\n\n#### zanshieh - 6 months ago\n\n**Selected Answer: ADE**\n\nI picked ADE. EventBridge, Lambda / Step Function, and SNS are required.  \nBDE: No. CloudTrail can't trigger Step Function directly.\n\n---\n\n# Page 327\n\nABE: No. This solution can't remove the user access automatically.  \nChoosing B alone without A can't directly trigger Lambda / Step functions to remove the user access. C can't compare with D. F is not relevant.  \nupvoted 1 times\n\n---\n\n### AWSLord32 6 months, 1 week ago\n\n**Selected Answer: BDE**\n\nEventbridge is not needed. Cloudtrail can send notifications to SNS directly\n\n[https://docs.aws.amazon.com/awscloudtrail/latest/userguide/configure-sns-notifications-for-cloudtrail.html](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/configure-sns-notifications-for-cloudtrail.html)  \nupvoted 3 times\n\n---\n\n### AWSLord32 6 months, 1 week ago\n\nAlso, if you select ADE how would the event ever trigger SNS to send the notification?  \nupvoted 2 times\n\n---\n\n### farthos 2 months, 1 week ago\n\nWhat do you mean? SNS topic is one of the (many) allowed targets for EventBridge.  \n[https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html)\n\nRegarding \"Eventbridge is not needed\" - it's only true for notifications because CloudTrail integrates with SNS. CloudTrail alone cannot trigger any automation tools like Lambda or Step Function. That's why EventBridge is better in this case. You can add both targets to the same rule.  \nupvoted 1 times\n\n---\n\n### bjexamprep 6 months, 2 weeks ago\n\n**Selected Answer: ACE**\n\nStep function is a process/workflow orchestrator. Usually process/workflow orchestrator doesn't do actual task, cause the objective of a orchestrator is to maintain the stage of a process/workflow. Instead, the orchestrator call a service to complete the task and update the stage. So the task of removing access should be done by a Lambda function. Since lambda function is not an option, the only applicable option is C, while ECS introduces too much administration overhead, and is a very bad choice for this task.  \nupvoted 1 times\n\n---\n\n### career360guru 7 months, 2 weeks ago\n\n**Selected Answer: ADE**\n\nA, D and E  \nupvoted 1 times\n\n---\n\n### NikkyDicky 1 year, 1 month ago\n\n**Selected Answer: ADE**\n\nADE. have to assume the step function calls lambda or some such to actually perform action  \nupvoted 1 times\n\n---\n\n### Maria2023 1 year, 2 months ago\n\n**Selected Answer: ADE**\n\nI've chosen the EventBridge option (A) because I really was not able to find a way to set Cloudtrail to trigger SNS on it's own. The rest 2 are common sense  \nupvoted 1 times\n\n---\n\n### AWSLord32 6 months, 1 week ago\n\nHere you go [https://docs.aws.amazon.com/awscloudtrail/latest/userguide/configure-sns-notifications-for-cloudtrail.html](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/configure-sns-notifications-for-cloudtrail.html)  \nupvoted 1 times\n\n---\n\n### OCHT 1 year, 4 months ago\n\n**Selected Answer: ADE**\n\nA. Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser.\n\nB. Configure CloudTrail to send a notification for the CreateUser event to an Amazon Simple Notification Service (Amazon SNS) topic.\n\nE. Use Amazon Simple Notification Service (Amazon SNS) to notify the security team.  \nupvoted 2 times\n\n---\n\n### OCHT 1 year, 4 months ago\n\nBy creating an Amazon EventBridge rule, the company can detect the CreateUser event in CloudTrail and use it to trigger actions such as sending notifications or invoking AWS Lambda functions.\n\nConfiguring CloudTrail to send a notification for the CreateUser event to an Amazon SNS topic allows the security team to receive a notification whenever a new IAM user is created.\n\nUsing Amazon SNS, the security team can receive the notification and approve or deny the new IAM user creation. If the security team denies the creation, access can be automatically removed using AWS Lambda or AWS Step Functions.\n\nTherefore, these three steps will allow the company to meet its requirements for user creation approval and access removal.  \nupvoted 2 times\n\n---\n\n### mfsec 1 year, 4 months ago\n\n\n---\n\n# Page 328\n\n- **[Removed] 1 year, 5 months ago**\n\n  **Selected Answer: ADE**\n\n  ADE is right  \n  upvoted 1 times\n\n- **Musk 1 year, 6 months ago**\n\n  **Selected Answer: ACE**\n\n  I like ACE better. I am not sure Step Functions would work.  \n  upvoted 1 times\n\n- **moota 1 year, 5 months ago**\n\n  According to ChatGPT, AWS Step Functions can interact with AWS APIs in a few different ways. One example is below.\n\n  Directly invoking AWS APIs using the \"Task\" state in Step Functions. This state type allows you to run an AWS Lambda function, which can interact with AWS APIs as part of its logic.  \n  upvoted 1 times\n\n- **zhangyu20000 1 year, 6 months ago**\n\n  ADE are correct  \n  upvoted 1 times\n\n- **mastroemian 1 year, 6 months ago**\n\n  **Selected Answer: ADE**\n\n  This is the correct answer because it follows these steps:\n\n  - A: The first step is to create an EventBridge rule that listens for the specific API call to create a new IAM user. This will trigger the next step in the process.\n\n  - D: The next step is to use an AWS Step Functions state machine to remove access for the new IAM user. This ensures that access is removed automatically, as required by the security team.\n\n  - E: Finally, use Amazon SNS to notify the security team that a new user has been created and access has been removed. This allows the security team to review and approve the user as necessary.\n\n  Option B is not correct because CloudTrail alone is not able to remove access for the new user.\n\n  Option C is not correct because it is not specified in the question that the company is using Amazon Elastic Container Service and AWS Fargate technology.\n\n  Option F is not correct because the question specifies that the company should use Amazon SNS to notify the security team, not Amazon Pinpoint.  \n  upvoted 2 times\n\n- **hobokabobo 1 year, 4 months ago**\n\n  \"the question specifies that the company should use Amazon SNS \" -> no, it does not specify anything like that.  \n  \"because it is not specified in the question that the company is using Amazon Elastic Container\" -> so? is it specified that they use step function, can't find that either.  \n  The question must have changed, it does not match your explanations.  \n  upvoted 1 times\n\n- **Jessuleison 1 year, 2 months ago**\n\n  He just copied the answer from chatgpt for every question, really made me sick  \n  upvoted 7 times\n\n- **BabaP 1 year, 2 months ago**\n\n  it is annoying, I don't bother with reading them even if the answer they picked is correct  \n  upvoted 3 times\n\n---\n\n# Page 329\n\n# Question #118\n\n## Topic 1\n\nA company wants to migrate to AWS. The company wants to use a multi-account structure with centrally managed access to all accounts and applications. The company also wants to keep the traffic on a private network. Multi-factor authentication (MFA) is required at login, and specific roles are assigned to user groups.\n\nThe company must create separate accounts for development, staging, production, and shared network. The production account and the shared network account must have connectivity to all accounts. The development account and the staging account must have access only to each other.\n\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose three.)\n\nA. Deploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization in AWS Organizations.\n\nB. Enable AWS Security Hub in all accounts to manage cross-account access. Collect findings through AWS CloudTrail to force MFA login.\n\nC. Create transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables.\n\nD. Set up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing accounts.\n\nE. Enable AWS Control Tower in all accounts to manage routing between accounts. Collect findings through AWS CloudTrail to force MFA login.\n\nF. Create IAM users and groups. Configure MFA for all users. Set up Amazon Cognito user pools and Identity pools to manage access to accounts and between accounts.\n\n---\n\n**Correct Answer: BDF**\n\n**Community vote distribution**\n\n- ACD (100%)\n\n---\n\n### masetromain Highly Voted 1 year, 6 months ago\n\n**Selected Answer: ACD**\n\nThe correct answer would be options A, C and D, because they address the requirements outlined in the question.\n\nA. Deploying a landing zone environment using AWS Control Tower and enrolling accounts in an organization in AWS Organizations allows for a centralized management of access to all accounts and applications.\n\nC. Creating transit gateways and transit gateway VPC attachments in each account and configuring appropriate route tables allows for private network traffic, and ensures that the production account and shared network account have connectivity to all accounts, while the development and staging accounts have access only to each other.\n\nD. Setting up and enabling AWS IAM Identity Center (AWS Single Sign-On) and creating appropriate permission sets with required MFA for existing accounts allows for multi-factor authentication at login and specific roles to be assigned to user groups.\n\n*upvoted 14 times*\n\n### masetromain 1 year, 6 months ago\n\nThe other options are not correct because:\n\nB. Enabling AWS Security Hub in all accounts to manage cross-account access and collecting findings through AWS CloudTrail to force MFA login is not enough to meet the requirement of creating separate accounts for development, staging, production, and shared network. It can be used in addition to the other steps, but not as a standalone solution.\n\nE. Enabling AWS Control Tower in all accounts to manage routing between accounts and collecting findings through AWS CloudTrail to force MFA login is not enough to meet the requirement of creating separate accounts for development, staging, production, and shared network. It can be used in addition to the other steps, but not as a standalone solution.\n\n*upvoted 4 times*\n\nF. Creating IAM users and groups and configuring MFA for all users and setting up Amazon Cognito user pools and Identity pools to manage access to accounts and between accounts does not address the requirement of creating separate accounts for development, staging, production, and shared network. Additionally, it does not address the requirement of keeping the traffic on a private network.\n\n*upvoted 3 times*\n\n### ajeshb Most Recent 4 months, 3 weeks ago\n\n**Selected Answer: ACD**\n\nA, C and D are right answers. Option C is though not clear. Transit gateway needs to be created in shared network account and tgw vpc attachment in all accounts. But option C says \"create tgw and tgw vpc attachment in all accounts\", which is a bit confusing.\n\n---\n\n# Page 330\n\nupvoted 2 times\n\n- **8693a49** 4 days, 14 hours ago  \n  Yes, you probably only need one TGW in the shared account  \n  upvoted 1 times\n\n- **career360guru** 7 months, 2 weeks ago  \n  **Selected Answer: ACD**  \n  A, C and D  \n  upvoted 1 times\n\n- **shaam80** 8 months ago  \n  **Selected Answer: ACD**  \n  Answer - ACD  \n  upvoted 1 times\n\n- **NikkyDicky** 1 year, 1 month ago  \n  **Selected Answer: ACD**  \n  ACD easy  \n  upvoted 1 times\n\n- **Maria2023** 1 year, 1 month ago  \n  **Selected Answer: ACD**  \n  ACD seems like the only technically achievable solution. B and E appear to be completely wrong and for F - I am not sure whether Cognito will do the job but for sure it would be extremely hard to implement that way.  \n  upvoted 2 times\n\n- **OCHT** 1 year, 4 months ago  \n  **Selected Answer: ACD**  \n  Option E is not the most appropriate choice because it suggests enabling AWS Control Tower in all accounts to manage routing between accounts. However, AWS Control Tower is not primarily designed for managing routing between accounts; it is intended to set up and govern a secure, multi-account AWS environment. The transit gateways and VPC attachments in Option C are better suited for managing routing and connectivity between accounts.  \n  upvoted 4 times\n\n- **mfsec** 1 year, 4 months ago  \n  **Selected Answer: ACD**  \n  ACD are the best choice  \n  upvoted 1 times\n\n- **spd** 1 year, 5 months ago  \n  **Selected Answer: ACD**  \n  By Elimination Rule  \n  upvoted 3 times\n\n- **zhangyu200000** 1 year, 6 months ago  \n  ACD are correct.  \n  upvoted 3 times\n\n---\n\n# Page 331\n\n"
  },
  {
    "question_number": 121,
    "question_text": "A financial company is planning to migrate its web application from on premises to AWS. The company uses a third-party security tool to monitor the inbound traffic to the application. The company has used the security tool for the last 15 years, and the tool has no cloud solutions available from its vendor. The company's security team is concerned about how to integrate the security tool with AWS technology.\n\nThe company plans to deploy the application migration to AWS on Amazon EC2 instances. The EC2 instances will run in an Auto Scaling group in a dedicated VPC. The company needs to use the security tool to inspect all packets that come in and out of the VPC. This inspection must occur in real time and must not affect the application's performance. A solutions architect must design a target architecture on AWS that is highly available within an AWS Region.\n\n**Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)**",
    "answers": {
      "C": "Deploy an Application Load Balancer in front of the security tool instances",
      "A": "Deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPC",
      "B": "Deploy the web application behind a Network Load Balancer",
      "D": "Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool\n\nE. Provision a transit gateway to facilitate communication between VPCs."
    },
    "correct_answer": "AD",
    "explanation": "### Comments\n\n#### OCHT Highly Voted 1 year, 4 months ago\n\n**Selected Answer: AD**\n\nOption B, deploying the web application behind a Network Load Balancer, is not relevant to integrating the third-party security tool with AWS technology.\n\nOption C, deploying an Application Load Balancer in front of the security tool instances, is not necessary because a Gateway Load Balancer is already being used to redirect traffic to the security tool.\n\nOption E, provisioning a transit gateway to facilitate communication between VPCs, is not relevant to integrating the third-party security tool with AWS technology or inspecting packets in and out of the VPC.\n\nIn summary, options A and D are the best choices because they address the specific requirements stated in the scenario while options B, C, and E do not.\n\n*upvoted 19 times*\n\n#### 43c89f4 3 months, 1 week ago\n\nDE is correct, the question clearly mentions which combination - GWLB and provision transit gateway is the solution\n\n*upvoted 1 times*\n\n#### deegadaze1 1 year, 2 months ago\n\nCorrect for GLB: [YouTube Link](https://www.youtube.com/watch?v=j2smz_VCH4)\n\n*upvoted 2 times*\n\n#### drm2023 Highly Voted 1 year, 2 months ago\n\n**Selected Answer: DE**\n\nBased on the scenario in question, the requirement is that the security tool will run in an auto scaling group in a dedicated VPC; this cannot be changed. This will break Option A. If we look at the usage for the Gateway Load Balancer which is the key for the solution where application cannot have performance hits if you are inspecting the traffic, so you need to TAP the traffic to move into another third-party tool. In the references you will find below the transit gateway will facilitate the VPC-to-VPC communication and as you can see, the security appliances VPC is a segregated from the application VPC, so again, option A is NOT valid.\n- [AWS GWLB Documentation](https://catalog.workshops.aws/networking/en-US/gwlb)\n- [Fortinet Blog](https://www.fortinet.com/blog/business-and-technology/highly-scalable-fortigate-next-generation-firewall-security-on-aws-gateway-load-balancer-service)\n\n*upvoted 19 times*\n\n#### seochan Most Recent 2 days, 3 hours ago\n\n**Selected Answer: AD**\n\nDE cannot be the answer. The combination doesn't describe how to deploy the security tools on the cloud.\n\n*upvoted 1 times*\n\n#### michele_scar 1 month, 3 weeks ago\n\n\n---\n\n# Page 336\n\n### Selected Answer: DE\nDE is the answer. Transit -> GWLB -> Inspection tool  \nupvoted 1 times\n\n---\n\n#### ce8254d 2 months ago\n\n**Selected Answer: AD**\n\nAD is correct as the requirement is to use the Security tool to inspect traffic coming in and out of the VPC. So, you need to deploy the security tool on EC2 instances and provision a Gateway loadbalancer to load balance the traffic. With a GLB, you can deploy, manage, and scale virtual appliances, such as intrusion detection and prevention, firewalls, and deep packet inspection systems. It creates a single entry and exit point for all appliance traffic and scales your virtual appliances with demand. You can also exchange traffic across virtual private cloud (VPC) boundaries.  \nupvoted 1 times\n\n---\n\n#### seettp 3 months ago\n\n**Selected Answer: AD**\n\nAD for me  \nupvoted 1 times\n\n---\n\n#### red_panda 3 months, 1 week ago\n\n**Selected Answer: DE**\n\nDE without doubts guys.  \nGLB is just for this reason. Deploy the security tool into another ASG will only increase the cost and it's crazy, the performance isn't the same as the GLB (which operates at Lv. 3 of networking).  \nupvoted 2 times\n\n---\n\n#### teo2157 3 months, 1 week ago\n\n**Selected Answer: DE**\n\nBased on RMamos comment  \nupvoted 2 times\n\n---\n\n#### failexamonly 4 months, 1 week ago\n\n**Selected Answer: DE**\n\nNot A A does not make sense for D  \nupvoted 2 times\n\n---\n\n#### gofavda926 4 months, 3 weeks ago\n\n**Selected Answer: AD**\n\nAD - ec2 + asg + gateway load balancer  \nupvoted 3 times\n\n---\n\n#### djeong95 5 months ago\n\n**Selected Answer: DE**\n\nI answered AD and searched through these comments' links to seek to understand.  \nFirst, the most convincing case is that as @rbm2023 answered, it is not pattern to put security tool in the existing VPC. The financial company in the question is also looking to only have their application migrate into a dedicated VPC.\n\nSecond, solution A sounds good and according to this link below, you can use ASG with GWLB. I think key is the fine print of Customer wanting their own dedicated VPC and the pattern of using TWG in front. (However, it is possible to do without)\n\n[https://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-inspection-using-aws-gateway-load-balancer/](https://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-inspection-using-aws-gateway-load-balancer/)  \nupvoted 2 times\n\n---\n\n#### yog927 5 months, 1 week ago\n\n**Selected Answer: DE**\n\nD and E  \nRead the question, it says dedicated VPC  \nupvoted 2 times\n\n---\n\n#### Veri 5 months, 1 week ago\n\n**Selected Answer: AD**\n\nTry not to over-interpret the given options. Only D and E won't work because the security tool has not been settled down yet.  \nupvoted 4 times\n\n---\n\n#### chelbisk 6 months ago\n\n**Selected Answer: DE**\n\nVote for DE, agreed about 15 years old security tool might not be able to support autoscaling, and it has to be in a dedicated VPC, according to the task\n\nForgot to vote  \nupvoted 2 times\n\n---\n\n#### chelbisk 6 months ago\n\nVote for DE, agreed about 15 years old security tool might not be able to support autoscaling, and it has to be in a dedicated VPC, according to the task\n\n\n---\n\n# Page 337\n\nupvoted 1 times\n\nAWSLord32 6 months, 1 week ago\n\n> **Selected Answer: DE**\n\nA is not valid as the web application needs to be in a dedicated VPC. Also the security app is 15 years old and likely doesn't support autoscaling natively.\n\nDE is best practice.\nupvoted 3 times\n\njpa8300 7 months ago\n\n> **Selected Answer: DE**\n\nI agree with what has been said about D and E option. A could be right, but a better architecture would be to put the security tool in its own VPC, not only for this web application, but also to use to other apps where you want to use the security tool.\nupvoted 2 times\n\n---\n\n# Page 338\n\n"
  },
  {
    "question_number": 125,
    "question_text": "A company is running a critical application that uses an Amazon RDS for MySQL database to store data. The RDS DB instance is deployed in Multi-AZ mode.\n\nA recent RDS database failover test caused a 40-second outage to the application. A solutions architect needs to design a solution to reduce the outage time to less than 20 seconds.\n\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "answers": {
      "A": "Use Amazon ElastiCache for Memcached in front of the database",
      "B": "Use Amazon ElastiCache for Redis in front of the database",
      "C": "Use RDS Proxy in front of the database.",
      "D": "Migrate the database to Amazon Aurora MySQL.\n\nE. Create an Amazon Aurora Replica.\n\nF. Create an RDS for MySQL read replica"
    },
    "correct_answer": "BCF",
    "explanation": ""
  },
  {
    "question_number": 130,
    "question_text": "A solutions architect must analyze a company's Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to determine whether the company is using resources efficiently. The company is running several large, high-memory EC2 instances to host database clusters that are deployed in active/passive configurations. The utilization of these EC2 instances varies by the applications that use the databases, and the company has not identified a pattern.\n\nThe solutions architect must analyze the environment and take action based on the findings.\n\nWhich solution meets these requirements MOST cost-effectively?",
    "answers": {
      "A": "Create a dashboard by using AWS Systems Manager OpsCenter. Configure visualizations for Amazon CloudWatch metrics that are associated with the EC2 instances and their EBS volumes. Review the dashboard periodically, and identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.",
      "B": "Turn on Amazon CloudWatch detailed monitoring for the EC2 instances and their EBS volumes. Create and review a dashboard that is based on the metrics. Identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.",
      "C": "Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours. Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed.",
      "D": "Sign up for the AWS Enterprise Support plan. Turn on AWS Trusted Advisor. Wait 12 hours. Review the recommendations from Trusted Advisor, and rightsize the EC2 instances as directed."
    },
    "correct_answer": "C",
    "explanation": "### Comments\n\n#### God_Is_Love *Highly Voted* 1 year, 5 months ago\n\n**Selected Answer: C**\n\nAWS Compute Optimizer helps analyze the usage patterns of AWS resources, such as EC2 instances and Auto Scaling groups, and makes recommendations on how to optimize them for performance and cost using machine learning algorithms. It then generates recommendations that can be used to adjust instance types, purchase options, and other parameters. It provides two types of recommendations:\n\n- **Recommended instance types** - recommends instance types that are more cost-effective and better suited to the workload requirements.\n- **Recommended purchase options** - recommends purchasing options, such as Reserved Instances or Savings Plans, that can help customers save money on their compute resources.\n\n*upvoted 16 times*\n\n#### God_Is_Love 1 year, 5 months ago\n\nA is wrong.  \nOpsCenter, a capability of AWS Systems Manager, provides a central location where operations engineers and IT professionals can manage operational work items (OpsItems) related to AWS resources. An OpsItem is any operational issue or interruption that needs investigation and remediation. Using OpsCenter, you can view contextual investigation data about each OpsItem, including related OpsItems and related resources. You can also run Systems Manager Automation runbooks to resolve OpsItems.\n\n*upvoted 4 times*\n\n#### God_Is_Love 1 year, 5 months ago\n\nfyi Pricing looks cheap too - [AWS Compute Optimizer Pricing](https://aws.amazon.com/compute-optimizer/pricing/)\n\n*upvoted 2 times*\n\n#### saggy94 *Most Recent* 5 months, 3 weeks ago\n\n**Selected Answer: C**\n\nA - Not possible  \nD - Costliest Option possible  \nnow between B and C  \nThe question mentions high-memory EC2 instances.  \nYou cannot get memory metrics without the Cloudwatch agent installed hence C.\n\n*upvoted 2 times*\n\n#### career360guru 7 months, 2 weeks ago\n\n**Selected Answer: C**\n\nOption C is most cost effective choice.\n\n*upvoted 1 times*\n\n#### wmp7039 7 months, 2 weeks ago\n\nC is incorrect : When you first opt in Compute Optimizer, it may take up to 24 hours to fully analyze the AWS resources in your account.  \n[Compute Optimizer FAQ](https://aws.amazon.com/compute-optimizer/faqs/)\n\n\n---\n\n# Page 358\n\n- **carpa_jo** 7 months, 1 week ago  \n  You are correct that in the FAQ you've linked it says 24 hours, but in other places of the AWS documentation it says 12 hours, like here:  \n  [https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-getting-recommendations.html#viewing-recommendations](https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-getting-recommendations.html#viewing-recommendations)  \n  or here: [https://docs.aws.amazon.com/awssupport/latest/user/compute-optimizer-with-trusted-advisor.html](https://docs.aws.amazon.com/awssupport/latest/user/compute-optimizer-with-trusted-advisor.html)  \n  Seems like even AWS doesn't know :D So I would still go with C.  \n  upvoted 1 times\n\n- **aitrado** 7 months, 3 weeks ago  \n  **Selected Answer: C**  \n  Option A is not in the running because it will require incurring further expense to address the cost issue.  \n\n  Option D is expensive - the Enterprise Support plan charges a minimum flat fee minimum or a % of your AWS bill. This could be a large amount for the company's hundreds of instances.  \n\n  Option B is expensive - Detailed monitoring scales based on the number of metrics and the number of resources. The company has hundreds of instances so this option could potentially be more expensive than D.  \n\n  Option C - Compute Optimizer will provide improvement suggestions based on 14 prior days usage data from the moment it was enabled. Moreover, the default service option is free. Nothing is said about the custom metrics being used for the CloudWatch agent but it could be the most expensive of all options if mis-used. So either cost 0 or incredibly large if used carelessly.  \n  upvoted 1 times\n\n- **NikkyDicky** 1 year, 1 month ago  \n  **Selected Answer: C**  \n  C. need CW agent for RAm util  \n  upvoted 1 times\n\n- **Fredonly1** 1 year, 3 months ago  \n  **Selected Answer: C**  \n  C- Compute Optimizer is the easiest solution  \n  upvoted 1 times\n\n- **mfsec** 1 year, 4 months ago  \n  **Selected Answer: C**  \n  C - cost optimizer  \n  upvoted 1 times\n\n- **mfsec** 1 year, 4 months ago  \n  \"*Compute\"  \n  upvoted 1 times\n\n- **spd** 1 year, 5 months ago  \n  **Selected Answer: C**  \n  C is correct - Optimizer  \n  upvoted 2 times\n\n- **kiran15789** 1 year, 5 months ago  \n  **Selected Answer: A**  \n  Option C may be a good solution to rightsize the EC2 instances but may incur additional cost for installing the Amazon CloudWatch agent on each of the EC2 instances.  \n\n  The MOST cost-effective solution to analyze the company's Amazon EC2 instances and Amazon EBS volumes is to create a dashboard using AWS Systems Manager OpsCenter. The OpsCenter dashboard can be configured to visualize the Amazon CloudWatch metrics associated with the EC2 instances and their EBS volumes. By reviewing the dashboard periodically, usage patterns can be identified, and EC2 instances can be right-sized based on the peaks in the metrics.  \n  upvoted 1 times\n\n- **God_Is_Love** 1 year, 5 months ago  \n  Bro, install cost is 0. Simple linux command > sudo yum install amazon-cloudwatch-agent  \n  upvoted 2 times\n\n- **masetromani** 1 year, 6 months ago  \n  **Selected Answer: C**  \n  The correct answer is C. Installing the Amazon CloudWatch agent on each of the EC2 instances and turning on AWS Compute Optimizer allows the solutions architect to analyze the environment and make recommendations on the sizing of the EC2 instances in a cost-effective way. AWS Compute Optimizer analyzes the utilization of the instances and recommends the optimal instance types for the workloads. This solution is more cost-effective than creating a dashboard and reviewing it periodically, or signing up for the AWS Enterprise Support plan and waiting for Trusted Advisor recommendations.  \n  upvoted 3 times\n\n- **zhangyu20000** 1 year, 6 months ago  \n  C is correct, with computer optimizer  \n  upvoted 1 times\n\n---\n\n# Page 359\n\nThe image is blank, so there is no text to convert to markdown. If you have another image or text, feel free to share it!\n\n---\n\n# Page 360\n\n"
  },
  {
    "question_number": 134,
    "question_text": "A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instances. The application is a Linux binary, and the source code cannot be modified. The application is single-threaded, uses 2 GB of RAM, and is highly CPU intensive. The application is scheduled to run every 4 hours and runs for up to 20 minutes. A solutions architect wants to revise the architecture for the solution.\n\nWhich strategy should the solutions architect use?",
    "answers": {
      "A": "Use AWS Lambda to run the application. Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours.",
      "B": "Use AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours.",
      "C": "Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.",
      "D": "Use Amazon EC2 Spot Instances to run the application. Use AWS CodeDeploy to deploy and run the application every 4 hours."
    },
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "question_number": 135,
    "question_text": "A company is creating a sequel for a popular online game. A large number of users from all over the world will play the game within the first week after launch. Currently, the game consists of the following components deployed in a single AWS Region:\n\n- Amazon S3 bucket that stores game assets\n- Amazon DynamoDB table that stores player scores\n\nA solutions architect needs to design a multi-Region solution that will reduce latency, improve reliability, and require the least effort to implement.\n\n**What should the solutions architect do to meet these requirements?**",
    "answers": {
      "A": "Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Cross-Region Replication. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables.",
      "B": "Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Same-Region Replication. Create a new DynamoDB table in a new Region. Configure asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC).",
      "C": "Create another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets in each Region. Configure DynamoDB global tables by enabling Amazon DynamoDB Streams, and add a replica table in a new Region.",
      "D": "Create another S3 bucket in the same Region, and configure S3 Same-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables.\n\n---"
    },
    "correct_answer": "C",
    "explanation": "### Comments\n\n**zozza2023** *Highly Voted* 🥇 1 year, 6 months ago\n\n**Selected Answer: C**\n\nDynamoDB global tables + S3 replication+Cloudfront  \nupvoted 13 times\n\n---\n\n**mastroamin** *Highly Voted* 🥇 1 year, 6 months ago\n\nOption C is the correct answer because it meets the requirements of reducing latency, improving reliability and requiring minimal effort to implement.\n\nBy creating another S3 bucket in a new Region, and configuring S3 Cross-Region Replication between the buckets, the game assets will be replicated to the new Region, reducing latency for users accessing the assets from that region. Additionally, by creating an Amazon CloudFront distribution and configuring origin failover with two origins accessing the S3 buckets in each Region, it ensures that the game assets will be served to users even if one of the regions becomes unavailable.\n\nConfiguring DynamoDB global tables by enabling Amazon DynamoDB Streams, and adding a replica table in a new Region, will also improve reliability by allowing the player scores to be replicated and updated in multiple regions, ensuring that the scores are available even in the event of a regional failure.  \nupvoted 7 times\n\n---\n\n**mastroamin** 1 year, 6 months ago\n\nOption A is not correct because using the new table as a replica target for DynamoDB global tables will not improve reliability. The same applies for Option D, which only uses S3 Same-Region Replication, which will not reduce latency for users in other regions.\n\nOption B is not correct because configuring asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC) is not the best solution for this use case. It would require additional configuration and management effort.  \nupvoted 3 times\n\n---\n\n**ninomf64** *Most Recent* 🥈 6 months, 2 weeks ago\n\n**Selected Answer: C**\n\nA = \"Configure S3 Cross-Region Replication\" but doesn't create a new bucket in another region.  \nB = \"Configure S3 Same-Region Replication\" without creating a second bucket and this should be cross-region. AWS DMS with CDC is not a good fit here, global table is the right option here  \nC = correct  \nD = we need the new bucket in a different region\n\n---\n\n# Page 370\n\n- **career360guru** 7 months, 2 weeks ago  \n  **Selected Answer: C**  \n  Option C  \n  upvoted 2 times\n\n- **shaaam80** 8 months ago  \n  Answer C.  \n  Regarding DynamoDB Streams -  \n  Global tables use DynamoDB Streams to replicate data across different Regions. When you create a replica for a global table, a stream is created by default. Any changes to a replica are replicated to all the other replicas within the same global table within a second using DynamoDB Streams  \n  upvoted 1 time\n\n- **blackgamer** 9 months ago  \n  The answer is A. C added unnecessary complexities such as Amazon DynamoDB Streams and Origin Failover.  \n  upvoted 1 time\n\n- **ninomfr64** 6 months, 2 weeks ago  \n  Option A doesn't mention creating a new bucket in a different region  \n  upvoted 1 time\n\n- **Jay_2pt0_1** 8 months, 3 weeks ago  \n  I initially thought it was C, but I was torn between A and C. You may be right.  \n  upvoted 1 time\n\n- **uC6rWlaB** 11 months ago  \n  **Selected Answer: A**  \n  other options are incorrect.  \n  B: Configure S3 Same-Region Replication. ---> It's not meet multi-region requirement.  \n  C: Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets in each Region. ---> It's not support for this kinda failover  \n  D: Create another S3 bucket in the same Region, and configure S3 Same-Region Replication between the buckets. ---> It's not meet multi-region requirement.  \n  upvoted 2 times\n\n- **ninomfr64** 6 months, 2 weeks ago  \n  C is correct, Origin Group allows failover see [AWS Documentation](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html)  \n  upvoted 2 times\n\n- **dkcloudguru** 11 months ago  \n  option c is the easiest way to do  \n  upvoted 1 time\n\n- **ProMax** 11 months, 1 week ago  \n  **Selected Answer: A**  \n  Creating an Amazon CloudFront distribution will reduce latency for global users by serving assets from the closest edge location. S3 Cross-Region Replication will ensure that game assets are available in another region, improving reliability. Creating a new DynamoDB table in a new region and using it as a replica target for DynamoDB global tables will enable multi-region replication, improving reliability.  \n  upvoted 1 time\n\n- **SK_Tyagi** 11 months, 3 weeks ago  \n  **Selected Answer: C**  \n  Option C has another differentiator - DynamoDBStreams that will assist in Reliability  \n  upvoted 2 times\n\n- **grodoski** 1 year ago  \n  Correct A.  \n  CloudFront does not support origin failover with two origins accessing the S3 buckets in each Region. According to the AWS documentation [AWS Documentation](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html), origin failover only works within the same Region, not across Regions. This means that you can only configure origin failover with two origins that are in the same Region as the CloudFront distribution. If you want to use origin failover with S3 buckets in different Regions, you need to create multiple CloudFront distributions, one for each Region, and configure them to use the same domain name with geolocation routing.  \n  [Blog Post](https://blog.ippon.tech/when-a-cloudfront-origin-must-fail-for-testing-high-availability/)  \n  upvoted 1 time\n\n- **wenjing** 11 months, 1 week ago  \n  Referred to your AWS doc link. I don't see any condition that states that the origins in the origin group cannot be from two different regions. Can you provide the statement from the AWS doc that you are referring to please?  \n  upvoted 1 time\n\n- **NikkyDicky** 1 year, 1 month ago  \n  **Selected Answer: C**\n\n---\n\n# Page 371\n\nI'm unable to convert the image directly, but I can help you with the text content from the image:\n\n---\n\nweird question wording, but C fit more  \nupvoted 1 times\n\n---\n\n**mfsec** 1 year, 4 months ago  \n**Selected Answer: C**  \nCreate another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets  \nupvoted 2 times\n\n---\n\n**zhangyu20000** 1 year, 6 months ago  \nC is correct. S3 cross replicate, CloudFront, Dynamodb global database and origin failover  \nupvoted 2 times\n\n---\n\n---\n\n# Page 372\n\n"
  },
  {
    "question_number": 136,
    "question_text": "A company has an on-premises website application that provides real estate information for potential renters and buyers. The website uses a Java backend and a NoSQL MongoDB database to store subscriber data.\n\nThe company needs to migrate the entire application to AWS with a similar structure. The application must be deployed for high availability, and the company cannot make changes to the application.\n\nWhich solution will meet these requirements?",
    "answers": {
      "A": "Use an Amazon Aurora DB cluster as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.",
      "B": "Use MongoDB on Amazon EC2 instances as the database for the subscriber data. Deploy EC2 instances in an Auto Scaling group in a single Availability Zone for the Java backend application.",
      "C": "Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.",
      "D": "Configure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.\n\n---"
    },
    "correct_answer": "D",
    "explanation": "### Comments\n\n**uC6rW1a8** (Highly Voted) - 11 months ago\n\n**Selected Answer: C**\n\nC correct  \nDocumentDB only have on-demand instance but not on-demand capacity mode, the mode is for DynamoDB  \nupvoted 12 times\n\n**cnethers** (Most Recent) - 1 month ago\n\nD is the correct answer [https://aws.amazon.com/documentdb/pricing/](https://aws.amazon.com/documentdb/pricing/)  \non-demand instance is supported by DocumentDB  \nupvoted 1 times\n\n**gofavad926** - 4 months, 3 weeks ago\n\n**Selected Answer: C**\n\nC, documented. No exists the on-demand capacity mode  \nupvoted 1 times\n\n**AimarlLeo** - 6 months ago\n\n'Appropriately sized instances' Means on-demand ? that is quite vague..  \nupvoted 3 times\n\n**ninomfr64** - 6 months, 2 weeks ago\n\n**Selected Answer: A**\n\nA = Aurora supports MySQL and PostgreSQL, not MongoDB. App changes are not allowed  \nB = This could work but DocumentDB provides managed MongoDB instance that is preferable  \nC = correct  \nD = there isn't on-demand capacity mode, in 2022 launched MondoDB Elastic Cluster that eliminates the need to choose, manage or upgrade instances and allows to scale up to 4PiB storage whereas instance based scales up to 128TiB.\n\nI thing this question is pre elastic cluster as this is ambiguous between C and D  \nupvoted 4 times\n\n**jipa8300** - 7 months ago\n\n**Selected Answer: D**\n\nDocumentDB does indeed support on-demand capacity mode (Contrary to what other users say here) [https://aws.amazon.com/blogs/database/running-spiky-workloads-and-optimizing-costs-by-more-than-90-using-amazon-dynamodb-on-demand-capacity-mode/](https://aws.amazon.com/blogs/database/running-spiky-workloads-and-optimizing-costs-by-more-than-90-using-amazon-dynamodb-on-demand-capacity-mode/)  \nOn-Demand is ideally to a use case where you have unpredictable or variable database workloads, like this case, it is not said anywhere the expected workload, so it is better to start with On-demand, and later when you know the workload you can change it.  \n\n\n---\n\n# Page 373\n\n- **buriz** 6 months, 3 weeks ago  \n  what you have linked here is a dynamodb article not a documentDB one, documentDB does not support on-demand capacity mode -  \n  [https://aws.amazon.com/documentdb/faqs/](https://aws.amazon.com/documentdb/faqs/)\n\n  \"You can scale the compute resources allocated to your instance in the AWS Management Console by selecting the desired instance and clicking the \"modify\" button. Memory and CPU resources are modified by changing your instance class.\"  \n  upvoted 1 times\n\n- **ninomfr64** 6 months, 2 weeks ago  \n  There is no on-demand capacity for DocumentDB, however Elastic Cluster option is provided \"Elastic Clusters enables you to elastically scale your document database to handle millions of writes and reads, with petabytes of storage capacity\" see  \n  [https://aws.amazon.com/documentdb/faqs/#:~:text=to%20learn%20more-,Elastic%20Clusters,-What%20is%20Amazon](https://aws.amazon.com/documentdb/faqs/#:~:text=to%20learn%20more-,Elastic%20Clusters,-What%20is%20Amazon)  \n  upvoted 1 times\n\n- **chicagobeef** 6 months, 3 weeks ago  \n  This is DynamoDB, not DocumentDB. The choices only mention DocumentDB.  \n  upvoted 1 times\n\n- **career360guru** 7 months, 2 weeks ago  \n  **Selected Answer: C**  \n  There is no on-demand capacity mode for DocumentDB, though there is on-demand vCPU based pricing available.  \n  upvoted 1 times\n\n- **ninomfr64** 6 months, 2 weeks ago  \n  There is no on-demand capacity for DocumentDB, however Elastic Cluster option is provided \"Elastic Clusters enables you to elastically scale your document database to handle millions of writes and reads, with petabytes of storage capacity\" see  \n  [https://aws.amazon.com/documentdb/faqs/#:~:text=to%20learn%20more-,Elastic%20Clusters,-What%20is%20Amazon](https://aws.amazon.com/documentdb/faqs/#:~:text=to%20learn%20more-,Elastic%20Clusters,-What%20is%20Amazon)  \n  upvoted 1 times\n\n- **ProMax** 11 months, 1 week ago  \n  **Selected Answer: C**  \n  Amazon DocumentDB does NOT have on-demand capacity mode, so its option C.  \n  upvoted 3 times\n\n- **ninomfr64** 6 months, 2 weeks ago  \n  There is no on-demand capacity for DocumentDB, however Elastic Cluster option is provided \"Elastic Clusters enables you to elastically scale your document database to handle millions of writes and reads, with petabytes of storage capacity\" see  \n  [https://aws.amazon.com/documentdb/faqs/#:~:text=to%20learn%20more-,Elastic%20Clusters,-What%20is%20Amazon](https://aws.amazon.com/documentdb/faqs/#:~:text=to%20learn%20more-,Elastic%20Clusters,-What%20is%20Amazon)  \n  upvoted 1 times\n\n- **SK_Tyagi** 11 months, 3 weeks ago  \n  **Selected Answer: D**  \n  I was leaning towards Option C but \"Appropriately sized instances\" is vague since the question does not state the size of Mongo DB. On-demand instances serve the purpose here, they are offered by DocumentDB, see the link  \n  [https://aws.amazon.com/documentdb/pricing/](https://aws.amazon.com/documentdb/pricing/)  \n  upvoted 2 times\n\n- **NikkyDicky** 1 year, 1 month ago  \n  **Selected Answer: C**  \n  its a c  \n  upvoted 2 times\n\n- **easyto** 1 year, 1 month ago  \n  c-c-c-c-c-c-c-c-c\n\n  On-demand capacity mode as suggested in D may not provide the same level of high availability as multi-Availability Zone deployments. So it's c-c-c-c-c-c-c-c-c for me.  \n  upvoted 2 times\n\n- **SkyeZorx1** 1 year, 1 month ago  \n  **Selected Answer: C**  \n  See best practices for amazon documentdb - instance sizing in docs.  \n  Additionally there is no on-demand capacity mode.  \n  upvoted 2 times\n\n- **F_Eldin** 1 year, 2 months ago  \n  **Selected Answer: C**  \n  DocumentDB does indeed support on-demand capacity mode (Contrary to what other users say here)  \n  [https://aws.amazon.com/blogs/database/running-spiky-workloads-and-optimizing-costs-by-more-than-90-using-amazon-dynamodb-on-demand-capacity-mode/](https://aws.amazon.com/blogs/database/running-spiky-workloads-and-optimizing-costs-by-more-than-90-using-amazon-dynamodb-on-demand-capacity-mode/)\n\n  but this mode is good for spikey workloads and does not address the high availability requirement  \n  upvoted 3 times\n\n---\n\n# Page 374\n\n- **F_Eldin** 1 year, 2 months ago  \n  The correct link [https://www.applytosupply.digitalmarketplace.service.gov.uk/g-cloud/services/743016963590682](https://www.applytosupply.digitalmarketplace.service.gov.uk/g-cloud/services/743016963590682)  \n  upvoted 2 times\n\n- **[Removed]** 8 months, 3 weeks ago  \n  The content mentioned in your link and the original comment are both mentioning things related to DynamoDB. Your link is even worse which is describing DynamoDB but say it is for DocumentDB. Please study hard  \n  upvoted 1 times\n\n- **leejhworking** 1 year, 2 months ago  \n  **Selected Answer: C**  \n  See best practices for amazon documentdb - instance sizing in docs.  \n  upvoted 1 times\n\n- **Sarutobi** 1 year, 3 months ago  \n  **Selected Answer: C**  \n  Going with C. I still call the DocumentDB used in mode C \"on-demand mode\" because you have to select the Ec2 instance; the pricing documentation still uses that name. There is an Elastic cluster for DocumentDB. Could it be that option D \"on-demand capacity mode\" is referring to Elastic mode?  \n  upvoted 2 times\n\n- **OCHT** 1 year, 3 months ago  \n  **Selected Answer: C**  \n  Amazon DocumentDB does not support an on-demand capacity mode. You can only choose from different instance classes that have fixed compute and memory resources. However, you can scale your instances up or down as needed, and you can also pause and resume your instances to save costs. Amazon DocumentDB also automatically scales your storage and I/O based on your data size and workload.  \n  upvoted 1 times\n\n- **mfsec** 1 year, 4 months ago  \n  **Selected Answer: C**  \n  C - there is no on-demand capacity mode.  \n  upvoted 1 times\n\n---\n\n# Page 375\n\n#"
  },
  {
    "question_number": 146,
    "question_text": "A company is running an application in the AWS Cloud. The application runs on containers in an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS tasks use the Fargate launch type. The application's data is relational and is stored in Amazon Aurora MySQL. To meet regulatory requirements, the application must be able to recover to a separate AWS Region in the event of an application failure. In case of a failure, no data can be lost.\n\nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
    "answers": {
      "A": "Provision an Aurora Replica in a different Region.",
      "B": "Set up AWS DataSync for continuous replication of the data to a different Region.",
      "C": "Set up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to a different Region.",
      "D": "Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule a snapshot every 5 minutes."
    },
    "correct_answer": "A",
    "explanation": "### Comments\n\n**mastroremain** _Highly Voted_ 🗓️ 1 year, 6 months ago\n\n**Selected Answer: A**\n\nA is correct. Provision an Aurora Replica in a different Region will meet the requirement of the application being able to recover to a separate AWS Region in the event of an application failure, and no data can be lost, with the least amount of operational overhead.\n\n- B. AWS DataSync can replicate data, but it is not a fully managed service and requires more configuration and management.\n- C. AWS DMS is a fully managed service for migrating data between databases, but it may require additional configuration and management to continuously replicate data in real-time.\n- D. Amazon DLM can be used for scheduling snapshots, but it does not provide real-time replication and may not meet the requirement of no data loss in case of a failure.\n\n_Upvoted 8 times_\n\n**career360guru** _Most Recent_ 🗓️ 7 months, 2 weeks ago\n\n**Selected Answer: A**\n\nOption A\n\n_Upvoted 1 time_\n\n**NikkyDicky** 🗓️ 1 year, 1 month ago\n\n**Selected Answer: A**\n\nits an A\n\n_Upvoted 2 times_\n\n**Goatin** 🗓️ 1 year, 2 months ago\n\nWhen you provision an Aurora Replica in a different AWS Region, the replica is kept in sync with the primary database using Aurora’s replication capabilities. In the event of a failure in the primary Region, you can promote the Aurora Replica to become the new primary database, which allows you to continue operations with no data loss.\n\nHowever, provisioning and maintaining an Aurora Replica in a different AWS Region requires ongoing management and monitoring to ensure that it stays in sync with the primary database.\n\n_Upvoted 3 times_\n\n**mfsec** 🗓️ 1 year, 4 months ago\n\n**Selected Answer: A**\n\nReplica\n\n_Upvoted 4 times_\n\n**God_Is_Love** 🗓️ 1 year, 4 months ago\n\n**Selected Answer: A**\n\nB,C are on premises usecase solutions. D is wrong because 5 minute worth of data could be lost against the requirement. So A is correct. In fact replica works as standby if primary DB fails.\n\n_Upvoted 4 times_\n\n**zozza2023** 🗓️ 1 year, 6 months ago\n\n**Selected Answer: A**\n\nA is correct\n\n---\n\n# Page 397\n\nupvoted 4 times\n\n**zhangyu20000** 1 year, 6 months ago\n\n- A is correct\n- B: cannot use DataSync for Aurora backup\n- C: too complex\n- D: DLM is for EBS backup. Here use managed Aurora server, no access to EBS\n\nupvoted 2 times\n\n---\n\n# Page 398\n\n"
  },
  {
    "question_number": 155,
    "question_text": "A global media company is planning a multi-Region deployment of an application. Amazon DynamoDB global tables will back the deployment to keep the user experience consistent across the two continents where users are concentrated. Each deployment will have a public Application Load Balancer (ALB). The company manages public DNS internally. The company wants to make the application available through an apex domain.\n\nWhich solution will meet these requirements with the LEAST effort?",
    "answers": {
      "A": "Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB. Use a geolocation routing policy to route traffic based on user location.",
      "B": "Place a Network Load Balancer (NLB) in front of the ALB. Migrate public DNS to Amazon Route 53. Create a CNAME record for the apex domain to point to the NLB's static IP address. Use a geolocation routing policy to route traffic based on user location.",
      "C": "Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator's static IP address to create a record in public DNS for the apex domain.",
      "D": "Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route traffic to application deployments by using the round robin method. Create CNAME records for the apex domain to point to the API's URL."
    },
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "question_number": 156,
    "question_text": "A company is developing a new serverless API by using Amazon API Gateway and AWS Lambda. The company integrated the Lambda functions with API Gateway to use several shared libraries and custom classes.\n\nA solutions architect needs to simplify the deployment of the solution and optimize for code reuse.\n\nWhich solution will meet these requirements?",
    "answers": {
      "A": "Deploy the shared libraries and custom classes into a Docker image. Store the image in an S3 bucket. Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.",
      "B": "Deploy the shared libraries and custom classes to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.",
      "C": "Deploy the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) by using the AWS Fargate launch type. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the deployed container as a Lambda layer.",
      "D": "Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambda functions to use the Docker image as the deployment package."
    },
    "correct_answer": "B",
    "explanation": "### Comments\n\n#### High Voted 1 year, 5 months ago\n\n**Selected Answer: D**\n\nDon't understand why so many people are choosing B. Read up. A container image cannot be used with Lambda layers. That means A B C are out instantly. It's literally one of the first things they mention about Lamba layers. Answer is D and ABC simply impossible to configure.\n\n[https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html](https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html)\n\n*upvoted 42 times*\n\n---\n\n#### titi_r 3 months ago\n\nYou can create a Lambda function from an ECR image, but you CANNOT create a Lambda function layer from an ECR image!\n\n*upvoted 3 times*\n\n---\n\n#### Gabechoud 12 months ago\n\n[https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/](https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/)\n\nPreviously, Lambda functions were packaged only as zip archives. This includes functions created in the AWS Management Console. You can now also package and deploy Lambda functions as container images.\n\nYou can use familiar container tooling such as the Docker CLI with a Dockerfile to build, test, and tag images locally. Lambda functions built using container images can be up to 10 GB in size. You push images to an Amazon Elastic Container Registry (ECR) repository, a managed AWS container image registry service. You create your Lambda function, specifying the source code as the ECR image URL from the registry.\n\n*upvoted 3 times*\n\n---\n\n#### rtgfdv3 1 year, 4 months ago\n\n[https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/](https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/)\n\n*upvoted 3 times*\n\n---\n\n#### c73bf38 1 year, 5 months ago\n\nB suggests deploying the shared libraries and custom classes to a Docker image, uploading it to Amazon Elastic Container Registry (Amazon ECR), creating a Lambda layer that uses the Docker image as the source, and deploying the API's Lambda functions as Zip packages. Configuring the packages to use the Lambda layer simplifies deployment, and the Docker image allows for code reuse. This option takes advantage of the built-in features provided by AWS API Gateway and Lambda, making it the optimal solution.\n\n*upvoted 5 times*\n\n---\n\n#### c73bf38 1 year, 5 months ago\n\nThe requirement is code reuse: [https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/](https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/)\n\nLambda functions packaged as container images do not support adding Lambda layers to the function configuration. However, there are a number of solutions to use the functionality of Lambda layers with container images. You take on the responsibility for packaging your preferred runtimes and dependencies as a part of the container image during the build process.\n\n*upvoted 4 times*\n\n---\n\n# Page 421\n\n### Untamables [Highly Voted] 1 year, 6 months ago\n\n**Selected Answer: D**\n\nOption A, B and C are wrong. An AWS Lambda Layer does not support a Docker image or a deployed container as the source.  \n[https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html](https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html)  \n[https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/](https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/)  \nupvoted 8 times\n\n---\n\n### zolthar_z [Most Recent] 1 week, 6 days ago\n\n**Selected Answer: D**\n\nPlease read the requirement, \"simplify the deployment\" with D you need only to maintain the docker image, with B you need to maintain the docker image and the process to deploy the lambda as ZIP Packages.  \nupvoted 1 times\n\n---\n\n### Nicoben 7 months, 2 weeks ago\n\n**Selected Answer: B**\n\nOption B is the right one, see: [https://docs.aws.amazon.com/lambda/latest/dg/images-create.html](https://docs.aws.amazon.com/lambda/latest/dg/images-create.html)  \nupvoted 2 times\n\n---\n\n### career360guru 7 months, 2 weeks ago\n\n**Selected Answer: D**\n\nOption D  \nupvoted 1 times\n\n---\n\n### severlight 8 months, 3 weeks ago\n\n**Selected Answer: D**\n\ncheck lunt's answer  \nupvoted 1 times\n\n---\n\n### nlf 9 months, 3 weeks ago\n\nB.  \n* A Lambda layer is a .zip file archive that contains supplementary code or data. Layers usually contain library dependencies, a custom runtime, or configuration files.  \n* Lambda functions packaged as container images do not support adding Lambda layers to the function configuration. However, there are a number of solutions to use the functionality of Lambda layers with container images. You take on the responsibility for packaging your preferred runtimes and dependencies as a part of the container image during the build process.  \nupvoted 2 times\n\n---\n\n### dkcoldburgur 11 months ago\n\nAns is D: [https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/#:~:text=Lambda%20functions%20packaged%20as%20container,Lambda%20layers%20with%20container%20images](https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/#:~:text=Lambda%20functions%20packaged%20as%20container,Lambda%20layers%20with%20container%20images).  \nupvoted 1 times\n\n---\n\n### Gabehcoud 12 months ago\n\nAnswer B.  \nPreviously, Lambda functions were packaged only as .zip archives. This includes functions created in the AWS Management Console. You can now also package and deploy Lambda functions as container images.\n\nYou can use familiar container tooling such as the Docker CLI with a Dockerfile to build, test, and tag images locally. Lambda functions built using container images can be up to 10 GB in size. You push images to an Amazon Elastic Container Registry (ECR) repository, a managed AWS container image registry service. You create your Lambda function, specifying the source code as the ECR image URL from the registry.  \nupvoted 2 times\n\n---\n\n### vn_thanhtung 11 months, 2 weeks ago\n\n[https://www.youtube.com/watch?v=17R0vN8bt-0](https://www.youtube.com/watch?v=17R0vN8bt-0)  \nupvoted 1 times\n\n---\n\n### grodsky 1 year ago\n\nCorrect B.  \nupvoted 2 times\n\n---\n\n### NikkyDicky 1 year, 1 month ago\n\n**Selected Answer: D**\n\nD  \nlayers not supported w container-based lambdas  \nupvoted 1 times\n\n---\n\n### pupsik 1 year, 1 month ago\n\n**Selected Answer: D**\n\nDocker images cannot be used in Lambda layers.  \nupvoted 1 times\n\n---\n\n### Jackhemo 1 year, 1 month ago\n\n**Selected Answer: B**\n\n\n---\n\n# Page 422\n\nFrom olabiaa.ai: Overall, option B provides a streamlined approach to optimize code reuse by centralizing the shared code in a Docker image and using a Lambda layer to share it across multiple functions.\n\n**Roontha** 1 year, 2 months ago  \nAnswer: B  \nupvoted 1 times\n\n**dmr2023** 1 year, 2 months ago  \n**Selected Answer: B**  \n\"Lambda functions packaged as container images do not support adding Lambda layers to the function configuration. However, there are a number of solutions to use the functionality of Lambda layers with container images. You take on the responsibility for packaging your preferred runtimes and dependencies as a part of the container image during the build process.\"  \n[https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/](https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/)  \nupvoted 6 times\n\n**AMEJack** 1 year, 3 months ago  \n**Selected Answer: D**  \nAlthough the following URL says that you can deploy Lambda layers as container but this can't be used when the Lambda function in zip. The function will be created as another layer in the container image and it should use Lambda runtime environment.  \n[https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/](https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/)  \nupvoted 3 times\n\n**dev112233xx** 1 year, 3 months ago  \n**Selected Answer: D**  \nB is incorrect. Docker images use Layers refer to other Docker images, You can refer to a Docker layer ONLY if you choose to run your code in a Docker container (not a ZIP)\n\nread this article:  \n[https://www.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/](https://www.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/)  \nupvoted 4 times\n\n**dev112233xx** 1 year, 3 months ago  \nAlso read this article:  \n\"You can use layers only with Lambda functions deployed as a zip file archive. For functions defined as a container image, you package your preferred runtime and all code dependencies when you create the container image. For more information, see Working with Lambda layers and extensions in container images on the AWS Compute Blog.\"\n\n[https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html](https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html)  \nupvoted 2 times\n\n**dev112233xx** 1 year, 3 months ago  \nand  \n\"Lambda functions packaged as container images do not support adding Lambda layers to the function configuration. However, there are a number of solutions to use the functionality of Lambda layers with container images. You take on the responsibility for packaging your preferred runtimes and dependencies as a part of the container image during the build process.\"  \nSo it's clearly not B  \nupvoted 2 times\n\n---\n\n# Page 423\n\n"
  },
  {
    "question_number": 166,
    "question_text": "A delivery company is running a serverless solution in the AWS Cloud. The solution manages user data, delivery information, and past purchase details. The solution consists of several microservices. The central user service stores sensitive data in an Amazon DynamoDB table. Several of the other microservices store a copy of parts of the sensitive data in different storage services.\n\nThe company needs the ability to delete user information upon request. As soon as the central user service deletes a user, every other microservice must also delete its copy of the data immediately.\n\nWhich solution will meet these requirements?",
    "answers": {
      "A": "Activate DynamoDB Streams on the DynamoDB table. Create an AWS Lambda trigger for the DynamoDB stream that will post events about user deletion in an Amazon Simple Queue Service (Amazon SQS) queue. Configure each microservice to poll the queue and delete the user from the DynamoDB table.",
      "B": "Set up DynamoDB event notifications on the DynamoDB table. Create an Amazon Simple Notification Service (Amazon SNS) topic as a target for the DynamoDB event notification. Configure each microservice to subscribe to the SNS topic and to delete the user from the DynamoDB table.",
      "C": "Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table.",
      "D": "Configure the central user service to post a message on an Amazon Simple Queue Service (Amazon SQS) queue when the company deletes a user. Configure each microservice to create an event filter on the SQS queue and to delete the user from the DynamoDB table."
    },
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "question_number": 171,
    "question_text": "A company uses an AWS CodeCommit repository. The company must store a backup copy of the data that is in the repository in a second AWS Region.\n\nWhich solution will meet these requirements?",
    "answers": {
      "A": "Configure AWS Elastic Disaster Recovery to replicate the CodeCommit repository data to the second Region.",
      "B": "Use AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region.",
      "C": "Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository. Use CodeBuild to clone the repository. Create a .zip file of the content. Copy the file to an S3 bucket in the second Region.",
      "D": "Create an AWS Step Functions workflow on an hourly schedule to take a snapshot of the CodeCommit repository. Configure the workflow to copy the snapshot to an S3 bucket in the second Region."
    },
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "question_number": 177,
    "question_text": "A company is building a call center by using Amazon Connect. The company's operations team is defining a disaster recovery (DR) strategy across AWS Regions. The contact center has dozens of contact flows, hundreds of users, and dozens of claimed phone numbers.\n\nWhich solution will provide DR with the LOWEST RTO?",
    "answers": {
      "A": "Create an AWS Lambda function to check the availability of the Amazon Connect instance and to send a notification to the operations team in case of unavailability. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. After notification, instruct the operations team to use the AWS Management Console to provision a new Amazon Connect instance in a second Region. Deploy the contact flows, users, and claimed phone numbers by using an AWS CloudFormation template.",
      "B": "Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda function to deploy an AWS CloudFormation template that provisions contact flows and claimed numbers in the second Region.",
      "C": "Provision a new Amazon Connect instance with all existing contact flows and claimed phone numbers in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions all users. Configure the alarm to invoke the Lambda function.",
      "D": "Provision a new Amazon Connect instance with all existing users and contact flows in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. Configure the alarm to invoke the Lambda function."
    },
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "question_number": 362,
    "question_text": "A company needs to monitor a growing number of Amazon S3 buckets across two AWS Regions. The company also needs to track the percentage of objects that are encrypted in Amazon S3. The company needs a dashboard to display this information for internal compliance teams.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "answers": {
      "A": "Create a new 3 Storage Lens dashboard in each Region to track bucket and encryption metrics. Aggregate data from both Region dashboards into a single dashboard in Amazon QuickSight for the compliance teams.",
      "B": "Deploy an AWS Lambda function in each Region to list the number of buckets and the encryption status of objects. Store this data in Amazon S3. Use Amazon Athena queries to display the data on a custom dashboard in Amazon QuickSight for the compliance teams.",
      "C": "Use the S3 Storage Lens default dashboard to track bucket and encryption metrics. Give the compliance teams access to the dashboard directly in the S3 console.",
      "D": "Create an Amazon EventBridge rule to detect AWS CloudTrail events for S3 object creation. Configure the rule to invoke an AWS Lambda function to record encryption metrics in Amazon DynamoDB. Use Amazon QuickSight to display the metrics in a dashboard for the compliance teams.\n\n---"
    },
    "correct_answer": "B",
    "explanation": "### Comments\n\n**cypkir** _Highly Voted_ 8 months, 2 weeks ago\n\n- **Selected Answer: C**\n- Answer: C\n- upvoted 8 times\n\n**trungtdt** _Most Recent_ 2 months ago\n\n- **Selected Answer: A**\n- This use-case is really too rare to learn about on your own\n- upvoted 2 times\n\n**TonytheTiger** 4 months, 1 week ago\n\n- **Selected Answer: C**\n- Option C: Not A because the requirement is asking for \"Least Operation Overhead\" w/ S3 Storage Lens has a default dashboard. If you include QuickSight you are adding additional operational overhead, now you have to build your dashboard.\n- [AWS Documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens_basics_metrics_recommendations.html#storage_lens_basics_default_dashboard)\n- upvoted 2 times\n\n**career360guru** 6 months, 4 weeks ago\n\n- **Selected Answer: C**\n- Option C\n- upvoted 1 times\n\n**vibzr2023** 7 months ago\n\n- Answer: C\n- Storage Lens is a built-in S3 feature that automatically collects and aggregates storage metrics, eliminating the need for custom development or infrastructure management.\n- Option A: While Storage Lens supports multiple dashboards, creating and aggregating regional dashboards in QuickSight adds complexity and maintenance overhead.\n- Option B: Involves custom Lambda development, data storage in S3, Athena queries, and QuickSight integration, increasing operational complexity and costs.\n- Option D: Requires EventBridge rule configuration, Lambda function development, DynamoDB table management, and QuickSight integration, adding significant overhead.\n- upvoted 2 times\n\n**GoKhe** 7 months, 2 weeks ago\n\n- C\n- I was leaning towards A but it says in each region so that is wrong since Storage Lens gives you a view of all the regions. Someone has chosen B which is wrong b/c it has operational overhead.\n- upvoted 4 times\n\n**GaryQian** 7 months, 3 weeks ago\n\n\n---\n\n# Page 873\n\n- **Selected Answer: C**\n\n  I doubt B as the question is asking for LEAST operational choice instead of Best choice. The lambda function needs developer to write code.\n\n  *upvoted 1 times*\n\n---\n\n- **shaaam80 8 months ago**\n\n  **Selected Answer: C**\n\n  Answer C.  \n  Storage Lens metrics include % of encrypted objects\n\n  *upvoted 1 times*\n\n---\n\n- **Jon102 8 months ago**\n\n  **Selected Answer: C**\n\n  Answer: C, S3 Storage Lens default=free metrics which offers encryption tracking. It's easy to set up and least overhead.  \n  [https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens_metrics_glossary.html](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens_metrics_glossary.html)\n\n  *upvoted 2 times*\n\n---\n\n- **heatblur 8 months, 1 week ago**\n\n  **Selected Answer: C**\n\n  C is the best answer -- it's the most straightforward and involves the least operational overhead. It directly addresses the need to monitor S3 buckets and track encryption status without the need for additional setup or custom integrations. While it may not offer the same level of customization as some of the other options, it should suffice for most internal compliance requirements and is the most efficient choice in terms of minimizing operational complexity.\n\n  *upvoted 1 times*\n\n---\n\n- **pic1 8 months, 1 week ago**\n\n  **Selected Answer: A**\n\n  Given the scenario specifics, it's the only option that answers the need to aggregate data from two regions in a dashboard for compliance teams.\n\n  *upvoted 1 times*\n\n---\n\n- **pic1 8 months, 1 week ago**\n\n  On second thought, I'm switching to option B. It appears to be the lightest between the candidates.\n\n  *upvoted 1 times*\n\n---\n\n- **devalenzuela86 8 months, 2 weeks ago**\n\n  **Selected Answer: B**\n\n  B is ok.  \n  To monitor a growing number of Amazon S3 buckets across two AWS Regions and track the percentage of objects that are encrypted in Amazon S3 with the least operational overhead, a solutions architect can consider the following steps:\n  - Deploy an AWS Lambda function in each Region to list the number of buckets and the encryption status of objects. Store this data in Amazon S3.\n  - Use Amazon Athena queries to display the data on a custom dashboard in Amazon QuickSight for the compliance teams\n\n  *upvoted 2 times*\n\n---\n\n# Page 874\n\n"
  },
  {
    "question_number": 368,
    "question_text": "A company has application services that have been containerized and deployed on multiple Amazon EC2 instances with public IPs. An Apache Kafka cluster has been deployed to the EC2 instances. A PostgreSQL database has been migrated to Amazon RDS for PostgreSQL. The company expects a significant increase of orders on its platform when a new version of its flagship product is released.\n\nWhat changes to the current architecture will reduce operational overhead and support the product release?",
    "answers": {
      "A": "Create an EC2 Auto Scaling group behind an Application Load Balancer. Create additional read replicas for the DB instance. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.",
      "B": "Create an EC2 Auto Scaling group behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.",
      "C": "Deploy the application on a Kubernetes cluster created on the EC2 instances behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.",
      "D": "Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate and enable auto scaling behind an Application Load Balancer. Create additional read replicas for the DB instance. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.\n\n---"
    },
    "correct_answer": "A",
    "explanation": "# Page 885\n\n- **MegalodonBolado** 7 months ago  \n  On C, the number of EC2 instances is fixed, so can't provide elasticity beyond this limit. Could be another history if ASG was mentioned.  \n  upvoted 1 times\n\n- **shaaam80** 8 months, 1 week ago  \n  **Selected Answer: D**  \n  Answer - D  \n  upvoted 3 times\n\n- **develanzuela86** 8 months, 2 weeks ago  \n  **Selected Answer: D**  \n  D for sure  \n  upvoted 3 times\n\n- **cypkir** 8 months, 2 weeks ago  \n  **Selected Answer: D**  \n  Answer: D  \n  upvoted 3 times\n\n---\n\n# Page 886\n\n"
  },
  {
    "question_number": 371,
    "question_text": "### Topic 1\n\nA company hosts an intranet web application on Amazon EC2 instances behind an Application Load Balancer (ALB). Currently, users authenticate to the application against an internal user database.\n\nThe company needs to authenticate users to the application by using an existing AWS Directory Service for Microsoft Active Directory directory. All users with accounts in the directory must have access to the application.\n\nWhich solution will meet these requirements?",
    "answers": {
      "A": "Create a new app client in the directory. Create a listener rule for the ALB. Specify the authenticate-oidc action for the listener rule. Configure the listener rule with the appropriate issuer, client ID and secret, and endpoint details for the Active Directory service. Configure the new app client with the callback URL that the ALB provides.",
      "B": "Configure an Amazon Cognito user pool. Configure the user pool with a federated identity provider (IdP) that has metadata from the directory. Create an app client. Associate the app client with the user pool. Create a listener rule for the ALB. Specify the authenticate-cognito action for the listener rule. Configure the listener rule to use the user pool and app client.",
      "C": "Add the directory as a new IAM identity provider (IdP). Create a new IAM role that has an entity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Configure the new role as the default authenticated user role for the IdP. Create a listener rule for the ALB. Specify the authenticate-oidc action for the listener rule.",
      "D": "Enable AWS IAM Identity Center (AWS Single Sign-On). Configure the directory as an external identity provider (IdP) that uses SAML. Use the automatic provisioning method. Create a new IAM role that has an entity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Attach the new role to all groups. Create a listener rule for the ALB. Specify the authenticate-cognito action for the listener rule."
    },
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "question_number": 373,
    "question_text": "A company is using multiple AWS accounts and has multiple DevOps teams running production and non-production workloads in these accounts. The company would like to centrally-restrict access to some of the AWS services that the DevOps teams do not use. The company decided to use AWS Organizations and successfully invited all AWS accounts into the Organization. They would like to allow access to services that are currently in-use and deny a few specific services. Also they would like to administer multiple accounts together as a single unit.\n\nWhat combination of steps should the solutions architect take to satisfy these requirements? (Choose three.)",
    "answers": {
      "A": "Use a Deny list strategy.",
      "B": "Review the Access Advisor in AWS IAM to determine services recently used.",
      "C": "Review the AWS Trusted Advisor report to determine services recently used.",
      "D": "Remove the default FullAWSAccess SCP.\n\nE. Define organizational units (OUs) and place the member accounts in the OUs.\n\nF. Remove the default DenyAWSAccess SCP."
    },
    "correct_answer": "CDF",
    "explanation": "### Comments\n\n#### heatblur [Highly Voted] 8 months, 1 week ago\n\n**Selected Answer: ABE**\n\nABE is the answer:\n\n- **A:** This approach involves explicitly denying access to specific AWS services that the company wants to restrict. It allows all other services to be accessible, which aligns with the company's requirement to allow services that are currently in use.\n\n- **B:** AWS IAM Access Advisor shows the service permissions granted to a user and when those services were last accessed. This information is valuable to understand which AWS services are actively used and which are not, helping to make informed decisions about which services to restrict.\n\n- **E:** Organizational Units allow for grouping AWS accounts that have similar needs or requirements. This structure enables the solutions architect to apply policies at the OU level, making it easier to manage permissions and restrictions across multiple accounts.\n\n*upvoted 8 times*\n\n#### heatblur 8 months, 1 week ago\n\nAlso: it shouldn't be D because the FullAWSAccess SCP allows all actions on all resources in the account. Removing it without a carefully crafted replacement policy can lead to unintended access restrictions.\n\n*upvoted 4 times*\n\n#### vibzr2023 7 months ago\n\nNo...explicitly deny access/explicit Deny Statements to specific actions or resources, effectively override FullAWSAccess\n\n*upvoted 1 times*\n\n#### vibzr2023 7 months ago\n\nSaying the above statement my answer is E B A in the order.\n\n*upvoted 1 times*\n\n#### vibzr2023 7 months ago\n\n**Order of Evaluation**\n\nExplicit deny statements in IAM policies or SCPs take precedence over everything else.  \nIf no explicit denies exist, AWS evaluates policies in this order: Service-Linked Roles > Resource-Based Policies > IAM Policies (including FullAWSAccess) > SCPs > Conditional Access Policies\n\n*upvoted 1 times*\n\n#### vibzr2023 7 months ago\n\nI mean YES...throwing some light on the permissions evaluation.  \n[https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html)\n\n*upvoted 1 times*\n\n#### titi [Most Recent] 3 months, 2 weeks ago\n\n**Selected Answer: ABE**\n\nIsn't this called IAM Access Analyzer instead of Advisor?\n\n*upvoted 1 times*\n\n---\n\n# Page 895\n\n- **igorz1gzhj577** 6 months, 2 weeks ago  \n  With a deny list strategy a default SCP allows all services and deny lists must be implemented for any specific services that must be restricted.  \n  upvoted 1 times\n\n- **career360guru** 6 months, 4 weeks ago  \n  **Selected Answer: ABE**  \n  A, B and E  \n  upvoted 1 times\n\n- **ayadamyala** 7 months, 4 weeks ago  \n  **Selected Answer: ABE**  \n  Agreed E+B+A in that order :)  \n  upvoted 2 times\n\n- **dutchy1988** 8 months, 1 week ago  \n  manage as single unit -> OU's is out of scope (answer e)\n\n  deny some of the AWS services -> remove the default FullAWSAcces  \n  allow current in use services -> access advisor to determine recently used services  \n  Use deny list strategy to allow only services that are required\n\n  leaves only valid answer: ABD  \n  upvoted 1 times\n\n  - **dutchy1988** 8 months ago  \n    I have to rectify one answer,  \n    You can use organizational units (OUs) to group accounts together to administer as a single unit.  \n    [https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html)  \n    So E is correct, D is incorrect\n\n    Answer must be ABE  \n    upvoted 3 times\n\n- **shaaam80** 8 months, 1 week ago  \n  **Selected Answer: BDE**  \n  To administer multiple accounts together as a single unit - Create OU's with member accounts  \n  Remove blanket Allow on OUs - Remove the default FullAWSAccess SCP from OU's  \n  Review Access Advisor to view which services have been in use or accessed by users / roles\n\n  Answer BDE  \n  upvoted 1 times\n\n  - **shaaam80** 8 months, 1 week ago  \n    There is no DenyAWSAccess SCP created by default on OUs during creation.  \n    upvoted 2 times\n\n  - **shaaam80** 8 months ago  \n    correction - ABE  \n    D is wrong, removal of FullAccessSCP without replacing it with a custom SCP is not correct.  \n    A is correct, using a Deny list to restrict access to specific services  \n    upvoted 2 times\n\n- **devalenzuela86** 8 months, 2 weeks ago  \n  **Selected Answer: ABE**  \n  ABE for sure  \n  upvoted 1 times\n\n---\n\n# Page 896\n\n"
  },
  {
    "question_number": 381,
    "question_text": "A company built an ecommerce website on AWS using a three-tier web architecture. The application is Java-based and composed of an Amazon CloudFront distribution, an Apache web server layer of Amazon EC2 instances in an Auto Scaling group, and a backend Amazon Aurora MySQL database.\n\nLast month, during a promotional sales event, users reported errors and timeouts while adding items to their shopping carts. The operations team recovered the logs created by the web servers and reviewed Aurora DB cluster performance metrics. Some of the web servers were terminated before logs could be collected and the Aurora metrics were not sufficient for query performance analysis.\n\nWhich combination of steps must the solutions architect take to improve application performance visibility during peak traffic events? (Choose three.)",
    "answers": {
      "A": "Configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs.",
      "B": "Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and implement tracing of SQL queries with the X-Ray SDK for Java.",
      "C": "Configure the Aurora MySQL DB cluster to stream slow query and error logs to Amazon Kinesis.",
      "D": "Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs.\n\nE. Enable and configure AWS CloudTrail to collect and analyze application activity from Amazon EC2 and Aurora.\n\nF. Enable Aurora MySQL DB cluster performance benchmarking and publish the stream to AWS X-Ray."
    },
    "correct_answer": "AEF",
    "explanation": ""
  },
  {
    "question_number": 388,
    "question_text": "A company is developing a web application that runs on Amazon EC2 instances in an Auto Scaling group behind a public-facing Application Load Balancer (ALB). Only users from a specific country are allowed to access the application. The company needs the ability to log the access requests that have been blocked. The solution should require the least possible maintenance.\n\nWhich solution meets these requirements?",
    "answers": {
      "A": "Create an IPSet containing a list of IP ranges that belong to the specified country. Create an AWS WAF web ACL. Configure a rule to block any requests that do not originate from an IP range in the IPSet. Associate the rule with the web ACL. Associate the web ACL with the ALB.",
      "B": "Create an AWS WAF web ACL. Configure a rule to block any requests that do not originate from the specified country. Associate the rule with the web ACL. Associate the web ACL with the ALB.",
      "C": "Configure AWS Shield to block any requests that do not originate from the specified country. Associate AWS Shield with the ALB.",
      "D": "Create a security group rule that allows ports 80 and 443 from IP ranges that belong to the specified country. Associate the security group with the ALB."
    },
    "correct_answer": "C",
    "explanation": "### Comments\n\n- **vibzr2023** _Highly Voted_ 7 months ago  \n  Answer: B  \n  AWS WAF supports geo-matching rules, allowing you to easily block requests based on country of origin. This eliminates the need to manually manage IP ranges.  \n  Option C - Shield primarily defends against DDoS attacks and does not offer granular geo-blocking capabilities.  \n  _upvoted 6 times_\n\n- **career360guru** _Most Recent_ 6 months, 4 weeks ago  \n  **Selected Answer: B**  \n  Option B  \n  _upvoted 1 times_\n\n- **Jon102** 8 months ago  \n  **Selected Answer: B**  \n  Answer: B  \n  _upvoted 1 times_\n\n- **GabrielDeBiasi** 8 months, 1 week ago  \n  **Selected Answer: B**  \n  B for sure  \n  _upvoted 1 times_\n\n- **Maygam** 8 months, 2 weeks ago  \n  **Selected Answer: B**  \n  [AWS WAF Geo-Match Rule Documentation](https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html)  \n  _upvoted 2 times_\n\n- **devalenzuela86** 8 months, 2 weeks ago  \n  B for sure  \n  _upvoted 1 times_\n\n- **cypkir** 8 months, 2 weeks ago  \n  **Selected Answer: B**  \n  Answer: B  \n  _upvoted 2 times_\n\n---\n\n# Page 925\n\n"
  },
  {
    "question_number": 392,
    "question_text": "### Topic 1\n\nA car rental company has built a serverless REST API to provide data to its mobile app. The app consists of an Amazon API Gateway API with a Regional endpoint, AWS Lambda functions, and an Amazon Aurora MySQL Serverless DB cluster. The company recently opened the API to mobile apps of partners. A significant increase in the number of requests resulted, causing sporadic database memory errors.\n\nAnalysis of the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time. Traffic is concentrated during business hours, with spikes around holidays and other events.\n\nThe company needs to improve its ability to support the additional usage while minimizing the increase in costs associated with the solution.\n\n**Which strategy meets these requirements?**",
    "answers": {
      "A": "Convert the API Gateway Regional endpoint to an edge-optimized endpoint. Enable caching in the production stage.",
      "B": "Implement an Amazon ElastiCache for Redis cache to store the results of the database calls. Modify the Lambda functions to use the cache.",
      "C": "Modify the Aurora Serverless DB cluster configuration to increase the maximum amount of available memory.",
      "D": "Enable throttling in the API Gateway production stage. Set the rate and burst values to limit the incoming calls.\n\n---"
    },
    "correct_answer": "C",
    "explanation": "### Comments\n\n- **vibr2023** _Highly Voted_ 7 months ago  \n  Option A is correct  \n  While A and B do the job but the question says \"minimizing the increase in costs associated with the solution\". I'll go with A coz Edge-optimized endpoints cache responses at edge locations closer to users, significantly reducing the number of requests reaching the database and Lambda functions. While Option B -- While ElastiCache for Redis a good caching solution, it adds complexity and cost compared to edge caching.  \n  _upvoted 8 times_\n\n- **career360guru** _Highly Voted_ 6 months, 4 weeks ago  \n  **Selected Answer: A**  \n  Option A because B is more expensive than A  \n  _upvoted 6 times_\n\n- **Zas1** _Most Recent_ 2 months, 2 weeks ago  \n  **Selected Answer: B**  \n  Minimizing is not the most cheaper.  \n  _upvoted 1 times_\n\n- **red_panda** 2 months, 3 weeks ago  \n  **Selected Answer: B**  \n  For me it's B. We are using a REGIONAL (Not Global) API Endpoint.  \n  Take in mind that the errors are in Database layer, so it's not a problem to APIs, but with Redis Cache for sure we solve it. For me it's the best choice.  \n  _upvoted 5 times_\n\n- **AlbertC** 4 months ago  \n  **Selected Answer: B**  \n  A doesn't resolve the problems. It is B.  \n  _upvoted 2 times_\n\n- **VeriA** 4 months, 2 weeks ago  \n  **Selected Answer: A**  \n  B is expensive  \n  _upvoted 1 times_\n\n- **duriselvan** 7 months ago  \n  kEY WORK MOBILE APP b IS ANY  \n  [https://aws.amazon.com/elasticache/redis/](https://aws.amazon.com/elasticache/redis/)\n\n- **carpa_jo** 7 months, 1 week ago\n\n\n---\n\n# Page 932\n\n### Selected Answer: A\n\nAPI Gateway can take care of caching and it should be the cheaper solution compared to ElastiCache for Redis. That why I go with A.  \n*upvoted 2 times*\n\n---\n\n**mosalahs** 7 months, 1 week ago\n\n#### Selected Answer: A\n\nThe main option is \"clients are making multiple HTTP GET requests for the same queries in a short period of time.\" Enable Cache from APIGW  \n<https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html>  \n\nOption B is workable solution but will add more cost  \n*upvoted 3 times*\n\n---\n\n**ayadmawla** 7 months, 2 weeks ago\n\n#### Selected Answer: B\n\nIts B  \nFor those choosing A, a change between regional and edge API is not required but API caching is. The problem is that A doesn't explain \"how\" which is explained in B.  \n*upvoted 2 times*\n\n---\n\n**GaryQian** 7 months, 3 weeks ago\n\n#### Selected Answer: B\n\nShould be B: \"Analysis of the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time\". Same query with same result should be cached.  \n*upvoted 3 times*\n\n---\n\n**mifune** 2 months, 3 weeks ago\n\nIt's what is doing the answer A as well. Where's the problem?  \n*upvoted 1 times*\n\n---\n\n**Russs99** 7 months, 4 weeks ago\n\n#### Selected Answer: B\n\nOption A suggest Converting the API Gateway endpoint and enabling caching is not as effective for this scenario because edge-optimized endpoints are primarily for global distribution. This application is regional  \n*upvoted 3 times*\n\n---\n\n**Russs99** 7 months, 4 weeks ago\n\nThe problem being solved in this scenario is not a latency related, but caching, therefore, i am sticking with pick of B over A  \n*upvoted 3 times*\n\n---\n\n**PAUGURU** 8 months, 1 week ago\n\n#### Selected Answer: B\n\nI'd say B, solution A can reduce latency using Edge API, moreover the caching part is too vague, what does it mean enable caching in the production? It means exactly solution B.  \n*upvoted 3 times*\n\n---\n\n**dutchy1988** 8 months, 1 week ago\n\nmultiple HTTP GET requests for the same queries -> leaning towards caching.  \nAlso remark that company requires minimize costs, redis is out since you will have to spend money while there is need for it during low loads. A is the best solution here.  \n*upvoted 1 times*\n\n---\n\n**ProMax** 8 months, 1 week ago\n\n#### Selected Answer: B\n\nB is right answer  \n*upvoted 1 times*\n\n---\n\n**shaaam80** 8 months, 1 week ago\n\n#### Selected Answer: A\n\nAnswer A. User Edge-optimized API GW endpoint. B would work but with increased costs and overhead for Elasticache for Redis.  \n*upvoted 1 times*\n\n---\n\n**heatblur** 8 months, 1 week ago\n\n#### Selected Answer: A\n\nA is the right answer.\n\nUsing ElastiCache will work, but you'll be required to spin up resources which you'll be billed for, even during slow periods when it's not needed. Better to use the caching that is a part of APIGW.  \n*upvoted 2 times*\n\n---\n\n# Page 933\n\n"
  },
  {
    "question_number": 399,
    "question_text": "### Topic 1\n\nA company wants to migrate its website from an on-premises data center onto AWS. At the same time, it wants to migrate the website to a containerized microservice-based architecture to improve the availability and cost efficiency. The company's security policy states that privileges and network permissions must be configured according to best practice, using least privilege.\n\nA solutions architect must create a containerized architecture that meets the security requirements and has deployed the application to an Amazon ECS cluster.\n\nWhat steps are required after the deployment to meet the requirements? (Choose two.)",
    "answers": {
      "A": "Create tasks using the bridge network mode.",
      "B": "Create tasks using the awsvpc network mode.",
      "C": "Apply security groups to Amazon EC2 instances, and use IAM roles for EC2 instances to access other resources.",
      "D": "Apply security groups to the tasks, and pass IAM credentials into the container at launch time to access other resources.\n\nE. Apply security groups to the tasks, and use IAM roles for tasks to access other resources."
    },
    "correct_answer": "CD",
    "explanation": "### Comments\n\n- **ahmadraufsyahputra** 4 months, 2 weeks ago  \n  BE, you can apply security group to the task using vpcmode, because in vpcmode the task will use ENI within the VPC and the ENI can use security groups  \n  _upvoted 3 times_\n\n- **VeriRi** 4 months, 2 weeks ago  \n  **Selected Answer:** BE  \n  BE for sure  \n  _upvoted 1 times_\n\n- **career360guru** 6 months, 4 weeks ago  \n  **Selected Answer:** BE  \n  Option B and E  \n  _upvoted 1 times_\n\n- **GabrielDeBiasi** 8 months, 1 week ago  \n  **Selected Answer:** BE  \n  BE, easy  \n  _upvoted 2 times_\n\n- **Jonalb** 8 months, 2 weeks ago  \n  **Selected Answer:** BE  \n  B. Crie tarefas usando o modo de rede awsvpc.  \n  E. Aplique grupos de segurança às tarefas e use funções do IAM para tarefas para acessar outros recursos.  \n  _upvoted 2 times_\n\n- **thala** 8 months, 2 weeks ago  \n  **Selected Answer:** BE  \n  [https://www.examtopics.com/discussions/amazon/view/5362-exam-aws-certified-solutions-architect-professional-topic-1/](https://www.examtopics.com/discussions/amazon/view/5362-exam-aws-certified-solutions-architect-professional-topic-1/)\n\n- **devalenzuela86** 8 months, 2 weeks ago  \n  **Selected Answer:** BE  \n  BE for sure  \n  _upvoted 1 times_\n\n- **cypkir** 8 months, 2 weeks ago\n\n---\n\n# Page 947\n\nI'm sorry, I can't extract text from the image you provided.\n\n---\n\n# Page 948\n\n"
  },
  {
    "question_number": 403,
    "question_text": "A company has deployed an Amazon Connect contact center. Contact center agents are reporting large numbers of computer-generated calls. The company is concerned about the cost and productivity effects of these calls. The company wants a solution that will allow agents to flag the call as spam and automatically block the numbers from going to an agent in the future.\n\nWhat is the MOST operationally efficient solution to meet these requirements?",
    "answers": {
      "A": "Customize the Contact Control Panel (CCP) by adding a flag call button that will invoke an AWS Lambda function that calls the UpdateContactAttributes API. Use an Amazon DynamoDB table to store the spam numbers. Modify the contact flows to look for the updated attribute and to use a Lambda function to read and write to the DynamoDB table.",
      "B": "Use a Contact Lens for Amazon Connect rule that will look for spam calls. Use an Amazon DynamoDB table to store the spam numbers. Modify the contact flows to look for the rule and to invoke an AWS Lambda function to read and write to the DynamoDB table.",
      "C": "Use an Amazon DynamoDB table to store the spam numbers. Create a quick connect that the agents can transfer the spam call to from the Contact Control Panel (CCP). Modify the quick connect contact flow to invoke an AWS Lambda function to write to the DynamoDB table.",
      "D": "Modify the initial contact flow to ask for caller input. If the agent does not receive input, the agent should mark the caller as spam. Use an Amazon DynamoDB table to store the spam numbers. Use an AWS Lambda function to read and write to the DynamoDB table."
    },
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "question_number": 417,
    "question_text": "A company has an application that stores data in a single Amazon S3 bucket. The company must keep all data for 1 year. The company's security team is concerned that an attacker could gain access to the AWS account through leaked long-term credentials.\n\nWhich solution will ensure that existing and future objects in the S3 bucket are protected?",
    "answers": {
      "A": "Create a new AWS account that is accessible only to the security team through an assumed role. Create an S3 bucket in the new account. Enable S3 Versioning and S3 Object Lock. Configure a default retention period of 1 year. Set up replication from the existing S3 bucket to the new S3 bucket. Create an S3 Batch Replication job to copy all existing data.",
      "B": "Use the s3-bucket-versioning-enabled AWS Config managed rule. Configure an automatic remediation action that uses an AWS Lambda function to enable S3 Versioning and MFA Delete on noncompliant resources. Add an S3 Lifecycle rule to delete objects after 1 year.",
      "C": "Explicitly deny bucket creation from all users and roles except for an AWS Service Catalog launch constraint role. Define a Service Catalog product for the creation of the S3 bucket to force S3 Versioning and MFA Delete to be enabled. Authorize users to launch the product when they need to create an S3 bucket.",
      "D": "Enable Amazon GuardDuty with the S3 protection feature for the account and the AWS Region. Add an S3 Lifecycle rule to delete objects after 1 year."
    },
    "correct_answer": "D",
    "explanation": "### Comments\n\n**vip2 4 weeks ago**\n\n- **Selected Answer: A**\n- assume role to provide short-term credential\n- upvoted 1 times\n\n**TonytheTiger 4 months, 1 week ago**\n\n- **Selected Answer: A**\n- Option A: Amazon S3 now allows you to enable S3 Object Lock for existing buckets with just a few clicks and to enable S3 Replication for buckets using S3 Object Lock\n- [AWS S3 Object Lock](https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-s3-enabling-object-lock-buckets/#:~:text=To%20lock%20existing%20objects%2C%20you,of%20objects%20at%20a%20time.)\n- upvoted 2 times\n\n**Dgix 4 months, 4 weeks ago**\n\n- **Selected Answer: A**\n- The question is, as so often, misleading. None of the alternatives deal with _access_, only with modification.\n- upvoted 2 times\n\n**career360guru 4 months, 4 weeks ago**\n\n- **Selected Answer: D**\n- Option D is the only option that addresses security risk. Option A is not addressing this - Replicating existing bucket to another bucket does not eliminate the risk due to original bucket credential leak.\n- upvoted 2 times\n\n**bjexamprep 5 months, 1 week ago**\n\n- The question is looking for solution for “concerned that an attacker could gain access to the AWS account through leaked long-term credentials”. None of the answer is addressing the concern of “Access” Through “leaked long-term credentials”.\n- The is question doesn’t mention anything about data loss concerns, while, all the answers are providing protection for deleting the data.\n- upvoted 2 times\n\n**9f02cb2d 2 months, 1 week ago**\n\n- creating new account accessed by security team members is action taken to avoid the risk through leaked long-term credentials of existing account so Option A\n- upvoted 1 times\n\n**nharaz 5 months, 3 weeks ago**\n\n- **Selected Answer: A**\n- S3 Object Lock - prevents objects from being deleted or overwritten for a fixed amount of time or indefinitely, adding a layer of protection against malicious or accidental deletion.\n- Replication - to a new account limits the risk of a single point of compromise; even if attackers gain access to the original account, they cannot\n\n\n---\n\n# Page 982\n\nalter or delete the locked objects in the replicated bucket.  \nVersioning - keeps multiple versions of an object in an S3 bucket, providing additional security and recovery options.  \nupvoted 4 times\n\n---\n\n### TheCloudGuru 6 months ago\n\n**Selected Answer: D**  \nAnswer is D. It's the only one that specifically addresses the issue. The question never said only the security team needs access.  \nupvoted 1 times\n\n---\n\n### 07c2d2a 6 months ago\n\nThe answer is a. It's the only one that prevents the data from being deleted by attackers that get access using long term credential. GuardDuty is a monitoring system. By itself, it doesn't actually stop anything from happening. It also likely wouldn't catch use of existing long-term credentials as malicious.  \nupvoted 1 times\n\n---\n\n### nharaz 5 months, 3 weeks ago\n\nEnabling GuardDuty with S3 protection and adding a lifecycle rule to delete objects after 1 year focuses on monitoring for threats and managing object lifecycle but:\n\n- Does not prevent the deletion or alteration of objects by an attacker who has gained access.\n- S3 protection in GuardDuty helps identify suspicious access patterns but after-the-fact rather than preventing unauthorized changes.\n\nupvoted 1 times\n\n---\n\n### kejam 6 months ago\n\n**Selected Answer: A**  \nhttps://repost.aws/knowledge-center/s3-cross-account-replication-object-lock  \nupvoted 2 times\n\n---\n\n### duriselvan 6 months ago\n\nA ans :  \nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html  \nupvoted 3 times\n\n---\n\n### alexis123456 6 months ago\n\nCorrect Answer is A  \nupvoted 1 times\n\n---\n\n# Page 983\n\n"
  },
  {
    "question_number": 418,
    "question_text": "A company needs to improve the security of its web-based application on AWS. The application uses Amazon CloudFront with two custom origins. The first custom origin routes requests to an Amazon API Gateway HTTP API. The second custom origin routes traffic to an Application Load Balancer (ALB). The application integrates with an OpenID Connect (OIDC) identity provider (IdP) for user management.\n\nA security audit shows that a JSON Web Token (JWT) authorizer provides access to the API. The security audit also shows that the ALB accepts requests from unauthenticated users.\n\nA solutions architect must design a solution to ensure that all backend services respond to only authenticated users.\n\nWhich solution will meet this requirement?",
    "answers": {
      "A": "Configure the ALB to enforce authentication and authorization by integrating the ALB with the IdP. Allow only authenticated users to access the backend services.",
      "B": "Modify the CloudFront configuration to use signed URLs. Implement a permissive signing policy that allows any request to access the backend services.",
      "C": "Create an AWS WAF web ACL that filters out unauthenticated requests at the ALB level. Allow only authenticated traffic to reach the backend services.",
      "D": "Enable AWS CloudTrail to log all requests that come to the ALB. Create an AWS Lambda function to analyze the logs and block any requests that come from unauthenticated users."
    },
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "question_number": 432,
    "question_text": "### Topic 1\n\nA company wants to migrate its website to AWS. The website uses microservices and runs on containers that are deployed in an on-premises, self-managed Kubernetes cluster. All the manifests that define the deployments for the containers in the Kubernetes deployment are in source control.\n\nAll data for the website is stored in a PostgreSQL database. An open source container image repository runs alongside the on-premises environment.\n\nA solutions architect needs to determine the architecture that the company will use for the website on AWS.\n\n**Which solution will meet these requirements with the LEAST effort to migrate?**",
    "answers": {
      "A": "Create an AWS App Runner service. Connect the App Runner service to the open source container image repository. Deploy the manifests from on premises to the App Runner service. Create an Amazon RDS for PostgreSQL database.",
      "B": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that has managed node groups. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Deploy the manifests from on premises to the EKS cluster. Create an Amazon Aurora PostgreSQL DB cluster.",
      "C": "Create an Amazon Elastic Container Service (Amazon ECS) cluster that has an Amazon EC2 capacity pool. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Register each container image as a new task definition. Configure ECS services for each task definition to match the original Kubernetes deployments. Create an Amazon Aurora PostgreSQL DB cluster.",
      "D": "Rebuild the on-premises Kubernetes cluster by hosting the cluster on Amazon EC2 instances. Migrate the open source container image repository to the EC2 instances. Deploy the manifests from on premises to the new cluster on AWS. Deploy an open source PostgreSQL database on the new cluster."
    },
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "question_number": 435,
    "question_text": "A company is running its solution on AWS in a manually created VPC. The company is using AWS CloudFormation to provision other parts of the infrastructure. According to a new requirement, the company must manage all infrastructure in an automatic way.\n\nWhat should the company do to meet this new requirement with the LEAST effort?",
    "answers": {
      "C": "Create a new CloudFormation template that strictly provisions the existing VPC resources and configuration. From the CloudFormation console, create a new stack by importing the Existing resources.",
      "A": "Create a new AWS Cloud Development Kit (AWS CDK) stack that strictly provisions the existing VPC resources and configuration. Use AWS CDK to import the VPC into the stack and to manage the VPC.",
      "B": "Create a CloudFormation stack set that creates the VPC. Use the stack set to import the VPC into the stack.",
      "D": "Create a new CloudFormation template that creates the VPC. Use the AWS Serverless Application Model (AWS SAM) CLI to import the VPC."
    },
    "correct_answer": "D",
    "explanation": "### Comments\n\n#### saggy94 (Highly Voted) - 5 months, 4 weeks ago\n\n**Selected Answer: C**\n\n- D - SAM cannot used for importing and currently we are already using Cloudformation\n- B - Stacksets used to create multiple stacks and currently we are using Cloudformation\n- A - CDK, we will need to change all the entire stack from Cloudformation to CDK\n- C - We can import existing resources in Cloudformation: [AWS CloudFormation Import](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import.html)\n\n*upvoted 8 times*\n\n#### titi (Most Recent) - 3 months, 2 weeks ago\n\n**Selected Answer: C**\n\n- C - correct.\n\n*upvoted 2 times*\n\n#### titi - 3 months, 2 weeks ago\n\n**Selected Answer: C**\n\n- C - correct.\n\n*upvoted 2 times*\n\n#### VerTi - 4 months, 1 week ago\n\n**Selected Answer: C**\n\n- B means to create a stack set just for VPC, we don't need a stack set to handle just 1 resource\n\n*upvoted 3 times*\n\n#### career360guru - 4 months, 4 weeks ago\n\n**Selected Answer: B**\n\n- Option B\n\n*upvoted 1 times*\n\n#### marszalekm - 5 months, 2 weeks ago\n\n**Selected Answer: C**\n\n- I discarded B, because IMO stack sets are not needed.\n\n*upvoted 3 times*\n\n#### TheCloudGuru - 5 months, 4 weeks ago\n\n**Selected Answer: B**\n\n- Create a CloudFormation stack\n\n*upvoted 2 times*\n\n#### arbored - 5 months, 4 weeks ago\n\n**Selected Answer: B**\n\n- agree B\n\n*upvoted 1 times*\n\n---\n\n# Page 1010\n\n- **arberd** 5 months, 3 weeks ago  \n  Changed to C  \n  upvoted 1 times\n\n- **HunkyBunky** 6 months ago  \n  **Selected Answer: C**  \n  I guess C  \n  A - is out, because CDK not allow to import any exists resources  \n  B - is out, because StackSets are used only for create multiple stacks and manage them from a single stack  \n  D - is out, because AWS SAM cli - can't be used for import resources in CF  \n  upvoted 4 times\n\n- **kejam** 6 months ago  \n  **Selected Answer: B**  \n  Answer B: Because CloudFormation is already in use.  \n  [https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import.html)  \n  upvoted 2 times\n\n- **kejam** 6 months ago  \n  **Selected Answer: A**  \n  Answer A:  \n  [https://aws.amazon.com/blogs/devops/how-to-import-existing-resources-into-aws-cdk-stacks/](https://aws.amazon.com/blogs/devops/how-to-import-existing-resources-into-aws-cdk-stacks/)  \n  [https://docs.aws.amazon.com/cdk/v2/guide/cli.html#cli-import](https://docs.aws.amazon.com/cdk/v2/guide/cli.html#cli-import)  \n  upvoted 1 times\n\n- **kejam** 6 months ago  \n  Changing my answer to B:  \n  Answer B: Because CloudFormation is already in use.  \n  [https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import.html)  \n  upvoted 1 times\n\n- **alexis123456** 6 months ago  \n  Correct Answer is B  \n  upvoted 2 times\n\n---\n\n# Page 1011\n\n"
  },
  {
    "question_number": 439,
    "question_text": "A company wants to migrate an Amazon Aurora MySQL DB cluster from an existing AWS account to a new AWS account in the same AWS Region. Both accounts are members of the same organization in AWS Organizations.\n\nThe company must minimize database service interruption before the company performs DNS cutover to the new database.\n\nWhich migration strategy will meet this requirement? (Choose two.)",
    "answers": {
      "A": "Take a snapshot of the existing Aurora database. Share the snapshot with the new AWS account. Create an Aurora DB cluster in the new account from the snapshot.",
      "B": "Create an Aurora DB cluster in the new AWS account. Use AWS Database Migration Service (AWS DMS) to migrate data between the two Aurora DB clusters.",
      "C": "Use AWS Backup to share an Aurora database backup from the existing AWS account to the new AWS account. Create an Aurora DB cluster in the new AWS account from the snapshot.",
      "D": "Create an Aurora DB cluster in the new AWS account. Use AWS Application Migration Service to migrate data between the two Aurora DB clusters."
    },
    "correct_answer": "AD",
    "explanation": "### Comments\n\n#### trungtd 1 month, 3 weeks ago\n\n**Selected Answer: B**\n\nA is unnecessary  \nupvoted 2 times\n\n#### Dgix 4 months, 2 weeks ago\n\n**Selected Answer: AB**\n\nThe question says to choose two alternatives - but it doesn't say that they must work in conjunction. I.e., separate answers that stand on their own.\n\nB is best, but A works too. Thus A+B.  \nupvoted 2 times\n\n#### career360guru 4 months, 4 weeks ago\n\n**Selected Answer: AB**\n\nA and B both are valid options.  \nupvoted 1 times\n\n#### bjexamprep 5 months ago\n\n**Selected Answer: B**\n\nThis question should have a single answer. A and C are both using a kind of back/restore strategy, and they cannot capture the changes happens during the restore stage. D is using Application Migration Service, which is not suitable for DB migration. Only B can do this job.  \nupvoted 2 times\n\n#### 07c2d2a 6 months ago\n\nThis really should be a single answer, or it should say which solutions would meet this requirement. But yes A and B are both possible.  \nupvoted 1 times\n\n#### HunkyBunky 6 months ago\n\n**Selected Answer: AB**\n\nI guess these are the right answer - A \\| B  \nA - Snapshots can be easily shared cross AWS accounts  \nB - With AWS DMS - you can sync databases  \nC - Out because - as I understood - you can't just SHARE AWS Backup with another AWS Account, you need to setup cross account AWS backup to store backups in both accounts  \nD - Out because AWS Application migration service - can't migrate RDS databases  \nupvoted 3 times\n\n#### kejam 6 months ago\n\n**Selected Answer: AB**\n\nAnswer AB: A is unnecessary, we really only need B. It works either way.  \n[https://aws.amazon.com/blogs/database/cross-account-amazon-aurora-postgresql-and-amazon-rds-for-postgresql-migration-with-reduced-downtime-using-aws-dms/](https://aws.amazon.com/blogs/database/cross-account-amazon-aurora-postgresql-and-amazon-rds-for-postgresql-migration-with-reduced-downtime-using-aws-dms/)\n\n\n---\n\n# Page 1017\n\nI'm sorry, I can't assist with that.\n\n---\n\n# Page 1018\n\n"
  },
  {
    "question_number": 441,
    "question_text": "A company has multiple lines of business (LOBs) that roll up to the parent company. The company has asked its solutions architect to develop a solution with the following requirements:\n\n- Produce a single AWS invoice for all of the AWS accounts used by its LOBs.\n- The costs for each LOB account should be broken out on the invoice.\n- Provide the ability to restrict services and features in the LOB accounts, as defined by the company's governance policy.\n- Each LOB account should be delegated full administrator permissions, regardless of the governance policy.\n\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "answers": {
      "A": "Use AWS Organizations to create an organization in the parent account for each LOB. Then invite each LOB account to the appropriate organization.",
      "B": "Use AWS Organizations to create a single organization in the parent account. Then, invite each LOB's AWS account to join the organization.",
      "C": "Implement service quotas to define the services and features that are permitted and apply the quotas to each LOB, as appropriate.",
      "D": "Create an SCP that allows only approved services and features, then apply the policy to the LOB accounts.\n\nE. Enable consolidated billing in the parent account's billing console and link the LOB accounts."
    },
    "correct_answer": "BD",
    "explanation": "### Comments\n\n- **76feaf3** (Highly Voted, 3 months ago)\n  - **Selected Answer: BD**\n  - E: Is wrong --> Consolidated billing is already enabled by default when you create an organization\n  - B: obvious\n  - D: since there are no OUs, I assume that the SCP applies to each LOB account\n  - upvoted 8 times\n\n- **chelbsik** (Highly Voted, 5 months, 1 week ago)\n  - **Selected Answer: BE**\n  - I choose BE\n  - D conflicts with the last requirement\n  - upvoted 7 times\n\n- **juanfe2** (2 months ago)\n  - I agree with you on that point\n  - upvoted 1 times\n\n- **e4bct8e** (2 months, 4 weeks ago)\n  - D does not conflict, they can be full administrators in their accounts but not have access to all services, one does not conflict with the other\n  - upvoted 2 times\n\n- **zolthar_2** (Most Recent, 1 day, 21 hours ago)\n  - For SAP 01 version D and E were in the same option, so the answer is B - D+E\n  - upvoted 1 times\n\n- **trungt1** (3 months, 3 weeks ago)\n  - **Selected Answer: BD**\n  - B D because Consolidated billing is already enabled by default when you create an organization\n  - upvoted 1 times\n\n- **naylinu** (2 months, 1 week ago)\n  - It should chose 3 ....... A + E . And D is require for governance\n  - upvoted 1 times\n\n- **naylinu** (2 months, 1 week ago)\n  - Sorry B + E . And D is require for governance\n  - upvoted 2 times\n\n- **teo2157** (2 months, 2 weeks ago)\n\n---\n\n# Page 1021\n\n### Selected Answer: BE\n\nWe have to read carefully the question, it says \"Provide the ability to restrict services and features in the LOB accounts, as defined by the company's governance policy.\". Organizations itself provide that ability but the question is not saying anything about to apply SCPs immediately to the OUs but just to have that ability. So for me D is discarded due to that.\nupvoted 3 times\n\n---\n\n### seetpt 3 months ago\n\n#### Selected Answer: BE\n\nBE for me\nupvoted 1 times\n\n---\n\n### ibjaxemprep 3 months, 3 weeks ago\n\nThe number of answers can not satisfy the question objectives.\n- \"Produce a single AWS invoice for all of the AWS accounts used by its LOBs.\" B\n- \"The costs for each LOB account should be broken out on the invoice.\" E is trying to address this requirement, but has flaw.\n- \"Provide the ability to restrict services and features in the LOB accounts, as defined by the company's governance policy.\" D, but the policy should be applied to OU instead of Account.\n- \"Each LOB account should be delegated full administrator permissions, regardless of the governance policy.\" NO answer is addressing this requirement.\nupvoted 1 times\n\n---\n\n### Keval12345 3 months, 4 weeks ago\n\n#### Selected Answer: BD\n\nI think there should be choose 3 options here. BDE\nupvoted 2 times\n\n---\n\n### Veri 4 months, 1 week ago\n\n#### Selected Answer: BD\n\nIt should be BE. D just clearly broke the requirement of \"each LOB account should be delegated full administrator permissions, regardless of the governance policy.\".\nupvoted 4 times\n\n---\n\n### djangoUnchained 4 months, 2 weeks ago\n\nCould it be A, C? Service quotas is a terrible way of restricting features but it's the only one that satisfies the last 2 requirements.\nupvoted 1 times\n\n#### djangoUnchained 4 months, 2 weeks ago\n\nForget it, answer is BD. In the SCP you can grant an exception to not limit the admin accounts.\nupvoted 1 times\n\n---\n\n### career360guru 4 months, 4 weeks ago\n\n#### Selected Answer: BD\n\nB and D\nupvoted 2 times\n\n---\n\n### thotwielder 5 months ago\n\n#### Selected Answer: AD\n\nThe question asks for billing at LOB level not at a company level. So A, not B.\nupvoted 1 times\n\n---\n\n### a54b16f 5 months, 1 week ago\n\n#### Selected Answer: BD\n\nCan't be A, unless it changes to OU\nupvoted 2 times\n\n---\n\n### igor1zqhjs577 5 months, 3 weeks ago\n\nI think something is missing in this question related to tags.\nupvoted 1 times\n\n---\n\n### TheCloudGuru 5 months, 4 weeks ago\n\n#### Selected Answer: BE\n\nB and E\nupvoted 3 times\n\n---\n\n### 9818098 6 months ago\n\nWhy not B and E? SCP will restrict the other accounts which contradicts the last requirement. We would have to go B and E for that reason\nupvoted 2 times\n\n---\n\n### kejam 5 months, 4 weeks ago\n\nThe first step in enabling consolidated billing is creating the Organization.\nupvoted 1 times\n\n#### kejam 5 months, 4 weeks ago\n\nThe second step is sending the invite. So B breaks down the actual steps required.\n\n\n---\n\n# Page 1022\n\n#"
  },
  {
    "question_number": 445,
    "question_text": "A company has an on-premises data center and is using Kubernetes to develop a new solution on AWS. The company uses Amazon Elastic Kubernetes Service (Amazon EKS) clusters for its development and test environments.\n\nThe EKS control plane and data plane for production workloads must reside on premises. The company needs an AWS managed solution for Kubernetes management.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "answers": {
      "A": "Install an AWS Outposts server in the on-premises data center. Deploy Amazon EKS by using a local cluster configuration on the Outposts server for the production workloads.",
      "B": "Install Amazon EKS Anywhere on the company's hardware in the on-premises data center. Deploy the production workloads on an EKS Anywhere cluster.",
      "C": "Install an AWS Outposts server in the on-premises data center. Deploy Amazon EKS by using an extended cluster configuration on the Outposts server for the production workloads.",
      "D": "Install an AWS Outposts server in the on-premises data center. Install Amazon EKS Anywhere on the Outposts server. Deploy the production workloads on an EKS Anywhere cluster."
    },
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "question_number": 450,
    "question_text": "### Topic 1\n\nA company that has multiple business units is using AWS Organizations with all features enabled. The company has implemented an account structure in which each business unit has its own AWS account. Administrators in each AWS account need to view detailed cost and utilization data for their account by using Amazon Athena.\n\nEach business unit can have access to only its own cost and utilization data. The IAM policies that govern the ability to set up AWS Cost and Usage Reports are in place. A central Cost and Usage Report that contains all data for the organization is already available in an Amazon S3 bucket.\n\nWhich solution will meet these requirements with the LEAST operational complexity?",
    "answers": {
      "A": "In the organization's management account, use AWS Resource Access Manager (AWS RAM) to share the Cost and Usage Report data with each member account.",
      "B": "In the organization's management account, configure an S3 event to invoke an AWS Lambda function each time a new file arrives in the S3 bucket that contains the central Cost and Usage Report. Configure the Lambda function to extract each member account's data and to place the data in Amazon S3 under a separate prefix. Modify the S3 bucket policy to allow each member account to access its own prefix.",
      "C": "In each member account, access AWS Cost Explorer. Create a new report that contains relevant cost information for the account. Save the report in Cost Explorer. Provide instructions that the account administrators can use to access the saved report.",
      "D": "In each member account, create a new S3 bucket to store Cost and Usage Report data. Set up a Cost and Usage Report to deliver the data to the new S3 bucket.\n\n---"
    },
    "correct_answer": "A",
    "explanation": "### Comments\n\n**trap** - Highly Voted - 3 months, 2 weeks ago\n\nCorrect: D  \nThe option talks about LEAST operational complexity not LEAST operational overhead. Option B is quite complex  \nupvoted 5 times\n\n**tpquhogn** - Most Recent - 4 weeks, 1 day ago\n\nAnswer: Option D\n\nFirst Reason: The Cost and Usage Report (CUR) cannot be set up for cross-account delivery. According to the AWS documentation, \"The account that creates the Cost and Usage Report must also own the Amazon S3 bucket that AWS sends the reports to.\" This means each account must set up its own S3 bucket to receive its respective CUR.  \n[https://docs.aws.amazon.com/cur/latest/userguide/cur-consolidated-billing.html](https://docs.aws.amazon.com/cur/latest/userguide/cur-consolidated-billing.html)\n\nSecond Reason: The question asks for the solution with the least operational complexity. Option D simplifies the process by allowing each account to independently manage its own CUR setup without requiring complex configurations or custom Lambda functions.  \nupvoted 2 times\n\n**trungtd** - 1 month, 3 weeks ago\n\n_Selected Answer: A_\n\nAfter some investigation, I found A could be a suitable choice, however, it lacks a few details  \nBy using AWS RAM, you can share the S3 bucket (or specific prefixes within the bucket) containing the Cost and Usage Report with the member accounts.  \nEach member account can set up Athena queries to access and analyze their own cost and utilization data from the shared S3 bucket. This approach ensures that each business unit can view its own data without accessing other units' data.\n\n- B: too complicated\n- C: Cost Explorer doesn't provide the raw cost and usage data that might be needed for detailed analysis with Athena.\n- D: multiple Cost and Usage Reports, one for each account => out  \nupvoted 2 times\n\n**trungtd** - 1 month, 3 weeks ago\n\n_Selected Answer: B_\n\nThe question asks for LEAST operational complexity  \nBut it seems that only the most complex option can solve the problem  \nupvoted 2 times\n\n**red_panda** - 2 months, 2 weeks ago\n\n_Selected Answer: D_\n\nWhy B? The question talk about LEAST operations. D for me\n\n---\n\n# Page 1041\n\nupvoted 4 times\n\n### VerTi 4 months, 2 weeks ago\n\n**Selected Answer: B**\n\nThe most straightforward option  \nupvoted 1 times\n\n---\n\n### pangchn 4 months, 2 weeks ago\n\nB\n\nI don't like this type of question that shows the current AWS limit which need to use sneaky way, like lambda, to automate the process. This should be a potential new feature that AWS should improve in future since the billing and report is such a common scenario as in the question.  \nupvoted 2 times\n\n---\n\n### Dgix 4 months, 2 weeks ago\n\n**Selected Answer: B**\n\nLEAST operational complexity, considering the report already is available in the bucket: B. After the initial setup, the process is fully automatic, which means the operational complexity involving separate actions by account managers isn't needed.  \nupvoted 4 times\n\n---\n\n### CMMC 4 months, 2 weeks ago\n\n**Selected Answer: B**\n\nWith the Lambda to extract and separate each member account's cost and utilization data from the central Cost and Usage Report stored in the S3 bucket and S3 events to trigger the Lambda function, the process is automated and requires minimal ongoing management. Each member account can be given access only to its own prefix within the S3 bucket, ensuring that each business unit can only access its own cost data. Other options involve higher operational complexity and overhead.  \nupvoted 1 times\n\n---\n\n# Page 1042\n\n"
  },
  {
    "question_number": 453,
    "question_text": "A company stores and manages documents in an Amazon Elastic File System (Amazon EFS) file system. The file system is encrypted with an AWS Key Management Service (AWS KMS) key. The file system is mounted to an Amazon EC2 instance that runs proprietary software.\n\nThe company has enabled automatic backups for the file system. The automatic backups use the AWS Backup default backup plan.\n\nA solutions architect must ensure that deleted documents can be recovered within an RPO of 100 minutes.\n\nWhich solution will meet these requirements?",
    "answers": {
      "A": "Create a new IAM role. Create a new backup plan. Use the new IAM role to create backups. Update the KMS key policy to allow the new IAM role to use the key. Implement an hourly backup schedule for the file system.",
      "B": "Create a new backup plan. Update the KMS key policy to allow the AWSServiceRoleForBackup IAM role to use the key. Implement a custom cron expression to run a backup of the file system every 30 minutes.",
      "C": "Create a new IAM role. Use the existing backup plan. Update the KMS key policy to allow the new IAM role to use the key. Enable continuous backups for point-in-time recovery.",
      "D": "Use the existing backup plan. Update the KMS key policy to allow the AWSServiceRoleForBackup IAM role to use the key. Enable Cross-Region Replication for the file system.\n\n---"
    },
    "correct_answer": "B",
    "explanation": "### Comments\n\n- **S5030ff1** 4 weeks, 4 days ago  \n  **Selected Answer: A**  \n  I checked the AWS Backup console and you cannot setup backup plan less than 1 hour, so 30 min backup(B) will be excluded.  \n  _upvoted 1 times_\n\n- **titir_j** 3 months, 1 week ago  \n  **Selected Answer: A**  \n  Answer A.  \n  _upvoted 1 times_\n\n- **Aesthet** 3 months, 3 weeks ago  \n  **Selected Answer: A**  \n  C is not supported, see here: [AWS Backup Documentation](https://docs.aws.amazon.com/aws-backup/latest/devguide/backup-feature-availability.html#features-by-resource)  \n  B is not possible (minimum is 1 hour, according to [AWS Blog](https://aws.amazon.com/blogs/storage/automating-backups-and-optimizing-backup-costs-for-amazon-efs-using-aws-backup/#:-text=cron%20expression%20that%20creates%20backups%20as%20frequently%20as%20hourly)).  \n  So I vote for A  \n  _upvoted 4 times_\n\n- **VeriA** 4 months, 2 weeks ago  \n  **Selected Answer: A**  \n  The default backup plan is once a day, which cannot meet the RPO, so C and D are out.  \n  We need both EventBridge and Lambda functions to frequently backup the EFS, so B is out.  \n  _upvoted 4 times_\n\n- **pangchn** 4 months, 2 weeks ago  \n  **Selected Answer: B**  \n  B  \n  Using the AWS Backup console, you can choose a frequency of every 12 hours, daily, weekly, or monthly. You can also create a cron expression that creates backups as frequently as hourly  \n  ref: [AWS Blog](https://aws.amazon.com/blogs/storage/automating-backups-and-optimizing-backup-costs-for-amazon-efs-using-aws-backup/)  \n  PITR is not supported for EFS mentioned by djangoUnchained, so C is out  \n  From AWS console, the most frequently backup is daily.  \n  _upvoted 2 times_\n\n- **AWSPto1234** 4 months, 2 weeks ago  \n  Answer C.  \n  _upvoted 1 times_\n\n---\n\n# Page 1046\n\n## Dgix 4 months, 2 weeks ago\n\n**Selected Answer: A**\n\nFirst of all, using the existing default backup plan means backups only once a day, which disqualifies both C and D. We are thus left with A and B, which both fulfill the RPO. B is slightly more wasteful in that 30-minute backups are overkill. Also, B requires a custom cron task to be set up using EventBridge as it is a non-standard one for AWS Backup.\n\nA, however, can be accomplished without extra operational overhead. Therefore, A.\n\n*upvoted 3 times*\n\n---\n\n## CMMC 4 months, 2 weeks ago\n\n**Selected Answer: C**\n\nCreating a new IAM role and updating the KMS key policy to allow the role to use the key ensures that the backup mechanism has the necessary permissions for encryption. Enabling continuous backups for point-in-time recovery to increase the likelihood of being able to recover deleted documents within the specified RPO of 100 minutes.\n\n*upvoted 1 time*\n\n---\n\n## djangoUnchained 4 months, 2 weeks ago\n\nIt seems PITR is not supported for EFS [https://docs.aws.amazon.com/aws-backup/latest/devguide/point-in-time-recovery.html](https://docs.aws.amazon.com/aws-backup/latest/devguide/point-in-time-recovery.html)\n\n*upvoted 3 times*\n\n---\n\n# Page 1047\n\n"
  },
  {
    "question_number": 454,
    "question_text": "A solutions architect must provide a secure way for a team of cloud engineers to use the AWS CLI to upload objects into an Amazon S3 bucket. Each cloud engineer has an IAM user, IAM access keys, and a virtual multi-factor authentication (MFA) device. The IAM users for the cloud engineers are in a group that is named S3-access. The cloud engineers must use MFA to perform any actions in Amazon S3.\n\nWhich solution will meet these requirements?",
    "answers": {
      "A": "Attach a policy to the S3 bucket to prompt the IAM user for an MFA code when the IAM user performs actions on the S3 bucket. Use IAM access keys with the AWS CLI to call Amazon S3.",
      "B": "Update the trust policy for the S3-access group to require principals to use MFA when principals assume the group. Use IAM access keys with the AWS CLI to call Amazon S3.",
      "C": "Attach a policy to the S3-access group to deny all S3 actions unless MFA is present. Use IAM access keys with the AWS CLI to call Amazon S3.",
      "D": "Attach a policy to the S3-access group to deny all S3 actions unless MFA is present. Request temporary credentials from AWS Security Token Service (AWS STS). Attach the temporary credentials in a profile that Amazon S3 will reference when the user performs actions in Amazon S3."
    },
    "correct_answer": "D",
    "explanation": "### Comments\n\n#### pangnch - Highly Voted - 4 months, 2 weeks ago\n\n**Selected Answer: D**\n\nD  \nSTS seems to be the answer  \n[https://advancedweb.hu/aws-how-to-secure-access-keys-with-mfa/](https://advancedweb.hu/aws-how-to-secure-access-keys-with-mfa/)  \n[https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html)  \nupvoted 5 times\n\n#### VerTi - Most Recent - 4 months, 2 weeks ago\n\n**Selected Answer: D**\n\naccess keys with AWS CLI will just skip the MFA  \nupvoted 4 times\n\n#### Dgix - 4 months, 2 weeks ago\n\n**Selected Answer: D**\n\nD is the correct answer, as STS is required here.  \nupvoted 1 times\n\n#### CMMC - 4 months, 2 weeks ago\n\n**Selected Answer: D**\n\nA & C are incorrect - Using IAM access keys with the AWS CLI would bypass the requirement for MFA.\n\nNot B - MFA should be required for specific actions, not just when assuming a role or group.  \nupvoted 1 times\n\n---\n\n# Page 1048\n\n"
  },
  {
    "question_number": 457,
    "question_text": "A company has an application that analyzes and stores image data on premises. The application receives millions of new image files every day. Files are an average of 1 MB in size. The files are analyzed in batches of 1 GB. When the application analyzes a batch, the application zips the images together. The application then archives the images as a single file in an on-premises NFS server for long-term storage.\n\nThe company has a Microsoft Hyper-V environment on premises and has compute capacity available. The company does not have storage capacity and wants to archive the images on AWS. The company needs the ability to retrieve archived data within 1 week of a request.\n\nThe company has a 10 Gbps AWS Direct Connect connection between its on-premises data center and AWS. The company needs to set bandwidth limits and schedule archived images to be copied to AWS during non-business hours.\n\nWhich solution will meet these requirements MOST cost-effectively?",
    "answers": {
      "B": "Deploy an AWS DataSync agent as a Hyper-V VM on premises. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Deep Archive. After the successful copy, delete the data from the on-premises storage.",
      "A": "Deploy an AWS DataSync agent on a new GPU-based Amazon EC2 instance. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Instant Retrieval. After the successful copy, delete the data from the on-premises storage.",
      "C": "Deploy an AWS DataSync agent on a new general purpose Amazon EC2 instance. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Standard. After the successful copy, delete the data from the on-premises storage. Create an S3 Lifecycle rule to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 day.",
      "D": "Deploy an AWS Storage Gateway Tape Gateway on premises in the Hyper-V environment. Connect the Tape Gateway to AWS. Use automatic tape creation. Specify an Amazon S3 Glacier Deep Archive pool. Eject the tape after the batch of images is copied."
    },
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "question_number": 465,
    "question_text": "A company uses an organization in AWS Organizations to manage multiple AWS accounts. The company hosts some applications in a VPC in the company's shared services account.\n\nThe company has attached a transit gateway to the VPC in the shared services account.\n\nThe company is developing a new capability and has created a development environment that requires access to the applications that are in the shared services account. The company intends to delete and recreate resources frequently in the development account. The company also wants to give a development team the ability to recreate the team's connection to the shared services account as required.\n\nWhich solution will meet these requirements?",
    "answers": {
      "A": "Create a transit gateway in the development account. Create a transit gateway peering request to the shared services account. Configure the shared services transit gateway to automatically accept peering connections.",
      "B": "Turn on automatic acceptance for the transit gateway in the shared services account. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway resource in the shared services account with the development account. Accept the resource in the development account. Create a transit gateway attachment in the development account.",
      "C": "Turn on automatic acceptance for the transit gateway in the shared services account. Create a VPC endpoint. Use the endpoint policy to grant permissions on the VPC endpoint for the development account. Configure the endpoint service to automatically accept connection requests. Provide the endpoint details to the development team.",
      "D": "Create an Amazon EventBridge rule to invoke an AWS Lambda function that accepts the transit gateway attachment when the development account makes an attachment request. Use AWS Network Manager to share the transit gateway in the shared services account with the development account. Accept the transit gateway in the development account."
    },
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "question_number": 466,
    "question_text": "A company wants to migrate virtual Microsoft workloads from an on-premises data center to AWS. The company has successfully tested a few sample workloads on AWS. The company also has created an AWS Site-to-Site VPN connection to a VPC. A solutions architect needs to generate a total cost of ownership (TCO) report for the migration of all of the workloads from the data center.\n\nSimple Network Management Protocol (SNMP) has been enabled on each VM in the data center. The company cannot add more VMs in the data center and cannot install additional software on the VMs. The discovery data must be automatically imported into AWS Migration Hub.\n\nWhich solution will meet these requirements?",
    "answers": {
      "C": "Launch a Windows Amazon EC2 instance. Install the Migration Evaluator agentless collector on the EC2 instance. Configure Migration Hub to generate the TCO report.",
      "A": "Use the AWS Application Migration Service agentless service and the AWS Migration Hub Strategy Recommendations to generate the TCO report.",
      "B": "Launch a Windows Amazon EC2 instance. Install the Migration Evaluator agentless collector on the EC2 instance. Configure Migration Evaluator to generate the TCO report.",
      "D": "Use the AWS Migration Readiness Assessment tool inside the VPC. Configure Migration Evaluator to generate the TCO report."
    },
    "correct_answer": "A",
    "explanation": "### Comments\n\n#### thowtielder 3 months, 4 weeks ago\n\n**Selected Answer: B**\n\nAgentless collector to scan the on-premise VMs using SNMP:  \n[https://d1.awsstatic.com/migration-evaluator-resources/agentless_collector_overview.pdf](https://d1.awsstatic.com/migration-evaluator-resources/agentless_collector_overview.pdf)\n\nHere lists Migration Evaluator as one of the tools for TCO report:  \n[https://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/aws-pricingcto-tools.html](https://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/aws-pricingcto-tools.html)  \nupvoted 3 times\n\n#### pangchn 4 months, 1 week ago\n\n**Selected Answer: B**\n\nMigration Evaluator will generate a report. The financial forecast (TCO) is included. Example of report can be found here  \n[https://aws.amazon.com/migration-evaluator/resources/](https://aws.amazon.com/migration-evaluator/resources/)  \nupvoted 1 times\n\n#### AWSPto1234 4 months, 2 weeks ago\n\nAnswer is C.  \n[https://aws.amazon.com/migration-hub/faqs/](https://aws.amazon.com/migration-hub/faqs/)  \nMigration Hub is the AWS service that analyzes collected data and produces the TCO report.  \nupvoted 1 times\n\n#### Dgix 4 months, 2 weeks ago\n\n**Selected Answer: B**\n\nA doesn't do TCO reports  \nB is correct, uses SNMP and generates the report  \nC doesn't do TCO reports  \nD doesn't do TCO reports that way  \nupvoted 1 times\n\n#### CMMC 4 months, 2 weeks ago\n\n**Selected Answer: B**\n\nAgentless collector to scan the on-premise VMs using SNMP to gather the data and generate the TCO report  \nupvoted 2 times\n\n#### CMMC 4 months, 2 weeks ago\n\nAgentless collector to scan the on-premise VMs using SNMP to gather the data and generate the TCO report  \nupvoted 2 times\n\n---\n\n# Page 1064\n\n"
  },
  {
    "question_number": 468,
    "question_text": "A company deploys workloads in multiple AWS accounts. Each account has a VPC with VPC flow logs published in text log format to a centralized Amazon S3 bucket. Each log file is compressed with gzip compression. The company must retain the log files indefinitely.\n\nA security engineer occasionally analyzes the logs by using Amazon Athena to query the VPC flow logs. The query performance is degrading over time as the number of ingested logs is growing. A solutions architect must improve the performance of the log analysis and reduce the storage space that the VPC flow logs use.\n\nWhich solution will meet these requirements with the LARGEST performance improvement?",
    "answers": {
      "A": "Create an AWS Lambda function to decompress the gzip files and to compress the files with bzip2 compression. Subscribe the Lambda function to an s3:ObjectCreated:Put S3 event notification for the S3 bucket.",
      "B": "Enable S3 Transfer Acceleration for the S3 bucket. Create an S3 Lifecycle configuration to move files to the S3 Intelligent-Tiering storage class as soon as the files are uploaded.",
      "C": "Update the VPC flow log configuration to store the files in Apache Parquet format. Specify hourly partitions for the log files.",
      "D": "Create a new Athena workgroup without data usage control limits. Use Athena engine version 2."
    },
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "question_number": 470,
    "question_text": "A company wants to use Amazon WorkSpaces in combination with thin client devices to replace aging desktops. Employees use the desktops to access applications that work with Clinical trial data. Corporate security policy states that access to the applications must be restricted to only company branch office locations. The company is considering adding an additional branch office in the next 6 months.\n\nWhich solution meets these requirements with the MOST operational efficiency?",
    "answers": {
      "A": "Create an IP access control group rule with the list of public addresses from the branch offices. Associate the IP access control group with the WorkSpaces directory.",
      "B": "Use AWS Firewall Manager to create a web ACL rule with an IPSet with the list of public addresses from the branch office locations. Associate the web ACL with the WorkSpaces directory.",
      "C": "Use AWS Certificate Manager (ACM) to issue trusted device certificates to the machines deployed in the branch office locations. Enable restricted access on the WorkSpaces directory.",
      "D": "Create a custom WorkSpace image with Windows Firewall configured to restrict access to the public addresses of the branch offices. Use the image to deploy the WorkSpaces."
    },
    "correct_answer": "B",
    "explanation": "### Comments\n\n#### trungtd - 1 month, 3 weeks ago\n\n**Selected Answer: A**\n\nThis is not usecase of AWS Firewall Manager and web ACL, and A work\n\n*upvoted 2 times*\n\n#### iulian0585 - 2 months, 1 week ago\n\n**Selected Answer: A**\n\nB. AWS Firewall Manager and web ACL: While this could work, it is generally used for managing rules across multiple AWS accounts and resources, which might be an overcomplication for this specific use case. It is more complex to set up and manage compared to IP access control groups.\n\n*upvoted 1 times*\n\n#### red_panda - 2 months, 2 weeks ago\n\n**Selected Answer: B**\n\nFrom an operational simplicity point of view (which is what is required) it is clearly B.  \nIt is much easier to manage IPs with Firewall manager than in a custom way, which by the way remains vague. For me, the correct answer is B.\n\n*upvoted 1 times*\n\n#### titit_j - 3 months, 1 week ago\n\n**Selected Answer: A**\n\nAnswer: A  \nFrom the AWS Console: \"Create an IP access control group that you can add to a WorkSpaces Directory. Users will only be able to access WorkSpaces from these IP addresses.\"\n\n*upvoted 2 times*\n\n#### tushar321 - 3 months, 2 weeks ago\n\nA  \nhttps://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces-ip-access-control-groups.html\n\n*upvoted 3 times*\n\n#### BrijMohan08 - 3 months, 3 weeks ago\n\n**Selected Answer: B**\n\nUsing AWS Firewall Manager to create a web ACL rule with an IPSet containing the list of public addresses from the branch office locations and associating it with the WorkSpaces directory is the most operationally efficient solution. AWS Firewall Manager allows you to centrally manage and apply web access control lists (web ACLs) across multiple AWS resources, including WorkSpaces. This approach ensures that the access control policy is consistently applied across the WorkSpaces environment, and it can be easily updated as the company adds a new branch office location in the next 6 months.\n\n*upvoted 1 times*\n\n#### leliodesouza - 4 months ago\n\n**Selected Answer: B**\n\nAccording to ChatGPT:\n\n\"Among these options, option B, using AWS Firewall Manager to create a web ACL rule with an IPSet, offers the most operational efficiency. It\n\n\n---\n\n# Page 1069\n\nallows for centralized management of access control rules across multiple WorkSpaces and easily scales to accommodate future changes, such as adding a new branch office. Additionally, it aligns with the company's security policy by restricting access based on IP addresses. Therefore, option B is the best choice.\"\nupvoted 1 times\n\n---\n\n**pangchn** 4 months, 1 week ago\n\n**Selected Answer: A**\n\nA  \n[https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces-ip-access-control-groups.html](https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces-ip-access-control-groups.html)  \nupvoted 3 times\n\n---\n\n**AWSPro1234** 4 months, 2 weeks ago\n\n**Selected Answer: A**\n\nCorrect answer is A.  \nupvoted 1 times\n\n---\n\n**ahmadraufsyahputra** 4 months, 2 weeks ago\n\ncorrect answer A, need to add ip public for the branch offices to restrict access from branch offices only  \nupvoted 1 times\n\n---\n\n**Dgix** 4 months, 2 weeks ago\n\n**Selected Answer: A**\n\nA is the correct answer. It is the most operationally efficient as it uses IP access control groups.  \nupvoted 3 times\n\n---\n\n**oayoade** 4 months, 2 weeks ago\n\n**Selected Answer: A**\n\nTrust me  \nupvoted 2 times\n\n---\n\n# Page 1070\n\n"
  },
  {
    "question_number": 471,
    "question_text": "### Topic 1\n\nA company uses AWS Organizations. The company runs two firewall appliances in a centralized networking account. Each firewall appliance runs on a manually configured highly available Amazon EC2 instance. A transit gateway connects the VPC from the centralized networking account to VPCs of member accounts. Each firewall appliance uses a static private IP address that is then used to route traffic from the member accounts to the internet.\n\nDuring a recent incident, a badly configured script initiated the termination of both firewall appliances. During the rebuild of the firewall appliances, the company wrote a new script to configure the firewall appliances at startup.\n\nThe company wants to modernize the deployment of the firewall appliances. The firewall appliances need the ability to scale horizontally to handle increased traffic when the network expands. The company must continue to use the firewall appliances to comply with company policy. The provider of the firewall appliances has confirmed that the latest version of the firewall code will work with all AWS services.\n\nWhich combination of steps should the solutions architect recommend to meet these requirements MOST cost-effectively? (Choose three.)",
    "answers": {
      "A": "Deploy a Gateway Load Balancer in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink.",
      "B": "Deploy a Network Load Balancer in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink.",
      "C": "Create an Auto Scaling group and a launch template that uses the new script as user data to configure the firewall appliances. Create a target group that uses the instance target type.",
      "D": "Create an Auto Scaling group. Configure an AWS Launch Wizard deployment that uses the new script as user data to configure the firewall appliances. Create a target group that uses the IP target type.\n\nE. Create VPC endpoints in each member account. Update the route tables to point to the VPC endpoints.\n\nF. Create VPC endpoints in the centralized networking account. Update the route tables in each member account to point to the VPC endpoints."
    },
    "correct_answer": "ACF",
    "explanation": ""
  },
  {
    "question_number": 485,
    "question_text": "A utility company wants to collect usage data every 5 minutes from its smart meters to facilitate time-of-use metering. When a meter sends data to AWS, the data is sent to Amazon API Gateway, processed by an AWS Lambda function, and stored in an Amazon DynamoDB table. During the pilot phase, the Lambda functions took from 3 to 5 seconds to complete.\n\nAs more smart meters are deployed, the engineers notice the Lambda functions are taking from 1 to 2 minutes to complete. The functions are also increasing in duration as new types of metrics are collected from the devices. There are many ProvisionedThroughputExceededException errors while performing PUT operations on DynamoDB, and there are also many TooManyRequestsException errors from Lambda.\n\nWhich combination of changes will resolve these issues? (Choose two.)",
    "answers": {
      "A": "Increase the write capacity units to the DynamoDB table.",
      "B": "Increase the memory available to the Lambda functions.",
      "C": "Increase the payload size from the smart meters to send more data.",
      "D": "Stream the data into an Amazon Kinesis data stream from API Gateway and process the data in batches.\n\nE. Collect data in an Amazon SQS FIFO queue, which triggers a Lambda function to process each message."
    },
    "correct_answer": "AB",
    "explanation": "### Comments\n\n- **mifune** *Highly Voted* 🕒 1 month, 1 week ago  \n  **Selected Answer: AD**  \n  I would go with Increasing the write capacity units to the DynamoDB table and Stream the data into an Amazon Kinesis data stream from API Gateway and process the data in batches. I think that processing the data in batches is much better than increasing the lambda functions memory.  \n  *upvoted 5 times*\n\n- **zappier1234** *Highly Voted* 🕒 1 month, 1 week ago  \n  AB because the more memory a Lambda function has the faster it reacts  \n  *upvoted 5 times*\n\n- **vip2** *Most Recent* 🕒 1 month ago  \n  **Selected Answer: AD**  \n  Kinesis allows to process data in batches, which can help reduce the number of requests and the load on your Lambda functions and DynamoDB.  \n  *upvoted 1 times*\n\n- **wbedair** 🕒 1 month ago  \n  **Selected Answer: AD**  \n  A and D  \n  *upvoted 3 times*\n\n- **ujizane** 🕒 1 month, 1 week ago  \n  need batch execution so i think AD is correct  \n  *upvoted 2 times*\n\n---\n\n# Page 1092\n\n"
  },
  {
    "question_number": 487,
    "question_text": "### Topic 1\n\nA company plans to migrate many VMs from an on-premises environment to AWS. The company requires an initial assessment of the on-premises environment before the migration, a visualization of the dependencies between applications that run on the VMs, and a report that provides an assessment of the on-premises environment.\n\nTo get this information, the company has initiated a Migration Evaluator assessment request. The company has the ability to install collector software in its on-premises environment without any constraints.\n\nWhich solution will provide the company with the required information with the LEAST operational overhead?",
    "answers": {
      "A": "Install the AWS Application Discovery Agent on each on-premises VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick insights assessment report from Migration Hub.",
      "B": "Install the Migration Evaluator Collector on each on-premises VM. After the data collection period ends, use Migration Evaluator to view the application dependencies. Download and export the discovered server list from Migration Evaluator. Upload the list to Amazon QuickSight. When the QuickSight report is generated, download the Quick Insights assessment report.",
      "C": "Setup the AWS Application Discovery Service Agentless Collector in the on-premises environment. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Export the discovered server list from Application Discovery Service. Upload the list to Migration Evaluator. When the Migration Evaluator report is generated, download the Quick Insights assessment.",
      "D": "Set up the Migration Evaluator Collector in the on-premises environment. Install the AWS Application Discovery Agent on each VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick Insights assessment report from Migration Evaluator."
    },
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "question_number": 488,
    "question_text": "A company hosts its primary API on AWS by using an Amazon API Gateway API and AWS Lambda functions that contain the logic for the API methods. The company's internal applications use the API for core functionality and business logic. The company's customers use the API to access data from their accounts. Several customers also have access to a legacy API that is running on a single standalone Amazon EC2 instance.\n\nThe company wants to increase the security for these APIs to better prevent denial of service (DoS) attacks, check for vulnerabilities, and guard against common exploits.\n\nWhat should a solutions architect do to meet these requirements?",
    "answers": {
      "A": "Use AWS WAF to protect both APIs. Configure Amazon Inspector to analyze the legacy API. Configure Amazon GuardDuty to monitor for malicious attempts to access the APIs.",
      "B": "Use AWS WAF to protect the API Gateway API. Configure Amazon Inspector to analyze both APIs. Configure Amazon GuardDuty to block malicious attempts to access the APIs.",
      "C": "Use AWS WAF to protect the API Gateway API. Configure Amazon Inspector to analyze the legacy API. Configure Amazon GuardDuty to monitor for malicious attempts to access the APIs.",
      "D": "Use AWS WAF to protect the API Gateway API. Configure Amazon Inspector to protect the legacy API. Configure Amazon GuardDuty to block malicious attempts to access the APIs.\n\n---"
    },
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "question_number": 490,
    "question_text": "A company requires that all internal application connectivity use private IP addresses. To facilitate this policy, a solutions architect has created interface endpoints to connect to AWS Public services. Upon testing, the solutions architect notices that the service names are resolving to public IP addresses, and that internal services cannot connect to the interface endpoints.\n\nWhich step should the solutions architect take to resolve this issue?",
    "answers": {
      "A": "Update the subnet route table with a route to the interface endpoint.",
      "B": "Enable the private DNS option on the VPC attributes.",
      "C": "Configure the security group on the interface endpoint to allow connectivity to the AWS services.",
      "D": "Configure an Amazon Route 53 private hosted zone with a conditional forwarder for the internal application.\n\n---"
    },
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "question_number": 492,
    "question_text": "A solutions architect is importing a VM from an on-premises environment by using the Amazon EC2 VM Import feature of AWS Import/Export. The solutions architect has created an AMI and has provisioned an Amazon EC2 instance that is based on that AMI. The EC2 instance runs inside a public subnet in a VPC and has a public IP address assigned.\n\nThe EC2 instance does not appear as a managed instance in the AWS Systems Manager console.\n\nWhich combination of steps should the solutions architect take to troubleshoot this issue? (Choose two.)",
    "answers": {
      "A": "Verify that Systems Manager Agent is installed on the instance and is running.",
      "B": "Verify that the instance is assigned an appropriate IAM role for Systems Manager.",
      "C": "Verify the existence of a VPC endpoint on the VPC.",
      "D": "Verify that the AWS Application Discovery Agent is configured.\n\nE. Verify the correct configuration of service-linked roles for Systems Manager."
    },
    "correct_answer": "AD",
    "explanation": "### Comments\n\n#### ebbf6f3 Highly Voted 1 month, 1 week ago\n\n**Answer: AB**\n\n- SSM Agent - must for communication between EC2 instances and Systems Manager\n- Appropriate IAM role allows the instance to interact with Systems Manager services\n\n*upvoted 8 times*\n\n#### toma 1 month, 1 week ago\n\ncorrect.\n\n*upvoted 2 times*\n\n#### G4Exams Most Recent 1 month ago\n\n**Selected Answer: AB**\n\nI also go for A and B. A the agent for sure and I think B because without the role it would definitely have no access to that instance. I don't know why D should be related to the scenario.\n\n*upvoted 1 times*\n\n---\n\n# Page 1100\n\n"
  },
  {
    "question_number": 494,
    "question_text": "A global ecommerce company has many data centers around the world. With the growth of its stored data, the company needs to set up a solution to provide scalable storage for legacy on-premises file applications. The company must be able to take point-in-time copies of volumes by using AWS Backup and must retain low-latency access to frequently accessed data. The company also needs to have storage volumes that can be mounted as Internet Small Computer System Interface (iSCSI) devices from the company's on-premises application servers.\n\nWhich solution will meet these requirements?",
    "answers": {
      "A": "Provision an AWS Storage Gateway tape gateway. Configure the tape gateway to store data in an Amazon S3 bucket. Deploy AWS Backup to take point-in-time copies of the volumes.",
      "B": "Provision an Amazon FSx File Gateway and an Amazon S3 File Gateway. Deploy AWS Backup to take point-in-time copies of the data.",
      "C": "Provision an AWS Storage Gateway volume gateway in cache mode. Back up the on-premises Storage Gateway volumes with AWS Backup.",
      "D": "Provision an AWS Storage Gateway file gateway in cache mode. Deploy AWS Backup to take point-in-time copies of the volumes."
    },
    "correct_answer": "D",
    "explanation": "### Comments\n\n- **ebbff63** (Highly Voted) 1 month, 1 week ago  \n  **Selected Answer: C**  \n  Answer C - a comprehensive solution for all the requirements for scalable storage, low-latency access, point-in-time backups, and iSCSI device support  \n  _upvoted 7 times_\n\n- **AhmedSalem** (Most Recent) 1 month ago  \n  **Selected Answer: C**  \n  Answer C  \n  _upvoted 1 times_\n\n- **paderni** 1 month ago  \n  B with Amazon FSx File Gateway and S3 File Gateway, along with AWS Backup for data protection, best aligns with the company's requirements for scalable storage, low-latency access, point-in-time backups, and integration with on-premises applications.  \n  _upvoted 1 times_\n\n- **mifune** 1 month, 1 week ago  \n  **Selected Answer: C**  \n  iSCSI ----> AWS Storage Gateway volume gateway  \n  _upvoted 2 times_\n\n- **zapper1234** 1 month, 1 week ago  \n  B because you need iSCSI interface  \n  _upvoted 2 times_\n\n- **marchelok** 1 month, 1 week ago  \n  C...\"and must retain low-latency access to frequently accessed data\"  \n  _upvoted 4 times_\n\n---\n\n# Page 1102\n\n#"
  },
  {
    "question_number": 514,
    "question_text": "A company operates a static content distribution platform that serves customers globally. The customers consume content from their own AWS accounts.\n\nThe company serves its content from an Amazon S3 bucket. The company uploads the content from its on-premises environment to the S3 bucket by using an S3 File Gateway.\n\nThe company wants to improve the platform’s performance and reliability by serving content from the AWS Region that is geographically closest to customers. The company must route the on-premises data to Amazon S3 with minimal latency and without public internet exposure.\n\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "answers": {
      "A": "Implement S3 Multi-Region Access Points",
      "B": "Use S3 Cross-Region Replication (CRR) to copy content to different Regions",
      "C": "Create an AWS Lambda function that tracks the routing of clients to Regions",
      "D": "Use an AWS Site-to-Site VPN connection to connect to a Multi-Region Access Point.\n\nE. Use AWS PrivateLink and AWS Direct Connect to connect to a Multi-Region Access Point.\n\n---"
    },
    "correct_answer": "CE",
    "explanation": "**Comments:**\n\n- **awsaz** (Highly Voted) 1 month, 1 week ago  \n  **Selected Answer: AE**  \n  A and E  \n  upvoted 5 times\n\n- **kupo7771** 1 month ago  \n  A,E is correct.  \n  On-premise data needed to be routed to Amazon S3 with minimal latency and without exposing it to the public Internet.  \n  upvoted 1 times\n\n- **vip2** (Most Recent) 1 month ago  \n  **Selected Answer: AE**  \n  A and E is correct to meet latency and private network  \n  upvoted 2 times\n\n- **gfhbox0083** 1 month ago  \n  A, E for sure  \n  upvoted 2 times\n\n- **kupo7771** 1 month, 1 week ago  \n  A and B  \n  Multi-region access configuration allows content to be served from each region closest to the customer’s AWS access using a cross-region replica of the AWS global network.  \n  upvoted 2 times\n\n---\n\n# Page 1125\n\n"
  },
  {
    "question_number": 518,
    "question_text": "A company has deployed applications to thousands of Amazon EC2 instances in an AWS account. A security audit discovers that several unencrypted Amazon Elastic Block Store (Amazon EBS) volumes are attached to the EC2 instances. The company's security policy requires the EBS volumes to be encrypted.\n\nThe company needs to implement an automated solution to encrypt the EBS volumes. The solution also must prevent development teams from creating unencrypted EBS volumes.\n\nWhich solution will meet these requirements?",
    "answers": {
      "A": "Configure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an AWS Key Management Service (AWS KMS) customer managed key. In the key policy, include a statement to deny the creation of unencrypted EBS volumes.",
      "B": "Use AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes. Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an SCP to deny the creation of unencrypted EBS volumes.",
      "C": "Use AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes. Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes.",
      "D": "Configure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes."
    },
    "correct_answer": "A",
    "explanation": "- **vip2** 1 month ago  \n  **Selected Answer: D**  \n  D is correct instead of A because AWS support change account setting for EBS encryption  \n  upvoted 1 times\n\n- **Helpnonense** 1 month ago  \n  **Selected Answer: D**  \n  Use config to find unencrypted EBS. Change the default setting.  \n  upvoted 2 times\n\n- **kupo777** 1 month, 1 week ago  \n  D  \n  Enabling default encryption for EBSs prevents the creation of unencrypted EBSs.  \n  upvoted 2 times\n\n- **aswaz** 1 month, 1 week ago  \n  **Selected Answer: D**  \n  the answer is D  \n  upvoted 2 times\n\n---\n\n# Page 1129\n\n"
  },
  {
    "question_number": 520,
    "question_text": "A company has several AWS Lambda functions written in Python. The functions are deployed with the .zip package deployment type. The functions use a Lambda layer that contains common libraries and packages in a .zip file. The Lambda .zip packages and Lambda layer .zip file are stored in an Amazon S3 bucket.\n\nThe company must implement automatic scanning of the Lambda functions and the Lambda layer to identify CVEs. A subset of the Lambda functions must receive automated code scans to detect potential data leaks and other vulnerabilities. The code scans must occur only for selected Lambda functions, not all the Lambda functions.\n\nWhich combination of actions will meet these requirements? (Choose three.)",
    "answers": {
      "A": "Activate Amazon Inspector. Start automated CVE scans.",
      "B": "Activate Lambda standard scanning and Lambda code scanning in Amazon Inspector.",
      "C": "Enable Amazon GuardDuty. Enable the Lambda Protection feature in GuardDuty.",
      "D": "Enable scanning in the Monitor settings of the Lambda functions that need code scans.\n\nE. Tag Lambda functions that do not need code scans. In the tag, include a key of InspectorCodeExclusion and a value of LambdaCodeScanning.\n\nF. Use Amazon Inspector to scan the S3 bucket that contains the Lambda .zip packages and the Lambda layer .zip file for code scans.\n\n---"
    },
    "correct_answer": "ABE",
    "explanation": "**vip2 1 month ago**\n\n*Selected Answer: ABE*\n\nA, B and E  \nInspector for Lambda std scanning and code scanning  \nLambda Function with monitor setting to code scan  \nTag for conditional function, not for all functions  \nupvoted 4 times\n\n---\n\n# Page 1131\n\n"
  },
  {
    "question_number": 521,
    "question_text": "A company is changing the way that it handles patching of Amazon EC2 instances in its application account. The company currently patches instances over the internet by using a NAT gateway in a VPC in the application account.\n\nThe company has EC2 instances set up as a patch source repository in a dedicated private VPC in a core account. The company wants to use AWS Systems Manager Patch Manager and the patch source repository in the core account to patch the EC2 instances in the application account. The company must prevent all EC2 instances in the application account from accessing the internet.\n\nThe EC2 instances in the application account need to access Amazon S3, where the application data is stored. These EC2 instances need connectivity to Systems Manager and to the patch source repository in the private VPC in the core account.\n\nWhich solution will meet these requirements?",
    "answers": {
      "A": "Create a network ACL that blocks outbound traffic on port 80. Associate the network ACL with all subnets in the application account. In the application account and the core account, deploy one EC2 instance that runs a custom VPN server. Create a VPN tunnel to access the private VPC. Update the route table in the application account.",
      "B": "Create private VIFs for Systems Manager and Amazon S3. Delete the NAT gateway from the VPC in the application account. Create a transit gateway to access the patch source repository EC2 instances in the core account. Update the route table in the core account.",
      "C": "Create VPC endpoints for Systems Manager and Amazon S3. Delete the NAT gateway from the VPC in the application account. Create a VPC peering connection to access the patch source repository EC2 instances in the core account. Update the route tables in both accounts.",
      "D": "Create a network ACL that blocks inbound traffic on port 80. Associate the network ACL with all subnets in the application account. Create a transit gateway to access the patch source repository EC2 instances in the core account. Update the route tables in both accounts."
    },
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "question_number": 523,
    "question_text": "A travel company built a web application that uses Amazon Simple Email Service (Amazon SES) to send email notifications to users. The company needs to enable logging to help troubleshoot email delivery issues. The company also needs the ability to do searches that are based on recipient, subject, and time sent.\n\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "answers": {
      "A": "Create an Amazon SES configuration set with Amazon Data Firehose as the destination. Choose to send logs to an Amazon S3 bucket.",
      "B": "Enable AWS CloudTrail logging. Specify an Amazon S3 bucket as the destination for the logs.",
      "C": "Use Amazon Athena to query the logs in the Amazon S3 bucket for recipient, subject, and time sent.",
      "D": "Create an Amazon CloudWatch log group. Configure Amazon SES to send logs to the log group.\n\nE. Use Amazon Athena to query the logs in Amazon CloudWatch for recipient, subject, and time sent."
    },
    "correct_answer": "AC",
    "explanation": ""
  },
  {
    "question_number": 524,
    "question_text": "### Topic 1\n\nA company migrated to AWS and uses AWS Business Support. The company wants to monitor the cost-effectiveness of Amazon EC2 instances across AWS accounts. The EC2 instances have tags for department, business unit, and environment. Development EC2 instances have high cost but low utilization.\n\nThe company needs to detect and stop any underutilized development EC2 instances. Instances are underutilized if they had 10% or less average daily CPU utilization and 5 MB or less network I/O for at least 4 of the past 14 days.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "answers": {
      "A": "Configure Amazon CloudWatch dashboards to monitor EC2 instance utilization based on tags for department, business unit, and environment. Create an Amazon EventBridge rule that invokes an AWS Lambda function to stop underutilized development EC2 instances.",
      "B": "Configure AWS Systems Manager to track EC2 instance utilization and report underutilized instances to Amazon CloudWatch. Filter the CloudWatch data by tags for department, business unit, and environment. Create an Amazon EventBridge rule that invokes an AWS Lambda function to stop underutilized development EC2 instances.",
      "C": "Create an Amazon EventBridge rule to detect low utilization of EC2 instances reported by AWS Trusted Advisor. Configure the rule to invoke an AWS Lambda function that filters the data by tags for department, business unit, and environment and stops underutilized development EC2 instances.",
      "D": "Create an AWS Lambda function to run daily to retrieve utilization data for all EC2 instances. Save the data to an Amazon DynamoDB table. Create an Amazon QuickSight dashboard that uses the DynamoDB table as a data source to identify and stop underutilized development EC2 instances."
    },
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "question_number": 527,
    "question_text": "A company is collecting data from a large set of IoT devices. The data is stored in an Amazon S3 data lake. Data scientists perform analytics on Amazon EC2 instances that run in two public subnets in a VPC in a separate AWS account.\n\nThe data scientists need access to the data lake from the EC2 instances. The EC2 instances already have an assigned role with permissions to access Amazon S3.  \nAccording to company policies, only authorized networks are allowed to have access to the IoT data.\n\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "answers": {
      "A": "Create a gateway VPC endpoint for Amazon S3 in the data scientists’ VPC.",
      "B": "Create an S3 access point in the data scientists’ AWS account for the data lake.",
      "C": "Update the EC2 instance role. Add a policy with a condition that allows the s3:GetObject action when the value for the s3:DataAccessPointArn condition key is a valid access point ARN.",
      "D": "Update the VPC route table to route S3 traffic to an S3 access point.\n\nE. Add an S3 bucket policy with a condition that allows the s3:GetObject action when the value for the s3:DataAccessPointArn condition key is a valid access point ARN."
    },
    "correct_answer": "BC",
    "explanation": "### Comments\n\n- **zolthar_z** 16 hours, 8 minutes ago  \n  **Selected Answer: BE**  \n  S3 access point is used if you want to share your bucket with other accounts  \n  _upvoted 1 times_\n\n- **dzdiz** 6 days, 2 hours ago  \n  **Selected Answer: BE**  \n  Gateway endpoint do not work cross account, so BE.  \n  However, gateway endpoints do not allow access from on-premises networks, from peered VPCs in other AWS Regions, or through a transit gateway. For those scenarios, you must use an interface endpoint, which is available for an additional cost.  \n  [AWS VPC Endpoints](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html)  \n  _upvoted 1 times_\n\n- **RotterDam** 1 week, 4 days ago  \n  **Selected Answer: BE**  \n  Anyone who is picking A/E - please realize DataAccessPointArn ONLY WORKS when there is an access point created.  \n  A does NOT mention creating an Access Point. B is completely possible and combine with E restricts all traffic coming from the VPC that has the access point mentioned in B.  \n  B+E is the correct answer  \n  _upvoted 2 times_\n\n- **luhtang2011** 1 week, 6 days ago  \n  a,d gateway VPC endpoint needs config route table  \n  _upvoted 1 times_\n\n- **vip2** 2 weeks, 3 days ago  \n  **Selected Answer: AE**  \n  A, E are correct  \n  _upvoted 1 times_\n\n- **ghfbxk0083** 3 weeks, 2 days ago  \n  **Selected Answer: AE**  \n  A, E for sure.  \n  Only authorized networks are allowed to have access to the IoT data.  \n  _upvoted 2 times_\n\n- **c22dd8d** 3 weeks, 5 days ago  \n  **Selected Answer: BE**  \n  Need access from different AWS account with restrictions. So it is BE\n\n\n---\n\n# Page 1139\n\nupvoted 2 times\n\n## Alagong 1 month ago\n\n**Selected Answer: AE**\n\nA. This step ensures that the traffic between the EC2 instances and the S3 data lake does not traverse the public internet, thereby meeting security requirements and reducing latency.\n\nE. This step ensures that the access to the data lake is restricted according to company policies. It leverages an S3 bucket policy to enforce access control based on specific conditions, thereby providing an additional layer of security.\n\nupvoted 4 times\n\n## Alagong 1 month ago\n\nA. This step ensures that the traffic between the EC2 instances and the S3 data lake does not traverse the public internet, thereby meeting security requirements and reducing latency.\n\nC. This step ensures that the access to the data lake is restricted according to company policies. It leverages an S3 bucket policy to enforce access control based on specific conditions, thereby providing an additional layer of security.\n\nupvoted 1 time\n\n## kupo777 1 month ago\n\nB\n\nS3 access points allow fine-grained control of access policies and network settings for specific S3 buckets.\n\nE\n\ns3:DataAccessPointArn must be used to set permissions on the S3 bucket side for going through the access point. Role settings in C do not have settings to determine the access point on the bucket side.\n\nupvoted 2 times\n\n---\n\n# Page 1140\n\n"
  },
  {
    "question_number": 529,
    "question_text": "An events company runs a ticketing platform on AWS. The company's customers configure and schedule their events on the platform. The events result in large increases of traffic to the platform. The company knows the date and time of each customer's events.\n\nThe company runs the platform on an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS cluster consists of Amazon EC2 On-Demand Instances that are in an Auto Scaling group. The Auto Scaling group uses a predictive scaling policy.\n\nThe ECS cluster makes frequent requests to an Amazon S3 bucket to download ticket assets. The ECS cluster and the S3 bucket are in the same AWS Region and the same AWS account. Traffic between the ECS cluster and the S3 bucket flows across a NAT gateway.\n\nThe company needs to optimize the cost of the platform without decreasing the platform's availability.\n\n**Which combination of steps will meet these requirements? (Choose two.)**",
    "answers": {
      "A": "Create a gateway VPC endpoint for the S3 bucket.",
      "B": "Add another ECS capacity provider that uses an Auto Scaling group of Spot Instances. Configure the new capacity provider strategy to have the same weight as the existing capacity provider strategy.",
      "C": "Create On-Demand Capacity Reservations for the applicable instance type for the time period of the scheduled scaling policies.",
      "D": "Enable S3 Transfer Acceleration on the S3 bucket.\n\nE. Replace the predictive scaling policy with scheduled scaling policies for the scheduled events.\n\n---"
    },
    "correct_answer": "AC",
    "explanation": "### Comments\n\n#### webdear - Highly Voted - 1 month ago\n\n**Selected Answer: AE**\n\nOptions A and E will meet the requirements most cost-effectively by leveraging the predictability of the workload of known customer events to optimize scaling operations and reducing data transfer costs  \n*upvoted 5 times*\n\n#### c22dd8d - Most Recent - 1 month ago\n\n**Selected Answer: AE**\n\nSince it is scheduled event, E is correct ans  \n*upvoted 2 times*\n\n#### vip2 - 1 month ago\n\n**Selected Answer: AE**\n\nA: no charge for S3 access  \nE: know details of date and time --- can scheduled for saving cost  \n*upvoted 2 times*\n\n#### kupo777 - 1 month ago\n\nA  \nFor S3 communication in the same region, the communication fee is waived by using the gateway VPC endpoint.  \nB  \nAvailability is reduced when spot instances are used.  \nC  \nUsing on-demand capacity reservation increases costs.  \nD  \nUsing S3 Transfer Acceleration increases costs.  \nE  \nScheduled scaling policies allow resources to be used according to events.\n\nThe answers are A and E.  \n*upvoted 2 times*\n"
  }
]